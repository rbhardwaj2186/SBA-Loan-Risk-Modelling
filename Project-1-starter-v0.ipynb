{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1 Applied ML \n",
    "# Professor Uri Smashnov\n",
    "# Net ID rxb220087 \n",
    "# Rakshit Bhardwaj\n",
    "# Date Nov 4th 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1 Starter\n",
    "\n",
    "**Here are some tips for submitting your project. You can use the points as partial check list before submission.**\n",
    "\n",
    "- **Give your notebook a clear and descriptive title.** \n",
    "- **Explain your work in Markdown cells.** This will make your notebook easier to read and understand. You can use different colors of font to highlight important points.\n",
    "- **Remove any unnecessary code or text.** For example, you should not include the template for training and scoring in your final submission.\n",
    "- **Package your submission in a single file.** I will deduct points for multiple files or incorrect folder structure.\n",
    "- **Name your notebooks correctly.** Include your name and Net-ID in the file name.\n",
    "- **Train your TE/WOE encoders on the training set only.** You can train them on the full dataset for your final model.\n",
    "- **Test your scoring function.** Most students scoring functions in the past din't work, so make sure to test yours before submitting your project.\n",
    "- **Avoid common mistakes in your scoring function.** For example, your scoring function should not:\n",
    "  - drop records, expect the target to be passed\n",
    "  - fit TE/WOE/Scalers\n",
    "  - return anything other than a Pandas DF.\n",
    "- **Make sure you have the required number of engineered features.** \n",
    "- **Don't create features and then not use them in the model**, if there is a reason not to use the feature in the model, explain.\n",
    "- **Don't include models in your notebook that you didn't train.** This is considered cheating and will result in a grade of zero for the project.\n",
    "- **Consistently display model performance metrics.** Use AUC or AUCPR for all models and iterations, and don't switch between metrics. For sure don't use accuracy, it is misleading metric for the imbalanced datasets. \n",
    "- **Discuss your model results in a Markdown cell.** Don't just print the results; explain what they mean.\n",
    "- **Include a conclusion section in your notebook.** This is your chance to summarize your findings and discuss the implications of your work.\n",
    "- **Treat your notebook like a project report that will be read by your manager who can't read Python code.** Make sure your notebook is clear, concise, and easy to understand.\n",
    "- **Display a preview of your dataset that you used for training.** This will help me understand what features you used in your model.\n",
    "- **Use the libraries versions specified on eLearning.** For example, you should use H2O 3.42.0.2  \n",
    "- **Use Python 3.10.11.** If you use another version and your code doesn't work on 3.10.11, it will be considered a bug in your code.\n",
    "- **When running H2O and want to suppress long prints (for example model summary), include \";\" at the end of the command.**\n",
    "- **Don't include the dataset with your deliverables.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Requirements Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is draft - version 0 - changes are possible and will be announced.**\n",
    "\n",
    "Project 1 is to allow students to practice Data Science concepts learned so far.\n",
    "\n",
    "The project will include following tasks:\n",
    "- Load dataset. Don't use \"index\" column for training.\n",
    "- Clean up the data:\n",
    "    - Encode/replace missing values\n",
    "    - Replace features values that appear incorrect\n",
    "- Encode categorical variables\n",
    "- Split dataset to Train/Validation/Test\n",
    "- Add engineered features\n",
    "- Train and tune ML model\n",
    "- Provide final metrics using Test dataset\n",
    "- Provide a scoring function that can be used to score new data. You can test your scoring function on the provided \"scoring\" dataset.\n",
    "\n",
    "**Don't use PCA or TruncatedSVD for this project.** The goal of using Linear models is to be able to interpret the results via coefficients, and PCA/TruncatedSVD will make use of coefficients unusable for interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of models to train\n",
    "\n",
    "Your final submission should include single model. \n",
    "The model set you should try to come up with best model per type of model:\n",
    "1. Identify best model from: Sklearn Logistic Regression - try all combinations of regularization\n",
    "2. Identify best model from: H2O-3 GLM - try different combinations of regularization\n",
    "\n",
    "**Evaluation metric: AUCPR**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering\n",
    "\n",
    "You should train/fit categorical features scalers and encoders on Train only. Use `transform` or equivalent function on Validation/Test datasets.\n",
    "\n",
    "It is important to understand all the steps before model training, so that you can reliably replicate and test them to produce scoring function.\n",
    "\n",
    "\n",
    "You should generate various new features. Examples of such features can be seen in the Module-3 lecture on GLMs.  \n",
    "Your final model should have at least **10** new engineered features.   \n",
    "On-hot-encoding, label encoding, and target encoding **is not included in the** **10** features to create.    \n",
    "You can attempt target encoding, however the technique is not expected to produce improvement for Linear models.\n",
    "\n",
    "Ideas for Feature engineering for various types of variables:\n",
    "1. https://docs.h2o.ai/driverless-ai/1-10-lts/docs/userguide/transformations.html\n",
    "2. GLM lecture and hands-on (Module-3)\n",
    "\n",
    "\n",
    "**Note**: \n",
    "- You don't have to perform feature engineering using H2O-3 even if you decided to use H2O-3 GLM for model training.\n",
    "- It is OK to perform feature engineering using any technique, as long as you can replicate it correctly in the Scoring function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold calculation\n",
    "\n",
    "You will need to calculate optimal threshold for class assignment using F1 metric:\n",
    "- If using sklearn, use F1 `macro`: `f1_score(y_true, y_pred, average='macro')` \n",
    "- If using H2O-3, use F1\n",
    "\n",
    "You will need to find optimal probability threshold for class assignment, the threshold that maximizes above F1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring function\n",
    "\n",
    "The Project-1 will be graded based on the completeness and performance of your final model against the hold-out dataset.\n",
    "The hold-out dataset will not be known to the students. As part of your deliverables, you will need to submit a scoring function. \n",
    "\n",
    "You need to submit a scoring function for the best model you trained, either Sklearn or H2O-3 model.  \n",
    "\n",
    "The scoring function will perform the following:\n",
    "- Accept dataset in the same format as provided with the project, minus \"MIS_Status\" column\n",
    "- Load trained model and any encoders/scalers that are needed to transform data\n",
    "- Transform dataset into format that can be scored with the trained model\n",
    "- Score the dataset and return the results, for each record\n",
    "    - Record ID\n",
    "    - Record label as determined by final model (0 or 1)\n",
    "    - If your model returns probabilities, you need to assign the label based on maximum F1 threshold\n",
    "    \n",
    "Scoring function header:\n",
    "```\n",
    "def project_1_scoring(data):\n",
    "    \"\"\"\n",
    "    Function to score input dataset.\n",
    "    \n",
    "    Input: dataset in Pandas DataFrame format\n",
    "    Output: Python list of labels in the same order as input records\n",
    "    \n",
    "    Flow:\n",
    "        - Load artifacts\n",
    "        - Transform dataset\n",
    "        - Score dataset\n",
    "        - Return labels\n",
    "    \n",
    "    \"\"\"\n",
    "    l = data.shape[0]\n",
    "    return l*[0]\n",
    "```\n",
    "\n",
    "Look for full example of scoring function at the bottom of the notebook. **Don't copy as is - this is just an example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverables in a single zip file in the following structure:\n",
    "- `notebook` (folder)\n",
    "    - Jupyter notebook with complete code to manipulate data, train and tune final model. `ipynb` format.\n",
    "    - Jupyter notebook with scoring function. `ipynb` format.\n",
    "- `artifacts` (folder)\n",
    "    - Model and any potential encoders in the \"pkl\" format or native H2O-3 format (for H2O-3 model)\n",
    "    - Scoring function that will load the final model and encoders. Separate from above notebook or `.py` file\n",
    "\n",
    "\n",
    "\n",
    "Your notebook should include explanations about your code and be designed to be easily followed and results replicated. Once you are done with the final version, you will need to test it by running all cells from top to bottom after restarting Kernel. It can be done by running `Kernel -> Restart & Run All`\n",
    "\n",
    "\n",
    "**Important**: To speed up progress, first produce working code using a small subset of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 1500)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Extend cell width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the CSV file and dropping the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"D:/Work/Gre/UTD/Courses/Fall/MIS6341/Softwares/Python/ml-fall-2023/Project1/SBA_loans_project_1.csv\")\n",
    "df.drop(columns=\"index\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "City                   25\n",
       "State                  12\n",
       "Zip                     0\n",
       "Bank                 1405\n",
       "BankState            1411\n",
       "NAICS                   0\n",
       "NoEmp                   0\n",
       "NewExist              128\n",
       "CreateJob               0\n",
       "RetainedJob             0\n",
       "FranchiseCode           0\n",
       "UrbanRural              0\n",
       "RevLineCr            4094\n",
       "LowDoc               2319\n",
       "DisbursementGross       0\n",
       "BalanceGross            0\n",
       "GrAppv                  0\n",
       "SBA_Appv                0\n",
       "MIS_Status              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City unique values are ['GLEN BURNIE' 'WEST BEND' 'SAN DIEGO' ... 'Orange park' 'GREENHAVEN'\n",
      " 'SCHAFFERSTOWN']\n",
      "\n",
      "\n",
      "City data type is object\n",
      "State unique values are ['MD' 'WI' 'CA' 'MA' 'MO' 'OH' 'IL' 'GA' 'MI' 'NY' 'SC' 'FL' 'KS' 'ID'\n",
      " 'AZ' 'NH' 'NM' 'KY' 'NJ' 'TX' 'PA' 'MN' 'OK' 'OR' 'WA' 'IN' 'UT' 'AL'\n",
      " 'MS' 'CO' 'NC' 'CT' 'ME' 'HI' 'LA' 'IA' 'MT' 'RI' 'WV' 'NV' 'AR' 'VA'\n",
      " 'TN' 'ND' 'VT' 'WY' 'AK' 'SD' 'DE' 'NE' 'DC' nan]\n",
      "\n",
      "\n",
      "State data type is object\n",
      "Zip unique values are [21060 53095 92128 ... 32006 56038 14784]\n",
      "\n",
      "\n",
      "Zip data type is int64\n",
      "Bank unique values are ['BUSINESS FINANCE GROUP, INC.' 'JPMORGAN CHASE BANK NATL ASSOC'\n",
      " 'UMPQUA BANK' ... 'WILSHIRE CREDIT CORP' 'NEVADA BANK & TRUST COMPANY'\n",
      " 'FIRST COMMUN BK OF OZARKS']\n",
      "\n",
      "\n",
      "Bank data type is object\n",
      "BankState unique values are ['VA' 'IL' 'OR' 'MA' 'OH' 'CA' 'SD' 'CT' 'RI' 'SC' 'WI' 'GA' 'MI' 'AZ'\n",
      " 'DE' 'NY' 'NM' 'KY' 'NC' 'NJ' 'MN' 'WA' 'UT' 'IN' 'AL' 'MS' 'TX' 'DC'\n",
      " 'CO' 'ID' 'PA' 'NH' 'MO' 'MD' 'HI' 'TN' 'IA' 'FL' 'LA' 'MT' nan 'KS' 'WV'\n",
      " 'NV' 'OK' 'NE' 'ME' 'ND' 'WY' 'AK' 'VT' 'AR' 'PR' 'GU' 'VI' 'EN']\n",
      "\n",
      "\n",
      "BankState data type is object\n",
      "NAICS unique values are [811111 722410      0 ... 927110 813211 336414]\n",
      "\n",
      "\n",
      "NAICS data type is int64\n",
      "NoEmp unique values are [   7   20    2    5    3    1   10    4   18    6   25   11   30   21\n",
      "    9   35   15    8   32   38   17   34   51   12   60   16   43   65\n",
      "   40  137   93  385   70   14   22   19   33   64   13  150   24   66\n",
      "  205  130   45   84   26    0   49 1500   50   90   79   23   39  200\n",
      "   68   28   55   46  318   53   75 2520   27   94  100   57   31   48\n",
      "   44   86   67   54   47   81   29   42   63   80   36  360   52  160\n",
      "   96   41  140  114   72  115  215   37   78  110  125  300  105  228\n",
      "  123   58   82  240  138  500  134  131  350  120   89  210   62 1233\n",
      "  107  113   59   85  118  225  117  400  135   97   69   76  346  185\n",
      "  145  208  169  250  216   74  127  171   73  220  103  116  247   77\n",
      "  139  180  159  325  104  330  175  112  165   95  217   61  423  163\n",
      " 1200  128  270  151  170   83 4012  450  101   92  387   56  181  600\n",
      "   71  106  193  102  207  505  162  158 1520  249   99   91  167  155\n",
      "  420  173  302   88  133  146 4501  386  157  122  121  214  161  189\n",
      " 1706  172  242 3737  176 1000  226  109  750 8000  344  142   87  136\n",
      "  190  430  152 1300  182  299  275   98 1900  455  407 2725  149  141\n",
      "  255  575  229  166 9999 5000  288  761 1718  221  433 3713  248  156\n",
      "  191  369  384 2120  124  458  310  232  209  196  263  154  800  174\n",
      "  290 5149  720  235 1800  227 3500 9000  277 2610  132  266  111  129\n",
      "  144  316  230 1382  390  375  108  285 4000  498 1125  253  204  202\n",
      "  241  195 1003  186  340  305  168  223  258  315 7241  840  363  717\n",
      "  143  192  495  178  213  147  119  257  625 6000  365  177  278  356\n",
      " 5084  370  320  404 2112 4953 1542  280  523 5812 1150  194 2500 9090\n",
      " 5680  332  650  307 2510  184  148  126  260 2200  464  153 1280  525\n",
      "  680 3000  218 2010  283  712 5013  274  306  254  197 3030 7000  183\n",
      "  308  188  187  256  294  179  222 5947  535  355  396  317 1145  276\n",
      " 2000  203  403  262 1711  480  348  850  296  376  199  466 8018  760\n",
      "  900  268  246  282 1981  605  224  512 1005 1012  429  485 1451 3600\n",
      "  442  585 1940  414 1020  353  550  343  212 1920  570  368 7212  544\n",
      " 9992  602 1700 3732  244  336 7991  273  323  609  304 3170  237  530\n",
      "  427  243  456 1515  267  700  383  521  405  234  985  271  604  515\n",
      "  314  484  395  238  510 5211  828  425 6501  198 5921  476 1629  164\n",
      "  261  413 1015  475 1100  322  298 3200  289  465  380  560 1073 1112\n",
      "  460 7231  499  640 1440  408  259 7007 7216  339  293  608  520 4005\n",
      "  782  540  324 1340  313  808  688 1550 2400 8500  342  576  206  463\n",
      "  201  245  362  233 2151  319 1502 7538  265 2005  354  211 3100  424\n",
      "  357 1980  780 1603  351 9945 2900  231  295  251  252 1400  713  328\n",
      " 1250  301  284 8041  410 4800  823 7389 7111  341 2401 1600 2121  312\n",
      "  329 1461 1644 7999 4100  479 2100  435  967  292  735  488  421 3900\n",
      " 1010 2300  858  606  382 2202  394 1960  441  236  279  269  448  345\n",
      "  287 4685  685 2020 2501  281 4658 2232  635 3009 5511  358 1050 1101\n",
      " 3400 3089 4847  660  538 3334  309]\n",
      "\n",
      "\n",
      "NoEmp data type is int64\n",
      "NewExist unique values are [ 1.  2.  0. nan]\n",
      "\n",
      "\n",
      "NewExist data type is float64\n",
      "CreateJob unique values are [   6    0    2    1    4   15   20   12   14    3    8   38    5   45\n",
      "   18    9   30   11    7   10   60   31   13   21   19   41   16   93\n",
      "   24   40   25   72   22   55   50   33   85  100   17   65   27   23\n",
      "   26   28   29   32   66 8800  200   47   49   35   34  150   75   53\n",
      "  108  455   37   36   44  180   80   52  153   70   48   62   71   97\n",
      "  300   74   54   39   57  452   95  310  140  450  600   69  123  386\n",
      "   64  451  110  456   99   67  125   42   96  182   51   90   89  480\n",
      "   56   46  433   58   92  250   59   79   61   98   68   73  167  106\n",
      "   84  500  105 3500  454   43  457  270   82 1200 1229  124  195   88\n",
      "  135   63  120  126  109  145  151  453  190  800 3100  252  198 1011\n",
      "  130   76  134 5621   83  116  149   86  170 1000  144  226  152  102\n",
      "  148  160  141  121  165   77  139  220  115  118 1711 1618  363   78\n",
      "  179  114  175  136 1016   81   91  225  146   87  280 1100 3000 5199\n",
      "  104  143  860  235  210  183  177  168  101  169  350  157  119  184\n",
      "  103  155  158  137 1150  154  112  365  138  189 2515  400  129  303\n",
      " 1027  174 1530  375  206  221  186   94 5085  397  255  122  264  256\n",
      "  166  163  162 2020  569 2140  205  240  222  214]\n",
      "\n",
      "\n",
      "CreateJob data type is int64\n",
      "RetainedJob unique values are [   7    0    1   11    4   18    6   15    5   32    2   21    9    3\n",
      "   25   20   34   35   51   12   30   16    8  137   19   28   40   10\n",
      "   13  130   14   43   49   90  200   38   17   50   55   22   23   33\n",
      "   45  100  252   75   27   24   26   42   86   67   84   81   44   60\n",
      "   65   36   96   54   53   29  140   47   72   31 8800  300   80  119\n",
      "   57  240  138   48   70  500   46  133  350   62  110   93   79   83\n",
      " 4441   74  185   41   37  150   98   39  186   59   63   52  124   82\n",
      "  155   73  127   71  154  390   58   77  141  170   85   94  114  450\n",
      "   66   76  387   61  180  172   95  118  310   68  220  102  135   56\n",
      "   92  225  120  400  175  125  189   69  192   64  176  105  104  107\n",
      "  121  136   97  123  428  134  497   88  312  103  600   87  143  145\n",
      "  165  251  101  156  109  160   78  232  112  675  122  270  167  275\n",
      "  190  233  205  268  113  316  280  250  147  210   99  498  208  204\n",
      "  202  128   91  360  223  363  195  315  375  162  235  278  356  129\n",
      "  212  184  217  158   89 4000  207  177  116  245  523  194  115  216\n",
      "  203 5000  370  132  187  290  199  226  148  274  215  393  117  131\n",
      "  197  267  256  911 3225  178  142  535  750 1600  236  151  169  173\n",
      "  403  320  144 1711  168  163  201  108  188  139  126  262  111  302\n",
      "  219  260  106 3200  196  417  214  485  230  157  149  550  544  602\n",
      "  153  146 1700  330  191  609  720  420  317  254  198  213  243  206\n",
      "  277  171  285  263  484  355  430  237  182  286  322  371  298 2200\n",
      "  183  152  940  259  257  292  325  161  540  221  475  342  362  404\n",
      "  585 3100  304  610  266  305  159  265  231 1300 9500  328  410  297\n",
      "  318  255  291  164  900  967  472  700 3900  384  229  448 3860  287\n",
      "  685  166  281 1000  480 1111  476  660  548]\n",
      "\n",
      "\n",
      "RetainedJob data type is int64\n",
      "FranchiseCode unique values are [    1     0 22470 ... 63863 50078 10739]\n",
      "\n",
      "\n",
      "FranchiseCode data type is int64\n",
      "UrbanRural unique values are [1 0 2]\n",
      "\n",
      "\n",
      "UrbanRural data type is int64\n",
      "RevLineCr unique values are ['0' 'N' 'Y' 'T' nan '2' 'R' '.' '1' '`' '-' '7' 'A' 'Q' '5' 'C' '3']\n",
      "\n",
      "\n",
      "RevLineCr data type is object\n",
      "LowDoc unique values are ['N' 'Y' 'A' nan 'R' '0' 'S' 'C' '1']\n",
      "\n",
      "\n",
      "LowDoc data type is object\n",
      "DisbursementGross unique values are [743000. 137000. 280000. ... 295685. 117493.  14408.]\n",
      "\n",
      "\n",
      "DisbursementGross data type is float64\n",
      "BalanceGross unique values are [0.00000e+00 4.15090e+04 1.15820e+05 8.46170e+04 8.27875e+05 4.31270e+04\n",
      " 9.11100e+03 3.95476e+05 1.76000e+03 6.00000e+02 2.50000e+04 3.71000e+04\n",
      " 9.96262e+05]\n",
      "\n",
      "\n",
      "BalanceGross data type is float64\n",
      "GrAppv unique values are [743000. 137000. 280000. ... 315442. 120227. 157930.]\n",
      "\n",
      "\n",
      "GrAppv data type is float64\n",
      "SBA_Appv unique values are [743000. 109737. 210000. ... 283199. 108204. 118447.]\n",
      "\n",
      "\n",
      "SBA_Appv data type is float64\n",
      "MIS_Status unique values are [0 1]\n",
      "\n",
      "\n",
      "MIS_Status data type is int64\n"
     ]
    }
   ],
   "source": [
    "#show unique values in each column and its data type\n",
    "for col in df.columns:\n",
    "    print(f'{col} unique values are {df[col].unique()}')\n",
    "    print(\"\\n\")\n",
    "    print(f'{col} data type is {df[col].dtype}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing extra variables\n",
    "In RevLineCr, LowDoc, and NewExist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RevLineCr ['N' 'Y']\n",
      "LowDoc ['N' 'Y']\n",
      "NewExist [1.0 2.0 None]\n"
     ]
    }
   ],
   "source": [
    "for i in df['RevLineCr']:\n",
    "    if i not in ['Y','N']:\n",
    "        df['RevLineCr'].replace(i,'N',inplace=True)\n",
    "print(\"RevLineCr\",df['RevLineCr'].unique())\n",
    "\n",
    "for i in df['LowDoc']:\n",
    "    if i not in ['Y','N']:\n",
    "        df['LowDoc'].replace(i,'N',inplace=True)\n",
    "print(\"LowDoc\",df['LowDoc'].unique())\n",
    "\n",
    "for i in df['NewExist']:\n",
    "    if i not in [1,2]:\n",
    "        df['NewExist'].replace(i,None,inplace=True)\n",
    "print(\"NewExist\",df['NewExist'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "City                   25\n",
       "State                  12\n",
       "Zip                     0\n",
       "Bank                 1405\n",
       "BankState            1411\n",
       "NAICS                   0\n",
       "NoEmp                   0\n",
       "NewExist             1060\n",
       "CreateJob               0\n",
       "RetainedJob             0\n",
       "FranchiseCode           0\n",
       "UrbanRural              0\n",
       "RevLineCr               0\n",
       "LowDoc                  0\n",
       "DisbursementGross       0\n",
       "BalanceGross            0\n",
       "GrAppv                  0\n",
       "SBA_Appv                0\n",
       "MIS_Status              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has a large number of missing values, as can be seen. We'll substitute the correct values for the missing ones. If the column is numeric, for instance, the column median will be used to fill in the missing numbers. In the event that the column is categorical, the mode of the column will be used to fill in the missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filling Categorical Values\n",
    "With the mode aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_cols=['City', 'State', 'Bank', 'BankState', 'RevLineCr', 'LowDoc','NewExist']\n",
    "for column in category_cols:\n",
    "  df[column]=df[column].fillna(df[column].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "City                 0\n",
       "State                0\n",
       "Zip                  0\n",
       "Bank                 0\n",
       "BankState            0\n",
       "NAICS                0\n",
       "NoEmp                0\n",
       "NewExist             0\n",
       "CreateJob            0\n",
       "RetainedJob          0\n",
       "FranchiseCode        0\n",
       "UrbanRural           0\n",
       "RevLineCr            0\n",
       "LowDoc               0\n",
       "DisbursementGross    0\n",
       "BalanceGross         0\n",
       "GrAppv               0\n",
       "SBA_Appv             0\n",
       "MIS_Status           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "h2o_df = df.copy() # Keeping dataset handy for H2O-GLM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the missing values have been restored, the dataset will be divided into training and testing datasets. Let's divide the dataset into 30% for testing and 70% for training. For repeatability, random state 123 is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((566472, 19), (242775, 19))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test = train_test_split(df,test_size=0.3,random_state=123)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training set has 566472 rows and testing set has 242775 samples\n",
    "\n",
    "Target encoding is a data preprocessing technique used to convert categorical variables into numerical values that can be used by machine learning algorithms. It works by replacing each category with the average value of the target variable for that category. This can be helpful for algorithms that cannot handle categorical variables directly.\n",
    "\n",
    "In this case the target variable is \"MIS_Status\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Zip</th>\n",
       "      <th>Bank</th>\n",
       "      <th>BankState</th>\n",
       "      <th>NAICS</th>\n",
       "      <th>NoEmp</th>\n",
       "      <th>NewExist</th>\n",
       "      <th>CreateJob</th>\n",
       "      <th>RetainedJob</th>\n",
       "      <th>FranchiseCode</th>\n",
       "      <th>UrbanRural</th>\n",
       "      <th>RevLineCr</th>\n",
       "      <th>LowDoc</th>\n",
       "      <th>DisbursementGross</th>\n",
       "      <th>BalanceGross</th>\n",
       "      <th>GrAppv</th>\n",
       "      <th>SBA_Appv</th>\n",
       "      <th>MIS_Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>217613</th>\n",
       "      <td>0.114650</td>\n",
       "      <td>0.184773</td>\n",
       "      <td>93001</td>\n",
       "      <td>0.031447</td>\n",
       "      <td>0.218517</td>\n",
       "      <td>235910</td>\n",
       "      <td>56</td>\n",
       "      <td>0.17044</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.071674</td>\n",
       "      <td>0.15307</td>\n",
       "      <td>0.187063</td>\n",
       "      <td>305000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>305000.0</td>\n",
       "      <td>244000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95730</th>\n",
       "      <td>0.137597</td>\n",
       "      <td>0.165992</td>\n",
       "      <td>44039</td>\n",
       "      <td>0.128698</td>\n",
       "      <td>0.159167</td>\n",
       "      <td>484121</td>\n",
       "      <td>0</td>\n",
       "      <td>0.17044</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.243491</td>\n",
       "      <td>0.15307</td>\n",
       "      <td>0.187063</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780446</th>\n",
       "      <td>0.139151</td>\n",
       "      <td>0.116799</td>\n",
       "      <td>68122</td>\n",
       "      <td>0.175694</td>\n",
       "      <td>0.159167</td>\n",
       "      <td>451120</td>\n",
       "      <td>20</td>\n",
       "      <td>0.17044</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>38510</td>\n",
       "      <td>0.243491</td>\n",
       "      <td>0.15307</td>\n",
       "      <td>0.187063</td>\n",
       "      <td>870400.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>974500.0</td>\n",
       "      <td>730875.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337263</th>\n",
       "      <td>0.140704</td>\n",
       "      <td>0.197662</td>\n",
       "      <td>14208</td>\n",
       "      <td>0.118949</td>\n",
       "      <td>0.167297</td>\n",
       "      <td>321114</td>\n",
       "      <td>50</td>\n",
       "      <td>0.17044</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.071674</td>\n",
       "      <td>0.15307</td>\n",
       "      <td>0.187063</td>\n",
       "      <td>200000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>200000.0</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199634</th>\n",
       "      <td>0.140354</td>\n",
       "      <td>0.144273</td>\n",
       "      <td>16335</td>\n",
       "      <td>0.193277</td>\n",
       "      <td>0.078307</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0.17044</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.071674</td>\n",
       "      <td>0.15307</td>\n",
       "      <td>0.187063</td>\n",
       "      <td>370000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>370000.0</td>\n",
       "      <td>296000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            City     State    Zip      Bank  BankState   NAICS  NoEmp  \\\n",
       "217613  0.114650  0.184773  93001  0.031447   0.218517  235910     56   \n",
       "95730   0.137597  0.165992  44039  0.128698   0.159167  484121      0   \n",
       "780446  0.139151  0.116799  68122  0.175694   0.159167  451120     20   \n",
       "337263  0.140704  0.197662  14208  0.118949   0.167297  321114     50   \n",
       "199634  0.140354  0.144273  16335  0.193277   0.078307       0     19   \n",
       "\n",
       "        NewExist  CreateJob  RetainedJob  FranchiseCode  UrbanRural  \\\n",
       "217613   0.17044          0            0              1    0.071674   \n",
       "95730    0.17044          0            0              0    0.243491   \n",
       "780446   0.17044          5           15          38510    0.243491   \n",
       "337263   0.17044          0            0              1    0.071674   \n",
       "199634   0.17044          0            0              1    0.071674   \n",
       "\n",
       "        RevLineCr    LowDoc  DisbursementGross  BalanceGross    GrAppv  \\\n",
       "217613    0.15307  0.187063           305000.0           0.0  305000.0   \n",
       "95730     0.15307  0.187063            24000.0           0.0   24000.0   \n",
       "780446    0.15307  0.187063           870400.0           0.0  974500.0   \n",
       "337263    0.15307  0.187063           200000.0           0.0  200000.0   \n",
       "199634    0.15307  0.187063           370000.0           0.0  370000.0   \n",
       "\n",
       "        SBA_Appv  MIS_Status  \n",
       "217613  244000.0           0  \n",
       "95730    12000.0           0  \n",
       "780446  730875.0           0  \n",
       "337263  150000.0           0  \n",
       "199634  296000.0           0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Target encoder\n",
    "import category_encoders as ce\n",
    "categorical_columns = ['City', 'State', 'Bank', 'BankState', 'RevLineCr', 'LowDoc','NewExist', 'UrbanRural']\n",
    "\n",
    "encoder = ce.TargetEncoder(cols=categorical_columns)\n",
    "encoder.fit(X_train, X_train['MIS_Status'])\n",
    "\n",
    "train_encoded = encoder.transform(X_train)\n",
    "test_encoded = encoder.transform(X_test)\n",
    "\n",
    "# Renaming the columns\n",
    "train_encoded.rename(columns={col: col + \"_trg\" if col in categorical_columns else col for col in train_encoded.columns}, inplace=False)\n",
    "test_encoded.rename(columns={col: col + \"_trg\" if col in categorical_columns else col for col in test_encoded.columns}, inplace=False)\n",
    "\n",
    "train_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StandardScaler in scikit-learn is a preprocessing technique that centers and scales numerical features such that they have a mean of zero and a standard deviation of one.\n",
    "\n",
    "We will make use of the StandardScaler, which is used to transform both the training and test data in the same way, ensuring that the features have the same mean and standard deviation in both datasets.\n",
    "\n",
    "Here we will scale it on the training set and transform on both training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Zip</th>\n",
       "      <th>Bank</th>\n",
       "      <th>BankState</th>\n",
       "      <th>NAICS</th>\n",
       "      <th>NoEmp</th>\n",
       "      <th>NewExist</th>\n",
       "      <th>CreateJob</th>\n",
       "      <th>RetainedJob</th>\n",
       "      <th>FranchiseCode</th>\n",
       "      <th>UrbanRural</th>\n",
       "      <th>RevLineCr</th>\n",
       "      <th>LowDoc</th>\n",
       "      <th>DisbursementGross</th>\n",
       "      <th>BalanceGross</th>\n",
       "      <th>GrAppv</th>\n",
       "      <th>SBA_Appv</th>\n",
       "      <th>MIS_Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>217613</th>\n",
       "      <td>0.114650</td>\n",
       "      <td>0.184773</td>\n",
       "      <td>93001</td>\n",
       "      <td>0.031447</td>\n",
       "      <td>0.218517</td>\n",
       "      <td>235910</td>\n",
       "      <td>0.600407</td>\n",
       "      <td>0.17044</td>\n",
       "      <td>-0.035373</td>\n",
       "      <td>-0.045454</td>\n",
       "      <td>1</td>\n",
       "      <td>0.071674</td>\n",
       "      <td>0.15307</td>\n",
       "      <td>0.187063</td>\n",
       "      <td>0.358949</td>\n",
       "      <td>-0.002296</td>\n",
       "      <td>0.394801</td>\n",
       "      <td>0.410973</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95730</th>\n",
       "      <td>0.137597</td>\n",
       "      <td>0.165992</td>\n",
       "      <td>44039</td>\n",
       "      <td>0.128698</td>\n",
       "      <td>0.159167</td>\n",
       "      <td>484121</td>\n",
       "      <td>-0.153600</td>\n",
       "      <td>0.17044</td>\n",
       "      <td>-0.035373</td>\n",
       "      <td>-0.045454</td>\n",
       "      <td>0</td>\n",
       "      <td>0.243491</td>\n",
       "      <td>0.15307</td>\n",
       "      <td>0.187063</td>\n",
       "      <td>-0.614207</td>\n",
       "      <td>-0.002296</td>\n",
       "      <td>-0.594552</td>\n",
       "      <td>-0.600302</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780446</th>\n",
       "      <td>0.139151</td>\n",
       "      <td>0.116799</td>\n",
       "      <td>68122</td>\n",
       "      <td>0.175694</td>\n",
       "      <td>0.159167</td>\n",
       "      <td>451120</td>\n",
       "      <td>0.115688</td>\n",
       "      <td>0.17044</td>\n",
       "      <td>-0.013978</td>\n",
       "      <td>0.018584</td>\n",
       "      <td>38510</td>\n",
       "      <td>0.243491</td>\n",
       "      <td>0.15307</td>\n",
       "      <td>0.187063</td>\n",
       "      <td>2.317036</td>\n",
       "      <td>-0.002296</td>\n",
       "      <td>2.751996</td>\n",
       "      <td>2.533233</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337263</th>\n",
       "      <td>0.140704</td>\n",
       "      <td>0.197662</td>\n",
       "      <td>14208</td>\n",
       "      <td>0.118949</td>\n",
       "      <td>0.167297</td>\n",
       "      <td>321114</td>\n",
       "      <td>0.519620</td>\n",
       "      <td>0.17044</td>\n",
       "      <td>-0.035373</td>\n",
       "      <td>-0.045454</td>\n",
       "      <td>1</td>\n",
       "      <td>0.071674</td>\n",
       "      <td>0.15307</td>\n",
       "      <td>0.187063</td>\n",
       "      <td>-0.004686</td>\n",
       "      <td>-0.002296</td>\n",
       "      <td>0.025114</td>\n",
       "      <td>0.001232</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199634</th>\n",
       "      <td>0.140354</td>\n",
       "      <td>0.144273</td>\n",
       "      <td>16335</td>\n",
       "      <td>0.193277</td>\n",
       "      <td>0.078307</td>\n",
       "      <td>0</td>\n",
       "      <td>0.102223</td>\n",
       "      <td>0.17044</td>\n",
       "      <td>-0.035373</td>\n",
       "      <td>-0.045454</td>\n",
       "      <td>1</td>\n",
       "      <td>0.071674</td>\n",
       "      <td>0.15307</td>\n",
       "      <td>0.187063</td>\n",
       "      <td>0.584056</td>\n",
       "      <td>-0.002296</td>\n",
       "      <td>0.623655</td>\n",
       "      <td>0.637638</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            City     State    Zip      Bank  BankState   NAICS     NoEmp  \\\n",
       "217613  0.114650  0.184773  93001  0.031447   0.218517  235910  0.600407   \n",
       "95730   0.137597  0.165992  44039  0.128698   0.159167  484121 -0.153600   \n",
       "780446  0.139151  0.116799  68122  0.175694   0.159167  451120  0.115688   \n",
       "337263  0.140704  0.197662  14208  0.118949   0.167297  321114  0.519620   \n",
       "199634  0.140354  0.144273  16335  0.193277   0.078307       0  0.102223   \n",
       "\n",
       "        NewExist  CreateJob  RetainedJob  FranchiseCode  UrbanRural  \\\n",
       "217613   0.17044  -0.035373    -0.045454              1    0.071674   \n",
       "95730    0.17044  -0.035373    -0.045454              0    0.243491   \n",
       "780446   0.17044  -0.013978     0.018584          38510    0.243491   \n",
       "337263   0.17044  -0.035373    -0.045454              1    0.071674   \n",
       "199634   0.17044  -0.035373    -0.045454              1    0.071674   \n",
       "\n",
       "        RevLineCr    LowDoc  DisbursementGross  BalanceGross    GrAppv  \\\n",
       "217613    0.15307  0.187063           0.358949     -0.002296  0.394801   \n",
       "95730     0.15307  0.187063          -0.614207     -0.002296 -0.594552   \n",
       "780446    0.15307  0.187063           2.317036     -0.002296  2.751996   \n",
       "337263    0.15307  0.187063          -0.004686     -0.002296  0.025114   \n",
       "199634    0.15307  0.187063           0.584056     -0.002296  0.623655   \n",
       "\n",
       "        SBA_Appv  MIS_Status  \n",
       "217613  0.410973           0  \n",
       "95730  -0.600302           0  \n",
       "780446  2.533233           0  \n",
       "337263  0.001232           0  \n",
       "199634  0.637638           0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from copy import deepcopy\n",
    "\n",
    "numerical_columns = [ 'NoEmp', 'CreateJob', 'RetainedJob', 'GrAppv', 'SBA_Appv', 'DisbursementGross', 'BalanceGross']\n",
    "scaler = StandardScaler()\n",
    "train_encoded[numerical_columns] = scaler.fit_transform(train_encoded[numerical_columns])\n",
    "test_encoded[numerical_columns] = scaler.transform(test_encoded[numerical_columns])\n",
    "\n",
    "train_encoded.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created Feature extraction by making use of old variables in the following way\n",
    "\n",
    "\n",
    "(1) Log_Disbursement which gives the natural logarithmic form of DisbursementGross variable\n",
    "\n",
    "(2) Log_GrAppv the logarithmic version of the approved loan amount by the bank\n",
    "\n",
    "(3) Log_SBA_Appv, the logarithmic amount of the approved loan that will be assisted by SBA \n",
    "\n",
    "(4) Log_BalanceGross, is the logarithmic amount of total amount in an account or the total value of a financial asset or liability before any deductions or adjustments are made.\n",
    "\n",
    "(5) TotalJobs variable which is an addition of Createjobs(New people recruited) and RetainedJob (workers working before)\n",
    "\n",
    "(6) IncomeToLoan its values are calculated by dividing the 'DisbursementGross' column by the 'SBA_Appv' column for each corresponding row. This ratio can help you analyze the relationship between the amount disbursed and the approved SBA loan amount in terms of income.\n",
    "\n",
    "(7)  EmployeesToLoanRatio, its values are calculated by dividing the 'NoEmp' column (number of employees) by the 'SBA_Appv' column (approved SBA loan amount) for each corresponding row. This ratio can help you analyze the relationship between the number of employees and the size of the SBA loan approved for each entry in the dataset.\n",
    "\n",
    "(8) JobPerLoan, its values are calculated by dividing the 'TotalJobs' column (representing the total number of jobs) by the 'SBA_Appv' column (approved SBA loan amount) for each corresponding row. This ratio can help you analyze the impact of the SBA loan on job creation or support, expressed as the number of jobs per unit of loan amount approved.\n",
    "\n",
    "(9) Gauren_SBA_Appv, Its values are calculated by dividing the 'GrAppv' column (gross amount approved by the lender) by the 'SBA_Appv' column (the approved SBA loan amount) for each corresponding row. This ratio helps you analyze the extent to which the SBA is guaranteeing the loan relative to the total loan amount approved by the lender.\n",
    "\n",
    "(10) DefaultRate, Finally, we create a new feature 'DefaultRate' in the 'train_encoded' DataFrame and set its value to the calculated default rate for the particular group of loans based on the \"MIS_Status\" variable. This feature will represent the percentage of loans in the group that are classified as defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Features\n",
    "import numpy as np\n",
    "# Apply the log transformation to the specific feature in your training data\n",
    "#small_constant = 1e-10  # You can adjust this constant as needed\n",
    "# df['LogColumn'] = np.log(df['OriginalColumn'] + small_constant)\n",
    "train_encoded['Log_DisbursementGross'] = np.log1p(train_encoded['DisbursementGross'])\n",
    "train_encoded['Log_GrAppv'] = np.log1p(train_encoded['GrAppv'])\n",
    "train_encoded['Log_SBA_Appv'] = np.log1p(train_encoded['SBA_Appv'])\n",
    "train_encoded['Log_BalanceGross'] = np.log1p(train_encoded['BalanceGross'])\n",
    "train_encoded['TotalJobs'] = train_encoded['CreateJob'] + train_encoded['RetainedJob']\n",
    "#train_encoded['Loan_Efficiency'] = train_encoded['DisbursementGross'] / (train_encoded['CreateJob'] + train_encoded['RetainedJob'] + 1)\n",
    "# Calculate 'LoanToIncomeRatio' as a ratio of 'SBA_Appv' to 'DisbursementGross'\n",
    "train_encoded['IncomeToLoanRatio'] = train_encoded['DisbursementGross'] / train_encoded['SBA_Appv']\n",
    "# Calculate 'LoanToEmployeesRatio' as a ratio of 'SBA_Appv' to 'NoEmp'\n",
    "train_encoded['EmployeesToLoanRatio'] = train_encoded['NoEmp'] / train_encoded['SBA_Appv']\n",
    "# Create a binary feature to indicate loans with a balance ('BalanceGross' > 0)\n",
    "#train_encoded['HasBalance'] = (train_encoded['BalanceGross'] > 0).astype(int)\n",
    "# Calculate 'LoanPerJob' as a ratio of 'SBA_Appv' to 'TotalJobs'\n",
    "train_encoded['JobPerLoan'] = train_encoded['TotalJobs'] / train_encoded['SBA_Appv'] \n",
    "# Calculate SBA's Gaurenteed Portion of Approved Loan\n",
    "train_encoded['Gauren_SBA_Appv'] = train_encoded['GrAppv'] / train_encoded['SBA_Appv']\n",
    "# Filter the DataFrame to include only the relevant rows\n",
    "default_group = train_encoded[train_encoded['MIS_Status'].isin([0, 1])]\n",
    "# Calculate the total number of loans in the filtered group\n",
    "total_loans = len(default_group)\n",
    "# Calculate the number of defaults (CHGOFF) in the filtered group\n",
    "default_loans = len(default_group[default_group['MIS_Status'] == 1])\n",
    "# Calculate the default rate as a percentage\n",
    "default_rate = (default_loans / total_loans) * 100\n",
    "# Create a new feature 'DefaultRate' with the calculated default rate\n",
    "train_encoded['DefaultRate'] = default_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['City', 'State', 'Zip', 'Bank', 'BankState', 'NAICS', 'NoEmp',\n",
       "       'NewExist', 'CreateJob', 'RetainedJob', 'FranchiseCode', 'UrbanRural',\n",
       "       'RevLineCr', 'LowDoc', 'DisbursementGross', 'BalanceGross', 'GrAppv',\n",
       "       'SBA_Appv', 'MIS_Status', 'Log_DisbursementGross', 'Log_GrAppv',\n",
       "       'Log_SBA_Appv', 'Log_BalanceGross', 'TotalJobs', 'IncomeToLoanRatio',\n",
       "       'EmployeesToLoanRatio', 'JobPerLoan', 'Gauren_SBA_Appv', 'DefaultRate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encoded.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Zip</th>\n",
       "      <th>Bank</th>\n",
       "      <th>BankState</th>\n",
       "      <th>NAICS</th>\n",
       "      <th>NoEmp</th>\n",
       "      <th>NewExist</th>\n",
       "      <th>CreateJob</th>\n",
       "      <th>RetainedJob</th>\n",
       "      <th>FranchiseCode</th>\n",
       "      <th>UrbanRural</th>\n",
       "      <th>RevLineCr</th>\n",
       "      <th>LowDoc</th>\n",
       "      <th>DisbursementGross</th>\n",
       "      <th>BalanceGross</th>\n",
       "      <th>GrAppv</th>\n",
       "      <th>SBA_Appv</th>\n",
       "      <th>MIS_Status</th>\n",
       "      <th>Log_DisbursementGross</th>\n",
       "      <th>Log_GrAppv</th>\n",
       "      <th>Log_SBA_Appv</th>\n",
       "      <th>Log_BalanceGross</th>\n",
       "      <th>TotalJobs</th>\n",
       "      <th>IncomeToLoanRatio</th>\n",
       "      <th>EmployeesToLoanRatio</th>\n",
       "      <th>JobPerLoan</th>\n",
       "      <th>Gauren_SBA_Appv</th>\n",
       "      <th>DefaultRate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>217613</th>\n",
       "      <td>0.114650</td>\n",
       "      <td>0.184773</td>\n",
       "      <td>93001</td>\n",
       "      <td>0.031447</td>\n",
       "      <td>0.218517</td>\n",
       "      <td>235910</td>\n",
       "      <td>0.600407</td>\n",
       "      <td>0.17044</td>\n",
       "      <td>-0.035373</td>\n",
       "      <td>-0.045454</td>\n",
       "      <td>1</td>\n",
       "      <td>0.071674</td>\n",
       "      <td>0.15307</td>\n",
       "      <td>0.187063</td>\n",
       "      <td>0.358949</td>\n",
       "      <td>-0.002296</td>\n",
       "      <td>0.394801</td>\n",
       "      <td>0.410973</td>\n",
       "      <td>0</td>\n",
       "      <td>0.306712</td>\n",
       "      <td>0.332752</td>\n",
       "      <td>0.344279</td>\n",
       "      <td>-0.002298</td>\n",
       "      <td>-0.080828</td>\n",
       "      <td>0.873414</td>\n",
       "      <td>1.460941</td>\n",
       "      <td>-0.196674</td>\n",
       "      <td>0.960651</td>\n",
       "      <td>17.509603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95730</th>\n",
       "      <td>0.137597</td>\n",
       "      <td>0.165992</td>\n",
       "      <td>44039</td>\n",
       "      <td>0.128698</td>\n",
       "      <td>0.159167</td>\n",
       "      <td>484121</td>\n",
       "      <td>-0.153600</td>\n",
       "      <td>0.17044</td>\n",
       "      <td>-0.035373</td>\n",
       "      <td>-0.045454</td>\n",
       "      <td>0</td>\n",
       "      <td>0.243491</td>\n",
       "      <td>0.15307</td>\n",
       "      <td>0.187063</td>\n",
       "      <td>-0.614207</td>\n",
       "      <td>-0.002296</td>\n",
       "      <td>-0.594552</td>\n",
       "      <td>-0.600302</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.952455</td>\n",
       "      <td>-0.902762</td>\n",
       "      <td>-0.917047</td>\n",
       "      <td>-0.002298</td>\n",
       "      <td>-0.080828</td>\n",
       "      <td>1.023163</td>\n",
       "      <td>0.255872</td>\n",
       "      <td>0.134645</td>\n",
       "      <td>0.990421</td>\n",
       "      <td>17.509603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780446</th>\n",
       "      <td>0.139151</td>\n",
       "      <td>0.116799</td>\n",
       "      <td>68122</td>\n",
       "      <td>0.175694</td>\n",
       "      <td>0.159167</td>\n",
       "      <td>451120</td>\n",
       "      <td>0.115688</td>\n",
       "      <td>0.17044</td>\n",
       "      <td>-0.013978</td>\n",
       "      <td>0.018584</td>\n",
       "      <td>38510</td>\n",
       "      <td>0.243491</td>\n",
       "      <td>0.15307</td>\n",
       "      <td>0.187063</td>\n",
       "      <td>2.317036</td>\n",
       "      <td>-0.002296</td>\n",
       "      <td>2.751996</td>\n",
       "      <td>2.533233</td>\n",
       "      <td>0</td>\n",
       "      <td>1.199072</td>\n",
       "      <td>1.322288</td>\n",
       "      <td>1.262213</td>\n",
       "      <td>-0.002298</td>\n",
       "      <td>0.004606</td>\n",
       "      <td>0.914656</td>\n",
       "      <td>0.045668</td>\n",
       "      <td>0.001818</td>\n",
       "      <td>1.086357</td>\n",
       "      <td>17.509603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337263</th>\n",
       "      <td>0.140704</td>\n",
       "      <td>0.197662</td>\n",
       "      <td>14208</td>\n",
       "      <td>0.118949</td>\n",
       "      <td>0.167297</td>\n",
       "      <td>321114</td>\n",
       "      <td>0.519620</td>\n",
       "      <td>0.17044</td>\n",
       "      <td>-0.035373</td>\n",
       "      <td>-0.045454</td>\n",
       "      <td>1</td>\n",
       "      <td>0.071674</td>\n",
       "      <td>0.15307</td>\n",
       "      <td>0.187063</td>\n",
       "      <td>-0.004686</td>\n",
       "      <td>-0.002296</td>\n",
       "      <td>0.025114</td>\n",
       "      <td>0.001232</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.004697</td>\n",
       "      <td>0.024804</td>\n",
       "      <td>0.001231</td>\n",
       "      <td>-0.002298</td>\n",
       "      <td>-0.080828</td>\n",
       "      <td>-3.803563</td>\n",
       "      <td>421.786886</td>\n",
       "      <td>-65.609474</td>\n",
       "      <td>20.385638</td>\n",
       "      <td>17.509603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199634</th>\n",
       "      <td>0.140354</td>\n",
       "      <td>0.144273</td>\n",
       "      <td>16335</td>\n",
       "      <td>0.193277</td>\n",
       "      <td>0.078307</td>\n",
       "      <td>0</td>\n",
       "      <td>0.102223</td>\n",
       "      <td>0.17044</td>\n",
       "      <td>-0.035373</td>\n",
       "      <td>-0.045454</td>\n",
       "      <td>1</td>\n",
       "      <td>0.071674</td>\n",
       "      <td>0.15307</td>\n",
       "      <td>0.187063</td>\n",
       "      <td>0.584056</td>\n",
       "      <td>-0.002296</td>\n",
       "      <td>0.623655</td>\n",
       "      <td>0.637638</td>\n",
       "      <td>0</td>\n",
       "      <td>0.459989</td>\n",
       "      <td>0.484680</td>\n",
       "      <td>0.493255</td>\n",
       "      <td>-0.002298</td>\n",
       "      <td>-0.080828</td>\n",
       "      <td>0.915969</td>\n",
       "      <td>0.160316</td>\n",
       "      <td>-0.126761</td>\n",
       "      <td>0.978071</td>\n",
       "      <td>17.509603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            City     State    Zip      Bank  BankState   NAICS     NoEmp  \\\n",
       "217613  0.114650  0.184773  93001  0.031447   0.218517  235910  0.600407   \n",
       "95730   0.137597  0.165992  44039  0.128698   0.159167  484121 -0.153600   \n",
       "780446  0.139151  0.116799  68122  0.175694   0.159167  451120  0.115688   \n",
       "337263  0.140704  0.197662  14208  0.118949   0.167297  321114  0.519620   \n",
       "199634  0.140354  0.144273  16335  0.193277   0.078307       0  0.102223   \n",
       "\n",
       "        NewExist  CreateJob  RetainedJob  FranchiseCode  UrbanRural  \\\n",
       "217613   0.17044  -0.035373    -0.045454              1    0.071674   \n",
       "95730    0.17044  -0.035373    -0.045454              0    0.243491   \n",
       "780446   0.17044  -0.013978     0.018584          38510    0.243491   \n",
       "337263   0.17044  -0.035373    -0.045454              1    0.071674   \n",
       "199634   0.17044  -0.035373    -0.045454              1    0.071674   \n",
       "\n",
       "        RevLineCr    LowDoc  DisbursementGross  BalanceGross    GrAppv  \\\n",
       "217613    0.15307  0.187063           0.358949     -0.002296  0.394801   \n",
       "95730     0.15307  0.187063          -0.614207     -0.002296 -0.594552   \n",
       "780446    0.15307  0.187063           2.317036     -0.002296  2.751996   \n",
       "337263    0.15307  0.187063          -0.004686     -0.002296  0.025114   \n",
       "199634    0.15307  0.187063           0.584056     -0.002296  0.623655   \n",
       "\n",
       "        SBA_Appv  MIS_Status  Log_DisbursementGross  Log_GrAppv  Log_SBA_Appv  \\\n",
       "217613  0.410973           0               0.306712    0.332752      0.344279   \n",
       "95730  -0.600302           0              -0.952455   -0.902762     -0.917047   \n",
       "780446  2.533233           0               1.199072    1.322288      1.262213   \n",
       "337263  0.001232           0              -0.004697    0.024804      0.001231   \n",
       "199634  0.637638           0               0.459989    0.484680      0.493255   \n",
       "\n",
       "        Log_BalanceGross  TotalJobs  IncomeToLoanRatio  EmployeesToLoanRatio  \\\n",
       "217613         -0.002298  -0.080828           0.873414              1.460941   \n",
       "95730          -0.002298  -0.080828           1.023163              0.255872   \n",
       "780446         -0.002298   0.004606           0.914656              0.045668   \n",
       "337263         -0.002298  -0.080828          -3.803563            421.786886   \n",
       "199634         -0.002298  -0.080828           0.915969              0.160316   \n",
       "\n",
       "        JobPerLoan  Gauren_SBA_Appv  DefaultRate  \n",
       "217613   -0.196674         0.960651    17.509603  \n",
       "95730     0.134645         0.990421    17.509603  \n",
       "780446    0.001818         1.086357    17.509603  \n",
       "337263  -65.609474        20.385638    17.509603  \n",
       "199634   -0.126761         0.978071    17.509603  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            City     State    Zip      Bank  BankState   NAICS     NoEmp  \\\n",
      "217613  0.114650  0.184773  93001  0.031447   0.218517  235910  0.600407   \n",
      "95730   0.137597  0.165992  44039  0.128698   0.159167  484121 -0.153600   \n",
      "780446  0.139151  0.116799  68122  0.175694   0.159167  451120  0.115688   \n",
      "337263  0.140704  0.197662  14208  0.118949   0.167297  321114  0.519620   \n",
      "199634  0.140354  0.144273  16335  0.193277   0.078307       0  0.102223   \n",
      "\n",
      "        NewExist  CreateJob  RetainedJob  FranchiseCode  UrbanRural  \\\n",
      "217613   0.17044  -0.035373    -0.045454              1    0.071674   \n",
      "95730    0.17044  -0.035373    -0.045454              0    0.243491   \n",
      "780446   0.17044  -0.013978     0.018584          38510    0.243491   \n",
      "337263   0.17044  -0.035373    -0.045454              1    0.071674   \n",
      "199634   0.17044  -0.035373    -0.045454              1    0.071674   \n",
      "\n",
      "        RevLineCr    LowDoc  DisbursementGross  BalanceGross    GrAppv  \\\n",
      "217613    0.15307  0.187063           0.358949     -0.002296  0.394801   \n",
      "95730     0.15307  0.187063          -0.614207     -0.002296 -0.594552   \n",
      "780446    0.15307  0.187063           2.317036     -0.002296  2.751996   \n",
      "337263    0.15307  0.187063          -0.004686     -0.002296  0.025114   \n",
      "199634    0.15307  0.187063           0.584056     -0.002296  0.623655   \n",
      "\n",
      "        SBA_Appv  MIS_Status  Log_DisbursementGross  Log_GrAppv  Log_SBA_Appv  \\\n",
      "217613  0.410973           0               0.306712    0.332752      0.344279   \n",
      "95730  -0.600302           0              -0.952455   -0.902762     -0.917047   \n",
      "780446  2.533233           0               1.199072    1.322288      1.262213   \n",
      "337263  0.001232           0              -0.004697    0.024804      0.001231   \n",
      "199634  0.637638           0               0.459989    0.484680      0.493255   \n",
      "\n",
      "        Log_BalanceGross  TotalJobs  IncomeToLoanRatio  EmployeesToLoanRatio  \\\n",
      "217613         -0.002298  -0.080828           0.873414              1.460941   \n",
      "95730          -0.002298  -0.080828           1.023163              0.255872   \n",
      "780446         -0.002298   0.004606           0.914656              0.045668   \n",
      "337263         -0.002298  -0.080828          -3.803563            421.786886   \n",
      "199634         -0.002298  -0.080828           0.915969              0.160316   \n",
      "\n",
      "        JobPerLoan  Gauren_SBA_Appv  DefaultRate  \n",
      "217613   -0.196674         0.960651    17.509603  \n",
      "95730     0.134645         0.990421    17.509603  \n",
      "780446    0.001818         1.086357    17.509603  \n",
      "337263  -65.609474        20.385638    17.509603  \n",
      "199634   -0.126761         0.978071    17.509603  \n"
     ]
    }
   ],
   "source": [
    "print(train_encoded.head())\n",
    "h2o_df = train_encoded.copy()\n",
    "# Saving  as a h2o_df.csv which will be imported directly in h2O for training as all encoding, scaling and feature extraction has already been done during Scikit learn model\n",
    "h2o_df.to_csv('D:/Work/Gre/UTD/Courses/Fall/MIS6341/Softwares/Python/ml-fall-2023/Project1/h2o_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on following columns: ['NAICS', 'NoEmp', 'NewExist', 'CreateJob', 'RetainedJob', 'FranchiseCode', 'UrbanRural', 'DisbursementGross', 'BalanceGross', 'GrAppv', 'SBA_Appv', 'Log_DisbursementGross', 'Log_GrAppv', 'Log_SBA_Appv', 'Log_BalanceGross', 'TotalJobs', 'IncomeToLoanRatio', 'EmployeesToLoanRatio', 'JobPerLoan', 'Gauren_SBA_Appv', 'DefaultRate']\n",
      "Best parameters found:  {'C': 10, 'l1_ratio': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best cross-validation score: 0.82\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "target_col = \"MIS_Status\"\n",
    "cols_to_drop = ['City', 'State', 'Zip','Bank', 'BankState', 'LowDoc','RevLineCr','MIS_Status', 'index']\n",
    "y = train_encoded[target_col]\n",
    "X = train_encoded.drop(columns=[target_col])\n",
    "\n",
    "param_grid = {'C':[10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "                'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "                'solver': ['lbfgs'],\n",
    "                 'l1_ratio': [0.1, 0.3, 0.7] # 'newton-cholesky', 'sag'\n",
    "                }\n",
    "\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000, random_state=0, n_jobs=-1)\n",
    "columns_to_train = [x for x in X.columns if x not in cols_to_drop]\n",
    "print(\"Training on following columns:\", columns_to_train)\n",
    "clf.fit(X[columns_to_train], y)\n",
    "grid1 = GridSearchCV(clf.fit(X[columns_to_train], y), \n",
    "                    param_grid, cv =7 , return_train_score= True)\n",
    "grid1.fit(X[columns_to_train], y)\n",
    "print(\"Best parameters found: \", grid1.best_params_)\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid1.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Hyperparameters Found:\n",
    "\n",
    "    'C': 10  \n",
    "    In logistic regression, the parameter 'C' is the regularization strength or the inverse of the regularization parameter. It controls the trade-off between fitting the training data well and preventing overfitting.\n",
    "A smaller 'C' value increases the regularization strength, leading to simpler models with smaller coefficients. This helps to prevent overfitting but may sacrifice some accuracy on the training data.\n",
    "A larger 'C' value decreases the regularization strength, allowing the model to fit the training data more closely. However, this may lead to overfitting.\n",
    "\n",
    "\n",
    "    'penalty': 'l2'\n",
    "        L2 regularization is a type of regularization used in logistic regression. It adds a penalty term to the loss function, which encourages the model to keep all feature weights small.\n",
    "    The penalty term is proportional to the square of the magnitude of feature coefficients. It helps prevent overfitting by reducing the influence of individual features in the model.\n",
    "    L2 regularization is controlled by the 'C' parameter, with a smaller 'C' increasing the strength of regularization and leading to smaller feature coefficients.\n",
    "    'solver': 'lbfgs'\n",
    "    LBFGS is an optimization algorithm used for logistic regression and other machine learning models.\n",
    "It's an iterative numerical optimization algorithm that finds the optimal values of the model parameters (coefficients) by minimizing the logistic regression loss function.\n",
    "LBFGS is suitable for problems with a large number of features, and it efficiently approximates the Hessian matrix, which is used to update the model parameters.\n",
    "It is one of the solvers available for logistic regression in scikit-learn and is known for its efficiency and effectiveness.\n",
    "\n",
    "Best Cross-Validation Score:\n",
    "\n",
    "    The best cross-validation score achieved by the model is 0.82."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Assuming you have a variable named 'best_params' containing the best hyperparameters\n",
    "best_params = grid1.best_params_\n",
    "\n",
    "with open('best_params.pkl', 'wb') as f:\n",
    "    pickle.dump(best_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "def calculate_optimal_threshold(classifier, X, y):\n",
    "    # Predict probabilities\n",
    "    y_prob = classifier.predict_proba(X)[:, 1]\n",
    "    \n",
    "    # Generate a range of thresholds\n",
    "    thresholds = np.linspace(0, 1, 100)\n",
    "    \n",
    "    # Compute F1 scores for different thresholds\n",
    "    f1_scores = [f1_score(y, (y_prob > threshold).astype(int), average='macro') for threshold in thresholds]\n",
    "    \n",
    "    # Find the optimal threshold with the highest F1 score\n",
    "    optimal_threshold = thresholds[np.argmax(f1_scores)]\n",
    "    \n",
    "    return optimal_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following parameters for controlling regularization strength and type:\n",
    "\n",
    "C: This parameter controls the inverse of the regularization strength in logistic regression models. A smaller C value increases the regularization strength, while a larger C value reduces it.\n",
    "\n",
    "penalty: This parameter specifies the type of regularization penalty applied. You have options for 'l2' (ridge), 'l1' (lasso), and 'elasticnet' penalties.\n",
    "\n",
    "'solver': The 'lbfgs' solver is chosen for optimization\n",
    "\n",
    "For 'l2' (ridge) regularization, the regularization term is typically represented as 'alpha' in some libraries (e.g., scikit-learn). In our case, it is controlled indirectly through the penalty and C parameters.\n",
    "\n",
    "For 'elasticnet' regularization, 'alpha' is a parameter that controls the balance between 'l1' and 'l2' regularization, and 'lambda' is the regularization strength. However, in your parameter grid, these are not explicitly defined.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Lasso (L1 Regularization):\n",
    "\n",
    "When alpha is set to 0, there is no regularization, and the model aims to fit the training data as closely as possible, potentially leading to overfitting.\n",
    "\n",
    "As alpha increases, the amount of L1 regularization also increases, causing some feature coefficients to be pushed to zero. This results in feature selection and simplifies the model.\n",
    "\n",
    "A higher alpha value indicates stronger regularization, which can help prevent overfitting by limiting the impact of individual features\n",
    "\n",
    "\n",
    "For Ridge (L2 Regularization):\n",
    "\n",
    "Similar to Lasso, when alpha is set to 0, there is no regularization, and the model aims to fit the data closely.\n",
    "\n",
    "As alpha increases, the amount of L2 regularization increases, causing feature coefficients to shrink but not necessarily become zero. This results in a smoother and more stable model.\n",
    "\n",
    "A higher alpha value indicates stronger regularization and helps prevent overfitting by reducing the influence of individual features.\n",
    "\n",
    "In practice, the choice of alpha depends on the specific problem and dataset. You typically perform hyperparameter tuning to find the optimal alpha value that balances model complexity and performance.\n",
    "\n",
    "lasso model <alpha = 0.1> Approx\n",
    "ridge model <alpha = 1.0> Approx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H2O Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h2o\n",
    "try:\n",
    "    h2o.cluster().shutdown()\n",
    "except:\n",
    "    pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "; Java HotSpot(TM) 64-Bit Server VM (build 25.361-b09, mixed mode)\n",
      "  Starting server from D:\\Work\\Gre\\UTD\\Courses\\Fall\\MIS6341\\Softwares\\Python\\ml-fall-2023\\Lib\\site-packages\\h2o\\backend\\bin\\h2o.jar\n",
      "  Ice root: C:\\Users\\Asus\\AppData\\Local\\Temp\\tmp0ch2yx2u\n",
      "  JVM stdout: C:\\Users\\Asus\\AppData\\Local\\Temp\\tmp0ch2yx2u\\h2o_Asus_started_from_python.out\n",
      "  JVM stderr: C:\\Users\\Asus\\AppData\\Local\\Temp\\tmp0ch2yx2u\\h2o_Asus_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "\n",
       "#h2o-table-1.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-1 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-1 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-1 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-1 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-1 .h2o-table th,\n",
       "#h2o-table-1 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-1 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-1\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption></caption>\n",
       "    <thead></thead>\n",
       "    <tbody><tr><td>H2O_cluster_uptime:</td>\n",
       "<td>10 secs</td></tr>\n",
       "<tr><td>H2O_cluster_timezone:</td>\n",
       "<td>America/Chicago</td></tr>\n",
       "<tr><td>H2O_data_parsing_timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O_cluster_version:</td>\n",
       "<td>3.42.0.3</td></tr>\n",
       "<tr><td>H2O_cluster_version_age:</td>\n",
       "<td>2 months and 14 days</td></tr>\n",
       "<tr><td>H2O_cluster_name:</td>\n",
       "<td>H2O_from_python_Asus_kzk1ma</td></tr>\n",
       "<tr><td>H2O_cluster_total_nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O_cluster_free_memory:</td>\n",
       "<td>3.556 Gb</td></tr>\n",
       "<tr><td>H2O_cluster_total_cores:</td>\n",
       "<td>16</td></tr>\n",
       "<tr><td>H2O_cluster_allowed_cores:</td>\n",
       "<td>16</td></tr>\n",
       "<tr><td>H2O_cluster_status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O_connection_url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O_connection_proxy:</td>\n",
       "<td>{\"http\": null, \"https\": null}</td></tr>\n",
       "<tr><td>H2O_internal_security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>Python_version:</td>\n",
       "<td>3.10.11 final</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n"
      ],
      "text/plain": [
       "--------------------------  -----------------------------\n",
       "H2O_cluster_uptime:         10 secs\n",
       "H2O_cluster_timezone:       America/Chicago\n",
       "H2O_data_parsing_timezone:  UTC\n",
       "H2O_cluster_version:        3.42.0.3\n",
       "H2O_cluster_version_age:    2 months and 14 days\n",
       "H2O_cluster_name:           H2O_from_python_Asus_kzk1ma\n",
       "H2O_cluster_total_nodes:    1\n",
       "H2O_cluster_free_memory:    3.556 Gb\n",
       "H2O_cluster_total_cores:    16\n",
       "H2O_cluster_allowed_cores:  16\n",
       "H2O_cluster_status:         locked, healthy\n",
       "H2O_connection_url:         http://127.0.0.1:54321\n",
       "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
       "H2O_internal_security:      False\n",
       "Python_version:             3.10.11 final\n",
       "--------------------------  -----------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from h2o.frame import H2OFrame\n",
    "h2o.init(max_mem_size = \"4G\", nthreads=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import an H2O DataFrame (df_h) from a CSV file located at the specified path and then display its first few rows using df_h.head(). It is imported like a\n",
    "a pandas data frame, only difference being that it resides inside a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: || (done) 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class='dataframe'>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">     City</th><th style=\"text-align: right;\">    State</th><th style=\"text-align: right;\">  Zip</th><th style=\"text-align: right;\">     Bank</th><th style=\"text-align: right;\">  BankState</th><th style=\"text-align: right;\">  NAICS</th><th style=\"text-align: right;\">     NoEmp</th><th style=\"text-align: right;\">  NewExist</th><th style=\"text-align: right;\">  CreateJob</th><th style=\"text-align: right;\">  RetainedJob</th><th style=\"text-align: right;\">  FranchiseCode</th><th style=\"text-align: right;\">  UrbanRural</th><th style=\"text-align: right;\">  RevLineCr</th><th style=\"text-align: right;\">   LowDoc</th><th style=\"text-align: right;\">  DisbursementGross</th><th style=\"text-align: right;\">  BalanceGross</th><th style=\"text-align: right;\">    GrAppv</th><th style=\"text-align: right;\">   SBA_Appv</th><th style=\"text-align: right;\">  MIS_Status</th><th style=\"text-align: right;\">  Log_DisbursementGross</th><th style=\"text-align: right;\">  Log_GrAppv</th><th style=\"text-align: right;\">  Log_SBA_Appv</th><th style=\"text-align: right;\">  Log_BalanceGross</th><th style=\"text-align: right;\">  TotalJobs</th><th style=\"text-align: right;\">  IncomeToLoanRatio</th><th style=\"text-align: right;\">  EmployeesToLoanRatio</th><th style=\"text-align: right;\">  JobPerLoan</th><th style=\"text-align: right;\">  Gauren_SBA_Appv</th><th style=\"text-align: right;\">  DefaultRate</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">0.11465  </td><td style=\"text-align: right;\">0.184773 </td><td style=\"text-align: right;\">93001</td><td style=\"text-align: right;\">0.0314465</td><td style=\"text-align: right;\">  0.218517 </td><td style=\"text-align: right;\"> 235910</td><td style=\"text-align: right;\"> 0.600407 </td><td style=\"text-align: right;\">  0.17044 </td><td style=\"text-align: right;\"> -0.0353733</td><td style=\"text-align: right;\">   -0.0454543</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">   0.0716743</td><td style=\"text-align: right;\">   0.15307 </td><td style=\"text-align: right;\">0.187063 </td><td style=\"text-align: right;\">          0.358949 </td><td style=\"text-align: right;\">   -0.00229552</td><td style=\"text-align: right;\"> 0.394801 </td><td style=\"text-align: right;\"> 0.410973  </td><td style=\"text-align: right;\">           0</td><td style=\"text-align: right;\">             0.306712  </td><td style=\"text-align: right;\">   0.332752 </td><td style=\"text-align: right;\">    0.344279  </td><td style=\"text-align: right;\">       -0.00229816</td><td style=\"text-align: right;\">-0.0808276 </td><td style=\"text-align: right;\">           0.873414</td><td style=\"text-align: right;\">             1.46094  </td><td style=\"text-align: right;\"> -0.196674  </td><td style=\"text-align: right;\">         0.960651</td><td style=\"text-align: right;\">      17.5096</td></tr>\n",
       "<tr><td style=\"text-align: right;\">0.137597 </td><td style=\"text-align: right;\">0.165992 </td><td style=\"text-align: right;\">44039</td><td style=\"text-align: right;\">0.128698 </td><td style=\"text-align: right;\">  0.159167 </td><td style=\"text-align: right;\"> 484121</td><td style=\"text-align: right;\">-0.1536   </td><td style=\"text-align: right;\">  0.17044 </td><td style=\"text-align: right;\"> -0.0353733</td><td style=\"text-align: right;\">   -0.0454543</td><td style=\"text-align: right;\">              0</td><td style=\"text-align: right;\">   0.243491 </td><td style=\"text-align: right;\">   0.15307 </td><td style=\"text-align: right;\">0.187063 </td><td style=\"text-align: right;\">         -0.614207 </td><td style=\"text-align: right;\">   -0.00229552</td><td style=\"text-align: right;\">-0.594552 </td><td style=\"text-align: right;\">-0.600302  </td><td style=\"text-align: right;\">           0</td><td style=\"text-align: right;\">            -0.952455  </td><td style=\"text-align: right;\">  -0.902762 </td><td style=\"text-align: right;\">   -0.917047  </td><td style=\"text-align: right;\">       -0.00229816</td><td style=\"text-align: right;\">-0.0808276 </td><td style=\"text-align: right;\">           1.02316 </td><td style=\"text-align: right;\">             0.255872 </td><td style=\"text-align: right;\">  0.134645  </td><td style=\"text-align: right;\">         0.990421</td><td style=\"text-align: right;\">      17.5096</td></tr>\n",
       "<tr><td style=\"text-align: right;\">0.139151 </td><td style=\"text-align: right;\">0.116799 </td><td style=\"text-align: right;\">68122</td><td style=\"text-align: right;\">0.175694 </td><td style=\"text-align: right;\">  0.159167 </td><td style=\"text-align: right;\"> 451120</td><td style=\"text-align: right;\"> 0.115688 </td><td style=\"text-align: right;\">  0.17044 </td><td style=\"text-align: right;\"> -0.0139777</td><td style=\"text-align: right;\">    0.0185835</td><td style=\"text-align: right;\">          38510</td><td style=\"text-align: right;\">   0.243491 </td><td style=\"text-align: right;\">   0.15307 </td><td style=\"text-align: right;\">0.187063 </td><td style=\"text-align: right;\">          2.31704  </td><td style=\"text-align: right;\">   -0.00229552</td><td style=\"text-align: right;\"> 2.752    </td><td style=\"text-align: right;\"> 2.53323   </td><td style=\"text-align: right;\">           0</td><td style=\"text-align: right;\">             1.19907   </td><td style=\"text-align: right;\">   1.32229  </td><td style=\"text-align: right;\">    1.26221   </td><td style=\"text-align: right;\">       -0.00229816</td><td style=\"text-align: right;\"> 0.00460579</td><td style=\"text-align: right;\">           0.914656</td><td style=\"text-align: right;\">             0.045668 </td><td style=\"text-align: right;\">  0.00181815</td><td style=\"text-align: right;\">         1.08636 </td><td style=\"text-align: right;\">      17.5096</td></tr>\n",
       "<tr><td style=\"text-align: right;\">0.140704 </td><td style=\"text-align: right;\">0.197662 </td><td style=\"text-align: right;\">14208</td><td style=\"text-align: right;\">0.118949 </td><td style=\"text-align: right;\">  0.167297 </td><td style=\"text-align: right;\"> 321114</td><td style=\"text-align: right;\"> 0.51962  </td><td style=\"text-align: right;\">  0.17044 </td><td style=\"text-align: right;\"> -0.0353733</td><td style=\"text-align: right;\">   -0.0454543</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">   0.0716743</td><td style=\"text-align: right;\">   0.15307 </td><td style=\"text-align: right;\">0.187063 </td><td style=\"text-align: right;\">         -0.0046858</td><td style=\"text-align: right;\">   -0.00229552</td><td style=\"text-align: right;\"> 0.0251141</td><td style=\"text-align: right;\"> 0.00123195</td><td style=\"text-align: right;\">           0</td><td style=\"text-align: right;\">            -0.00469681</td><td style=\"text-align: right;\">   0.0248039</td><td style=\"text-align: right;\">    0.00123119</td><td style=\"text-align: right;\">       -0.00229816</td><td style=\"text-align: right;\">-0.0808276 </td><td style=\"text-align: right;\">          -3.80356 </td><td style=\"text-align: right;\">           421.787    </td><td style=\"text-align: right;\">-65.6095    </td><td style=\"text-align: right;\">        20.3856  </td><td style=\"text-align: right;\">      17.5096</td></tr>\n",
       "<tr><td style=\"text-align: right;\">0.140354 </td><td style=\"text-align: right;\">0.144273 </td><td style=\"text-align: right;\">16335</td><td style=\"text-align: right;\">0.193277 </td><td style=\"text-align: right;\">  0.0783071</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\"> 0.102223 </td><td style=\"text-align: right;\">  0.17044 </td><td style=\"text-align: right;\"> -0.0353733</td><td style=\"text-align: right;\">   -0.0454543</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">   0.0716743</td><td style=\"text-align: right;\">   0.15307 </td><td style=\"text-align: right;\">0.187063 </td><td style=\"text-align: right;\">          0.584056 </td><td style=\"text-align: right;\">   -0.00229552</td><td style=\"text-align: right;\"> 0.623655 </td><td style=\"text-align: right;\"> 0.637638  </td><td style=\"text-align: right;\">           0</td><td style=\"text-align: right;\">             0.459989  </td><td style=\"text-align: right;\">   0.48468  </td><td style=\"text-align: right;\">    0.493255  </td><td style=\"text-align: right;\">       -0.00229816</td><td style=\"text-align: right;\">-0.0808276 </td><td style=\"text-align: right;\">           0.915969</td><td style=\"text-align: right;\">             0.160316 </td><td style=\"text-align: right;\"> -0.126761  </td><td style=\"text-align: right;\">         0.978071</td><td style=\"text-align: right;\">      17.5096</td></tr>\n",
       "<tr><td style=\"text-align: right;\">0.164738 </td><td style=\"text-align: right;\">0.188249 </td><td style=\"text-align: right;\">77381</td><td style=\"text-align: right;\">0.141056 </td><td style=\"text-align: right;\">  0.180023 </td><td style=\"text-align: right;\"> 532490</td><td style=\"text-align: right;\">-0.140136 </td><td style=\"text-align: right;\">  0.17044 </td><td style=\"text-align: right;\"> -0.0268151</td><td style=\"text-align: right;\">   -0.0411851</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">   0.243491 </td><td style=\"text-align: right;\">   0.251569</td><td style=\"text-align: right;\">0.187063 </td><td style=\"text-align: right;\">         -0.607811 </td><td style=\"text-align: right;\">   -0.00229552</td><td style=\"text-align: right;\">-0.608635 </td><td style=\"text-align: right;\">-0.60902   </td><td style=\"text-align: right;\">           0</td><td style=\"text-align: right;\">            -0.93601   </td><td style=\"text-align: right;\">  -0.938115 </td><td style=\"text-align: right;\">   -0.939099  </td><td style=\"text-align: right;\">       -0.00229816</td><td style=\"text-align: right;\">-0.0680002 </td><td style=\"text-align: right;\">           0.998014</td><td style=\"text-align: right;\">             0.230101 </td><td style=\"text-align: right;\">  0.111655  </td><td style=\"text-align: right;\">         0.999368</td><td style=\"text-align: right;\">      17.5096</td></tr>\n",
       "<tr><td style=\"text-align: right;\">0.172811 </td><td style=\"text-align: right;\">0.223814 </td><td style=\"text-align: right;\">48334</td><td style=\"text-align: right;\">0.214858 </td><td style=\"text-align: right;\">  0.198182 </td><td style=\"text-align: right;\"> 811111</td><td style=\"text-align: right;\">-0.0458851</td><td style=\"text-align: right;\">  0.17044 </td><td style=\"text-align: right;\"> -0.0353733</td><td style=\"text-align: right;\">   -0.0113008</td><td style=\"text-align: right;\">              0</td><td style=\"text-align: right;\">   0.243491 </td><td style=\"text-align: right;\">   0.251569</td><td style=\"text-align: right;\">0.187063 </td><td style=\"text-align: right;\">         -0.584843 </td><td style=\"text-align: right;\">   -0.00229552</td><td style=\"text-align: right;\">-0.65335  </td><td style=\"text-align: right;\">-0.636699  </td><td style=\"text-align: right;\">           0</td><td style=\"text-align: right;\">            -0.879098  </td><td style=\"text-align: right;\">  -1.05944  </td><td style=\"text-align: right;\">   -1.01252   </td><td style=\"text-align: right;\">       -0.00229816</td><td style=\"text-align: right;\">-0.0466741 </td><td style=\"text-align: right;\">           0.918554</td><td style=\"text-align: right;\">             0.0720672</td><td style=\"text-align: right;\">  0.0733063 </td><td style=\"text-align: right;\">         1.02615 </td><td style=\"text-align: right;\">      17.5096</td></tr>\n",
       "<tr><td style=\"text-align: right;\">0.275041 </td><td style=\"text-align: right;\">0.184773 </td><td style=\"text-align: right;\">90016</td><td style=\"text-align: right;\">0.182257 </td><td style=\"text-align: right;\">  0.180023 </td><td style=\"text-align: right;\"> 453210</td><td style=\"text-align: right;\">-0.0997428</td><td style=\"text-align: right;\">  0.17044 </td><td style=\"text-align: right;\"> -0.0353733</td><td style=\"text-align: right;\">   -0.0454543</td><td style=\"text-align: right;\">          50564</td><td style=\"text-align: right;\">   0.0716743</td><td style=\"text-align: right;\">   0.15307 </td><td style=\"text-align: right;\">0.0897581</td><td style=\"text-align: right;\">         -0.437584 </td><td style=\"text-align: right;\">   -0.00229552</td><td style=\"text-align: right;\">-0.414989 </td><td style=\"text-align: right;\">-0.391073  </td><td style=\"text-align: right;\">           1</td><td style=\"text-align: right;\">            -0.575514  </td><td style=\"text-align: right;\">  -0.536125 </td><td style=\"text-align: right;\">   -0.496057  </td><td style=\"text-align: right;\">       -0.00229816</td><td style=\"text-align: right;\">-0.0808276 </td><td style=\"text-align: right;\">           1.11893 </td><td style=\"text-align: right;\">             0.255049 </td><td style=\"text-align: right;\">  0.206682  </td><td style=\"text-align: right;\">         1.06116 </td><td style=\"text-align: right;\">      17.5096</td></tr>\n",
       "<tr><td style=\"text-align: right;\">0.170819 </td><td style=\"text-align: right;\">0.179137 </td><td style=\"text-align: right;\">19805</td><td style=\"text-align: right;\">0.214858 </td><td style=\"text-align: right;\">  0.198182 </td><td style=\"text-align: right;\"> 722110</td><td style=\"text-align: right;\">-0.113207 </td><td style=\"text-align: right;\">  0.17044 </td><td style=\"text-align: right;\"> -0.0353733</td><td style=\"text-align: right;\">   -0.0326467</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">   0.243491 </td><td style=\"text-align: right;\">   0.251569</td><td style=\"text-align: right;\">0.187063 </td><td style=\"text-align: right;\">         -0.680008 </td><td style=\"text-align: right;\">   -0.00229552</td><td style=\"text-align: right;\">-0.661447 </td><td style=\"text-align: right;\">-0.641712  </td><td style=\"text-align: right;\">           0</td><td style=\"text-align: right;\">            -1.13946   </td><td style=\"text-align: right;\">  -1.08308  </td><td style=\"text-align: right;\">   -1.02642   </td><td style=\"text-align: right;\">       -0.00229816</td><td style=\"text-align: right;\">-0.06802   </td><td style=\"text-align: right;\">           1.05968 </td><td style=\"text-align: right;\">             0.176414 </td><td style=\"text-align: right;\">  0.105998  </td><td style=\"text-align: right;\">         1.03075 </td><td style=\"text-align: right;\">      17.5096</td></tr>\n",
       "<tr><td style=\"text-align: right;\">0.0820406</td><td style=\"text-align: right;\">0.0793893</td><td style=\"text-align: right;\">85267</td><td style=\"text-align: right;\">0.329864 </td><td style=\"text-align: right;\">  0.0762233</td><td style=\"text-align: right;\"> 484220</td><td style=\"text-align: right;\">-0.140136 </td><td style=\"text-align: right;\">  0.186978</td><td style=\"text-align: right;\"> -0.0310942</td><td style=\"text-align: right;\">   -0.0369159</td><td style=\"text-align: right;\">              1</td><td style=\"text-align: right;\">   0.187265 </td><td style=\"text-align: right;\">   0.15307 </td><td style=\"text-align: right;\">0.187063 </td><td style=\"text-align: right;\">         -0.664423 </td><td style=\"text-align: right;\">   -0.00229552</td><td style=\"text-align: right;\">-0.645604 </td><td style=\"text-align: right;\">-0.631905  </td><td style=\"text-align: right;\">           0</td><td style=\"text-align: right;\">            -1.0919    </td><td style=\"text-align: right;\">  -1.03734  </td><td style=\"text-align: right;\">   -0.999413  </td><td style=\"text-align: right;\">       -0.00229816</td><td style=\"text-align: right;\">-0.0680101 </td><td style=\"text-align: right;\">           1.05146 </td><td style=\"text-align: right;\">             0.221768 </td><td style=\"text-align: right;\">  0.107627  </td><td style=\"text-align: right;\">         1.02168 </td><td style=\"text-align: right;\">      17.5096</td></tr>\n",
       "</tbody>\n",
       "</table><pre style='font-size: smaller; margin-bottom: 1em;'>[10 rows x 29 columns]</pre>"
      ],
      "text/plain": [
       "     City      State    Zip       Bank    BankState    NAICS       NoEmp    NewExist    CreateJob    RetainedJob    FranchiseCode    UrbanRural    RevLineCr     LowDoc    DisbursementGross    BalanceGross      GrAppv     SBA_Appv    MIS_Status    Log_DisbursementGross    Log_GrAppv    Log_SBA_Appv    Log_BalanceGross    TotalJobs    IncomeToLoanRatio    EmployeesToLoanRatio    JobPerLoan    Gauren_SBA_Appv    DefaultRate\n",
       "---------  ---------  -----  ---------  -----------  -------  ----------  ----------  -----------  -------------  ---------------  ------------  -----------  ---------  -------------------  --------------  ----------  -----------  ------------  -----------------------  ------------  --------------  ------------------  -----------  -------------------  ----------------------  ------------  -----------------  -------------\n",
       "0.11465    0.184773   93001  0.0314465    0.218517    235910   0.600407     0.17044    -0.0353733     -0.0454543                1     0.0716743     0.15307   0.187063             0.358949      -0.00229552   0.394801    0.410973               0               0.306712       0.332752       0.344279           -0.00229816  -0.0808276              0.873414               1.46094     -0.196674             0.960651        17.5096\n",
       "0.137597   0.165992   44039  0.128698     0.159167    484121  -0.1536       0.17044    -0.0353733     -0.0454543                0     0.243491      0.15307   0.187063            -0.614207      -0.00229552  -0.594552   -0.600302               0              -0.952455      -0.902762      -0.917047           -0.00229816  -0.0808276              1.02316                0.255872     0.134645             0.990421        17.5096\n",
       "0.139151   0.116799   68122  0.175694     0.159167    451120   0.115688     0.17044    -0.0139777      0.0185835            38510     0.243491      0.15307   0.187063             2.31704       -0.00229552   2.752       2.53323                0               1.19907        1.32229        1.26221            -0.00229816   0.00460579             0.914656               0.045668     0.00181815           1.08636         17.5096\n",
       "0.140704   0.197662   14208  0.118949     0.167297    321114   0.51962      0.17044    -0.0353733     -0.0454543                1     0.0716743     0.15307   0.187063            -0.0046858     -0.00229552   0.0251141   0.00123195             0              -0.00469681     0.0248039      0.00123119         -0.00229816  -0.0808276             -3.80356              421.787      -65.6095              20.3856          17.5096\n",
       "0.140354   0.144273   16335  0.193277     0.0783071        0   0.102223     0.17044    -0.0353733     -0.0454543                1     0.0716743     0.15307   0.187063             0.584056      -0.00229552   0.623655    0.637638               0               0.459989       0.48468        0.493255           -0.00229816  -0.0808276              0.915969               0.160316    -0.126761             0.978071        17.5096\n",
       "0.164738   0.188249   77381  0.141056     0.180023    532490  -0.140136     0.17044    -0.0268151     -0.0411851                1     0.243491      0.251569  0.187063            -0.607811      -0.00229552  -0.608635   -0.60902                0              -0.93601       -0.938115      -0.939099           -0.00229816  -0.0680002              0.998014               0.230101     0.111655             0.999368        17.5096\n",
       "0.172811   0.223814   48334  0.214858     0.198182    811111  -0.0458851    0.17044    -0.0353733     -0.0113008                0     0.243491      0.251569  0.187063            -0.584843      -0.00229552  -0.65335    -0.636699               0              -0.879098      -1.05944       -1.01252            -0.00229816  -0.0466741              0.918554               0.0720672    0.0733063            1.02615         17.5096\n",
       "0.275041   0.184773   90016  0.182257     0.180023    453210  -0.0997428    0.17044    -0.0353733     -0.0454543            50564     0.0716743     0.15307   0.0897581           -0.437584      -0.00229552  -0.414989   -0.391073               1              -0.575514      -0.536125      -0.496057           -0.00229816  -0.0808276              1.11893                0.255049     0.206682             1.06116         17.5096\n",
       "0.170819   0.179137   19805  0.214858     0.198182    722110  -0.113207     0.17044    -0.0353733     -0.0326467                1     0.243491      0.251569  0.187063            -0.680008      -0.00229552  -0.661447   -0.641712               0              -1.13946       -1.08308       -1.02642            -0.00229816  -0.06802                1.05968                0.176414     0.105998             1.03075         17.5096\n",
       "0.0820406  0.0793893  85267  0.329864     0.0762233   484220  -0.140136     0.186978   -0.0310942     -0.0369159                1     0.187265      0.15307   0.187063            -0.664423      -0.00229552  -0.645604   -0.631905               0              -1.0919        -1.03734       -0.999413           -0.00229816  -0.0680101              1.05146                0.221768     0.107627             1.02168         17.5096\n",
       "[10 rows x 29 columns]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from h2o.estimators.glm import H2OGeneralizedLinearEstimator\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "csv_file_path = 'D:/Work/Gre/UTD/Courses/Fall/MIS6341/Softwares/Python/ml-fall-2023/Project1/h2o_df.csv'\n",
    "\n",
    "df_h = h2o.import_file(csv_file_path)\n",
    "\n",
    "df_h.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 1em 0 1em 0;'>Rows:566472\n",
       "Cols:29\n",
       "</pre>"
      ],
      "text/plain": [
       "Rows:566472\n",
       "Cols:29\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table class='dataframe'>\n",
       "<thead>\n",
       "<tr><th>       </th><th>City                </th><th>State              </th><th>Zip              </th><th>Bank                </th><th>BankState          </th><th>NAICS             </th><th>NoEmp                </th><th>NewExist            </th><th>CreateJob            </th><th>RetainedJob            </th><th>FranchiseCode     </th><th>UrbanRural         </th><th>RevLineCr           </th><th>LowDoc              </th><th>DisbursementGross     </th><th>BalanceGross         </th><th>GrAppv                </th><th>SBA_Appv             </th><th>MIS_Status         </th><th>Log_DisbursementGross  </th><th>Log_GrAppv          </th><th>Log_SBA_Appv        </th><th>Log_BalanceGross      </th><th>TotalJobs             </th><th>IncomeToLoanRatio  </th><th>EmployeesToLoanRatio  </th><th>JobPerLoan           </th><th>Gauren_SBA_Appv   </th><th>DefaultRate          </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>type   </td><td>real                </td><td>real               </td><td>int              </td><td>real                </td><td>real               </td><td>int               </td><td>real                 </td><td>real                </td><td>real                 </td><td>real                   </td><td>int               </td><td>real               </td><td>real                </td><td>real                </td><td>real                  </td><td>real                 </td><td>real                  </td><td>real                 </td><td>int                </td><td>real                   </td><td>real                </td><td>real                </td><td>real                  </td><td>real                  </td><td>real               </td><td>real                  </td><td>real                 </td><td>real              </td><td>real                 </td></tr>\n",
       "<tr><td>mins   </td><td>0.002343996205589325</td><td>0.07006369426751592</td><td>0.0              </td><td>0.0                 </td><td>0.05249080575058509</td><td>0.0               </td><td>-0.153600434175742   </td><td>0.17044022221184818 </td><td>-0.03537327806289165 </td><td>-0.04545428677284537   </td><td>0.0               </td><td>0.07167428097535124</td><td>0.1530699251248866  </td><td>0.08975812818488481 </td><td>-0.6973236731572443   </td><td>-0.002295517679303139</td><td>-0.6783474528695129   </td><td>-0.6521736527433548  </td><td>0.0                </td><td>-1.195091272676935     </td><td>-1.1342833622057995 </td><td>-1.056051925886739  </td><td>-0.002298156418966926 </td><td>-0.08082756483573703  </td><td>-3249.7995325776096</td><td>-6217.269704293639    </td><td>-6946.62619277787    </td><td>-1425.410980018344</td><td>17.509603299015662   </td></tr>\n",
       "<tr><td>mean   </td><td>0.17567877828071593 </td><td>0.1750960329901566 </td><td>53853.91636479826</td><td>0.17619418541215054 </td><td>0.175097323980688  </td><td>398590.7101374825 </td><td>8.12805739334945e-18 </td><td>0.1750960329901566  </td><td>7.224939905199511e-18</td><td>-5.5190513164718486e-18</td><td>2748.614023288001 </td><td>0.17509603299015666</td><td>0.17509603299015658 </td><td>0.17509603299015658 </td><td>-1.204156650866585e-18</td><td>5.218012153755202e-18</td><td>1.7058885887276622e-17</td><td>-6.70313868982399e-17</td><td>0.1750960329901566 </td><td>-0.2785689460749845    </td><td>-0.279916152561005  </td><td>-0.27948106942859474</td><td>-0.0022487117990158925</td><td>1.0636717082654834e-17</td><td>2.112198455611862  </td><td>0.4918000867212119    </td><td>-0.2489405491913917  </td><td>1.496274454678428 </td><td>17.509603299015662   </td></tr>\n",
       "<tr><td>maxs   </td><td>0.6626422957411695  </td><td>0.27204321821338995</td><td>99999.0          </td><td>0.8892988929799549  </td><td>0.3800327332242226 </td><td>928120.0          </td><td>134.4770591134275    </td><td>0.1869783599731616  </td><td>37.62078512466269    </td><td>40.51182843681707      </td><td>92006.0           </td><td>0.2434914068009948 </td><td>0.25156900049734354 </td><td>0.18706258164639383 </td><td>37.39775945774547     </td><td>552.889522851626     </td><td>18.58692184996103     </td><td>23.199528251630852   </td><td>1.0                </td><td>3.6479991104368574     </td><td>2.9748620909714023  </td><td>3.186333139238373   </td><td>6.316965249625691     </td><td>75.14418220289946     </td><td>6396.558729319238  </td><td>37035.12034202456     </td><td>11627.399763221902   </td><td>1043.390760820282 </td><td>17.509603299015662   </td></tr>\n",
       "<tr><td>sigma  </td><td>0.06639769763247672 </td><td>0.04199181924329759</td><td>31185.22931842036</td><td>0.12227701398294173 </td><td>0.07450052965800262</td><td>263321.75739069114</td><td>1.000000882657328    </td><td>0.007437873600660589</td><td>1.0000008826573281   </td><td>1.0000008826573283     </td><td>12747.306218543337</td><td>0.07945017170394587</td><td>0.041041501003144853</td><td>0.031956251232178115</td><td>1.0000008826573277    </td><td>1.00000088265733     </td><td>1.0000008826573281    </td><td>1.0000008826573283   </td><td>0.38004955887207637</td><td>0.6751342455877893     </td><td>0.673495391264487   </td><td>0.6724602419470982  </td><td>0.015557287489121684  </td><td>1.9967248008580234    </td><td>48.40854150735596  </td><td>57.47035513066714     </td><td>29.416621353104784   </td><td>11.623336619659533</td><td>4.396234168225107e-15</td></tr>\n",
       "<tr><td>zeros  </td><td>0                   </td><td>0                  </td><td>176              </td><td>20864               </td><td>0                  </td><td>127266            </td><td>0                    </td><td>0                   </td><td>0                    </td><td>0                      </td><td>131369            </td><td>0                  </td><td>0                   </td><td>0                   </td><td>0                     </td><td>0                    </td><td>0                     </td><td>0                    </td><td>467285             </td><td>0                      </td><td>0                   </td><td>0                   </td><td>0                     </td><td>0                     </td><td>0                  </td><td>0                     </td><td>0                    </td><td>0                 </td><td>0                    </td></tr>\n",
       "<tr><td>missing</td><td>0                   </td><td>0                  </td><td>0                </td><td>0                   </td><td>0                  </td><td>0                 </td><td>0                    </td><td>0                   </td><td>0                    </td><td>0                      </td><td>0                 </td><td>0                  </td><td>0                   </td><td>0                   </td><td>0                     </td><td>0                    </td><td>0                     </td><td>0                    </td><td>0                  </td><td>0                      </td><td>0                   </td><td>0                   </td><td>0                     </td><td>0                     </td><td>0                  </td><td>0                     </td><td>0                    </td><td>0                 </td><td>0                    </td></tr>\n",
       "<tr><td>0      </td><td>0.11464974937637376 </td><td>0.18477258385515263</td><td>93001.0          </td><td>0.031446540880519576</td><td>0.21851689918783204</td><td>235910.0          </td><td>0.6004066600002538   </td><td>0.17044022221184818 </td><td>-0.03537327806289165 </td><td>-0.04545428677284537   </td><td>1.0               </td><td>0.07167428097535124</td><td>0.1530699251248866  </td><td>0.18706258164639383 </td><td>0.35894908638142187   </td><td>-0.002295517679303139</td><td>0.394801069280922     </td><td>0.4109726208502541   </td><td>0.0                </td><td>0.3067116704358818     </td><td>0.3327518024385963  </td><td>0.3442792686074715  </td><td>-0.002298156418966926 </td><td>-0.08082756483573703  </td><td>0.873413624583551  </td><td>1.4609407769259248    </td><td>-0.19667384330497317 </td><td>0.9606505378974516</td><td>17.509603299015662   </td></tr>\n",
       "<tr><td>1      </td><td>0.137596588115639   </td><td>0.1659919028340081 </td><td>44039.0          </td><td>0.12869797760320908 </td><td>0.15916691590309687</td><td>484121.0          </td><td>-0.153600434175742   </td><td>0.17044022221184818 </td><td>-0.03537327806289165 </td><td>-0.04545428677284537   </td><td>0.0               </td><td>0.2434914068009948 </td><td>0.1530699251248866  </td><td>0.18706258164639383 </td><td>-0.6142071281443656   </td><td>-0.002295517679303139</td><td>-0.5945517349325695   </td><td>-0.6003022273404686  </td><td>0.0                </td><td>-0.9524546549496404    </td><td>-0.9027619966303708 </td><td>-0.9170465858109552 </td><td>-0.002298156418966926 </td><td>-0.08082756483573703  </td><td>1.0231631671025114 </td><td>0.25587183785114576   </td><td>0.1346447858336776   </td><td>0.99042067121194  </td><td>17.509603299015662   </td></tr>\n",
       "<tr><td>2      </td><td>0.1391509433962264  </td><td>0.11679920477137178</td><td>68122.0          </td><td>0.1756944756092088  </td><td>0.15916691590309687</td><td>451120.0          </td><td>0.11568781374425648  </td><td>0.17044022221184818 </td><td>-0.013977733515888482</td><td>0.01858352805387556    </td><td>38510.0           </td><td>0.2434914068009948 </td><td>0.1530699251248866  </td><td>0.18706258164639383 </td><td>2.3170363593098213    </td><td>-0.002295517679303139</td><td>2.751995739818049     </td><td>2.533233145474642    </td><td>0.0                </td><td>1.199071721409156      </td><td>1.3222878957003827  </td><td>1.2622133569935532  </td><td>-0.002298156418966926 </td><td>0.00460579453798708   </td><td>0.9146557881768466 </td><td>0.045668048340090896  </td><td>0.0018181486951624856</td><td>1.0863570708974033</td><td>17.509603299015662   </td></tr>\n",
       "<tr><td>3      </td><td>0.1407035175879397  </td><td>0.19766159276417383</td><td>14208.0          </td><td>0.1189488243430152  </td><td>0.16729655837889998</td><td>321114.0          </td><td>0.5196201856242543   </td><td>0.17044022221184818 </td><td>-0.03537327806289165 </td><td>-0.04545428677284537   </td><td>1.0               </td><td>0.07167428097535124</td><td>0.1530699251248866  </td><td>0.18706258164639383 </td><td>-0.004685798049922222 </td><td>-0.002295517679303139</td><td>0.02511407838264226   </td><td>0.0012319496005647338</td><td>0.0                </td><td>-0.0046968108174749315 </td><td>0.024803902380498698</td><td>0.00123119137332351 </td><td>-0.002298156418966926 </td><td>-0.08082756483573703  </td><td>-3.803563106619152 </td><td>421.7868859132362     </td><td>-65.60947363324371   </td><td>20.385637830581544</td><td>17.509603299015662   </td></tr>\n",
       "<tr><td>4      </td><td>0.14035375121577756 </td><td>0.14427252985884909</td><td>16335.0          </td><td>0.19327731092436976 </td><td>0.07830707560361704</td><td>0.0               </td><td>0.10222340134825655  </td><td>0.17044022221184818 </td><td>-0.03537327806289165 </td><td>-0.04545428677284537   </td><td>1.0               </td><td>0.07167428097535124</td><td>0.1530699251248866  </td><td>0.18706258164639383 </td><td>0.5840563957913015    </td><td>-0.002295517679303139</td><td>0.6236549207893809    </td><td>0.6376376730309333   </td><td>0.0                </td><td>0.4599888961620474     </td><td>0.4846797319622258  </td><td>0.4932547601265484  </td><td>-0.002298156418966926 </td><td>-0.08082756483573703  </td><td>0.91596908478613   </td><td>0.16031581205412476   </td><td>-0.12676096199199305 </td><td>0.9780710067284966</td><td>17.509603299015662   </td></tr>\n",
       "<tr><td>5      </td><td>0.16473815924050095 </td><td>0.1882487805974511 </td><td>77381.0          </td><td>0.1410558507971523  </td><td>0.18002315611603092</td><td>532490.0          </td><td>-0.14013602177974208 </td><td>0.17044022221184818 </td><td>-0.02681506024409038 </td><td>-0.041185099117730634  </td><td>1.0               </td><td>0.2434914068009948 </td><td>0.25156900049734354 </td><td>0.18706258164639383 </td><td>-0.6078106173677496   </td><td>-0.002295517679303139</td><td>-0.6086350488715516   </td><td>-0.6090201139628024  </td><td>0.0                </td><td>-0.936010436896243     </td><td>-0.9381147754788066 </td><td>-0.9390991626800832 </td><td>-0.002298156418966926 </td><td>-0.06800015936182102  </td><td>0.998014028490483  </td><td>0.23010081041149533   </td><td>0.11165503043791804  </td><td>0.9993677300922869</td><td>17.509603299015662   </td></tr>\n",
       "<tr><td>6      </td><td>0.1728110599078341  </td><td>0.22381434467720585</td><td>48334.0          </td><td>0.21485771162285663 </td><td>0.1981815002622836 </td><td>811111.0          </td><td>-0.045885135007742606</td><td>0.17044022221184818 </td><td>-0.03537327806289165 </td><td>-0.011300785531927536  </td><td>0.0               </td><td>0.2434914068009948 </td><td>0.25156900049734354 </td><td>0.18706258164639383 </td><td>-0.5848427454291907   </td><td>-0.002295517679303139</td><td>-0.6533495706278197   </td><td>-0.6366994039887123  </td><td>0.0                </td><td>-0.8790979038493719    </td><td>-1.059438414961258  </td><td>-1.012524699207778  </td><td>-0.002298156418966926 </td><td>-0.04667406359481919  </td><td>0.9185539389001204 </td><td>0.07206718699638688   </td><td>0.07330627813128383  </td><td>1.026150749529212 </td><td>17.509603299015662   </td></tr>\n",
       "<tr><td>7      </td><td>0.27504127682993945 </td><td>0.18477258385515263</td><td>90016.0          </td><td>0.182256711409396   </td><td>0.18002315611603092</td><td>453210.0          </td><td>-0.0997427845917423  </td><td>0.17044022221184818 </td><td>-0.03537327806289165 </td><td>-0.04545428677284537   </td><td>50564.0           </td><td>0.07167428097535124</td><td>0.1530699251248866  </td><td>0.08975812818488481 </td><td>-0.4375844699919985   </td><td>-0.002295517679303139</td><td>-0.4149894822105479   </td><td>-0.391072948404457   </td><td>1.0                </td><td>-0.5755143250547002    </td><td>-0.5361254527846345 </td><td>-0.4960568023622707 </td><td>-0.002298156418966926 </td><td>-0.08082756483573703  </td><td>1.1189331089693224 </td><td>0.25504905158662605   </td><td>0.20668155433789612  </td><td>1.0611561958035405</td><td>17.509603299015662   </td></tr>\n",
       "<tr><td>8      </td><td>0.1708185053380783  </td><td>0.17913669064748203</td><td>19805.0          </td><td>0.21485771162285663 </td><td>0.1981815002622836 </td><td>722110.0          </td><td>-0.11320719698774225 </td><td>0.17044022221184818 </td><td>-0.03537327806289165 </td><td>-0.03264672380750118   </td><td>1.0               </td><td>0.2434914068009948 </td><td>0.25156900049734354 </td><td>0.18706258164639383 </td><td>-0.6800077262795612   </td><td>-0.002295517679303139</td><td>-0.6614474761427344   </td><td>-0.6417121887965542  </td><td>0.0                </td><td>-1.1394584281034796    </td><td>-1.0830760321756987 </td><td>-1.0264186736605694 </td><td>-0.002298156418966926 </td><td>-0.06802000187039284  </td><td>1.0596771234076527 </td><td>0.17641428503960202   </td><td>0.10599767786545444  </td><td>1.0307541101614277</td><td>17.509603299015662   </td></tr>\n",
       "<tr><td>9      </td><td>0.0820405814622732  </td><td>0.07938931297709924</td><td>85267.0          </td><td>0.32986399789891396 </td><td>0.07622333751568382</td><td>484220.0          </td><td>-0.14013602177974208 </td><td>0.1869783599731616  </td><td>-0.031094169153491016</td><td>-0.03691591146261591   </td><td>1.0               </td><td>0.18726450640542577</td><td>0.1530699251248866  </td><td>0.18706258164639383 </td><td>-0.6644233740896465   </td><td>-0.002295517679303139</td><td>-0.6456037479613795   </td><td>-0.6319045663464287  </td><td>0.0                </td><td>-1.091904955472731     </td><td>-1.0373396358189255 </td><td>-0.9994130438532118 </td><td>-0.002298156418966926 </td><td>-0.06801008061610692  </td><td>1.0514615805535896 </td><td>0.2217676991796185    </td><td>0.10762713903039244  </td><td>1.0216791938918204</td><td>17.509603299015662   </td></tr>\n",
       "</tbody>\n",
       "</table><pre style='font-size: smaller; margin-bottom: 1em;'>[566472 rows x 29 columns]</pre>"
      ],
      "text/plain": [
       "         City                  State                Zip                Bank                  BankState            NAICS               NoEmp                  NewExist              CreateJob              RetainedJob              FranchiseCode       UrbanRural           RevLineCr             LowDoc                DisbursementGross       BalanceGross           GrAppv                  SBA_Appv               MIS_Status           Log_DisbursementGross    Log_GrAppv            Log_SBA_Appv          Log_BalanceGross        TotalJobs               IncomeToLoanRatio    EmployeesToLoanRatio    JobPerLoan             Gauren_SBA_Appv     DefaultRate\n",
       "-------  --------------------  -------------------  -----------------  --------------------  -------------------  ------------------  ---------------------  --------------------  ---------------------  -----------------------  ------------------  -------------------  --------------------  --------------------  ----------------------  ---------------------  ----------------------  ---------------------  -------------------  -----------------------  --------------------  --------------------  ----------------------  ----------------------  -------------------  ----------------------  ---------------------  ------------------  ---------------------\n",
       "type     real                  real                 int                real                  real                 int                 real                   real                  real                   real                     int                 real                 real                  real                  real                    real                   real                    real                   int                  real                     real                  real                  real                    real                    real                 real                    real                   real                real\n",
       "mins     0.002343996205589325  0.07006369426751592  0.0                0.0                   0.05249080575058509  0.0                 -0.153600434175742     0.17044022221184818   -0.03537327806289165   -0.04545428677284537     0.0                 0.07167428097535124  0.1530699251248866    0.08975812818488481   -0.6973236731572443     -0.002295517679303139  -0.6783474528695129     -0.6521736527433548    0.0                  -1.195091272676935       -1.1342833622057995   -1.056051925886739    -0.002298156418966926   -0.08082756483573703    -3249.7995325776096  -6217.269704293639      -6946.62619277787      -1425.410980018344  17.509603299015662\n",
       "mean     0.17567877828071593   0.1750960329901566   53853.91636479826  0.17619418541215054   0.175097323980688    398590.7101374825   8.12805739334945e-18   0.1750960329901566    7.224939905199511e-18  -5.5190513164718486e-18  2748.614023288001   0.17509603299015666  0.17509603299015658   0.17509603299015658   -1.204156650866585e-18  5.218012153755202e-18  1.7058885887276622e-17  -6.70313868982399e-17  0.1750960329901566   -0.2785689460749845      -0.279916152561005    -0.27948106942859474  -0.0022487117990158925  1.0636717082654834e-17  2.112198455611862    0.4918000867212119      -0.2489405491913917    1.496274454678428   17.509603299015662\n",
       "maxs     0.6626422957411695    0.27204321821338995  99999.0            0.8892988929799549    0.3800327332242226   928120.0            134.4770591134275      0.1869783599731616    37.62078512466269      40.51182843681707        92006.0             0.2434914068009948   0.25156900049734354   0.18706258164639383   37.39775945774547       552.889522851626       18.58692184996103       23.199528251630852     1.0                  3.6479991104368574       2.9748620909714023    3.186333139238373     6.316965249625691       75.14418220289946       6396.558729319238    37035.12034202456       11627.399763221902     1043.390760820282   17.509603299015662\n",
       "sigma    0.06639769763247672   0.04199181924329759  31185.22931842036  0.12227701398294173   0.07450052965800262  263321.75739069114  1.000000882657328      0.007437873600660589  1.0000008826573281     1.0000008826573283       12747.306218543337  0.07945017170394587  0.041041501003144853  0.031956251232178115  1.0000008826573277      1.00000088265733       1.0000008826573281      1.0000008826573283     0.38004955887207637  0.6751342455877893       0.673495391264487     0.6724602419470982    0.015557287489121684    1.9967248008580234      48.40854150735596    57.47035513066714       29.416621353104784     11.623336619659533  4.396234168225107e-15\n",
       "zeros    0                     0                    176                20864                 0                    127266              0                      0                     0                      0                        131369              0                    0                     0                     0                       0                      0                       0                      467285               0                        0                     0                     0                       0                       0                    0                       0                      0                   0\n",
       "missing  0                     0                    0                  0                     0                    0                   0                      0                     0                      0                        0                   0                    0                     0                     0                       0                      0                       0                      0                    0                        0                     0                     0                       0                       0                    0                       0                      0                   0\n",
       "0        0.11464974937637376   0.18477258385515263  93001.0            0.031446540880519576  0.21851689918783204  235910.0            0.6004066600002538     0.17044022221184818   -0.03537327806289165   -0.04545428677284537     1.0                 0.07167428097535124  0.1530699251248866    0.18706258164639383   0.35894908638142187     -0.002295517679303139  0.394801069280922       0.4109726208502541     0.0                  0.3067116704358818       0.3327518024385963    0.3442792686074715    -0.002298156418966926   -0.08082756483573703    0.873413624583551    1.4609407769259248      -0.19667384330497317   0.9606505378974516  17.509603299015662\n",
       "1        0.137596588115639     0.1659919028340081   44039.0            0.12869797760320908   0.15916691590309687  484121.0            -0.153600434175742     0.17044022221184818   -0.03537327806289165   -0.04545428677284537     0.0                 0.2434914068009948   0.1530699251248866    0.18706258164639383   -0.6142071281443656     -0.002295517679303139  -0.5945517349325695     -0.6003022273404686    0.0                  -0.9524546549496404      -0.9027619966303708   -0.9170465858109552   -0.002298156418966926   -0.08082756483573703    1.0231631671025114   0.25587183785114576     0.1346447858336776     0.99042067121194    17.509603299015662\n",
       "2        0.1391509433962264    0.11679920477137178  68122.0            0.1756944756092088    0.15916691590309687  451120.0            0.11568781374425648    0.17044022221184818   -0.013977733515888482  0.01858352805387556      38510.0             0.2434914068009948   0.1530699251248866    0.18706258164639383   2.3170363593098213      -0.002295517679303139  2.751995739818049       2.533233145474642      0.0                  1.199071721409156        1.3222878957003827    1.2622133569935532    -0.002298156418966926   0.00460579453798708     0.9146557881768466   0.045668048340090896    0.0018181486951624856  1.0863570708974033  17.509603299015662\n",
       "3        0.1407035175879397    0.19766159276417383  14208.0            0.1189488243430152    0.16729655837889998  321114.0            0.5196201856242543     0.17044022221184818   -0.03537327806289165   -0.04545428677284537     1.0                 0.07167428097535124  0.1530699251248866    0.18706258164639383   -0.004685798049922222   -0.002295517679303139  0.02511407838264226     0.0012319496005647338  0.0                  -0.0046968108174749315   0.024803902380498698  0.00123119137332351   -0.002298156418966926   -0.08082756483573703    -3.803563106619152   421.7868859132362       -65.60947363324371     20.385637830581544  17.509603299015662\n",
       "4        0.14035375121577756   0.14427252985884909  16335.0            0.19327731092436976   0.07830707560361704  0.0                 0.10222340134825655    0.17044022221184818   -0.03537327806289165   -0.04545428677284537     1.0                 0.07167428097535124  0.1530699251248866    0.18706258164639383   0.5840563957913015      -0.002295517679303139  0.6236549207893809      0.6376376730309333     0.0                  0.4599888961620474       0.4846797319622258    0.4932547601265484    -0.002298156418966926   -0.08082756483573703    0.91596908478613     0.16031581205412476     -0.12676096199199305   0.9780710067284966  17.509603299015662\n",
       "5        0.16473815924050095   0.1882487805974511   77381.0            0.1410558507971523    0.18002315611603092  532490.0            -0.14013602177974208   0.17044022221184818   -0.02681506024409038   -0.041185099117730634    1.0                 0.2434914068009948   0.25156900049734354   0.18706258164639383   -0.6078106173677496     -0.002295517679303139  -0.6086350488715516     -0.6090201139628024    0.0                  -0.936010436896243       -0.9381147754788066   -0.9390991626800832   -0.002298156418966926   -0.06800015936182102    0.998014028490483    0.23010081041149533     0.11165503043791804    0.9993677300922869  17.509603299015662\n",
       "6        0.1728110599078341    0.22381434467720585  48334.0            0.21485771162285663   0.1981815002622836   811111.0            -0.045885135007742606  0.17044022221184818   -0.03537327806289165   -0.011300785531927536    0.0                 0.2434914068009948   0.25156900049734354   0.18706258164639383   -0.5848427454291907     -0.002295517679303139  -0.6533495706278197     -0.6366994039887123    0.0                  -0.8790979038493719      -1.059438414961258    -1.012524699207778    -0.002298156418966926   -0.04667406359481919    0.9185539389001204   0.07206718699638688     0.07330627813128383    1.026150749529212   17.509603299015662\n",
       "7        0.27504127682993945   0.18477258385515263  90016.0            0.182256711409396     0.18002315611603092  453210.0            -0.0997427845917423    0.17044022221184818   -0.03537327806289165   -0.04545428677284537     50564.0             0.07167428097535124  0.1530699251248866    0.08975812818488481   -0.4375844699919985     -0.002295517679303139  -0.4149894822105479     -0.391072948404457     1.0                  -0.5755143250547002      -0.5361254527846345   -0.4960568023622707   -0.002298156418966926   -0.08082756483573703    1.1189331089693224   0.25504905158662605     0.20668155433789612    1.0611561958035405  17.509603299015662\n",
       "8        0.1708185053380783    0.17913669064748203  19805.0            0.21485771162285663   0.1981815002622836   722110.0            -0.11320719698774225   0.17044022221184818   -0.03537327806289165   -0.03264672380750118     1.0                 0.2434914068009948   0.25156900049734354   0.18706258164639383   -0.6800077262795612     -0.002295517679303139  -0.6614474761427344     -0.6417121887965542    0.0                  -1.1394584281034796      -1.0830760321756987   -1.0264186736605694   -0.002298156418966926   -0.06802000187039284    1.0596771234076527   0.17641428503960202     0.10599767786545444    1.0307541101614277  17.509603299015662\n",
       "9        0.0820405814622732    0.07938931297709924  85267.0            0.32986399789891396   0.07622333751568382  484220.0            -0.14013602177974208   0.1869783599731616    -0.031094169153491016  -0.03691591146261591     1.0                 0.18726450640542577  0.1530699251248866    0.18706258164639383   -0.6644233740896465     -0.002295517679303139  -0.6456037479613795     -0.6319045663464287    0.0                  -1.091904955472731       -1.0373396358189255   -0.9994130438532118   -0.002298156418966926   -0.06801008061610692    1.0514615805535896   0.2217676991796185      0.10762713903039244    1.0216791938918204  17.509603299015662\n",
       "[566472 rows x 29 columns]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_h.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check if the data frame is indeed H2OFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is H2O data frame\n"
     ]
    }
   ],
   "source": [
    "if isinstance(df_h, h2o.H2OFrame):\n",
    "    print('It is H2O data frame')\n",
    "else:\n",
    "    print('It is not H2O data frame')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code splits an H2O DataFrame into training, validation, and test sets and separates predictor columns from the response column for machine learning tasks.\n",
    "\n",
    "The first number, 0.7, specifies that 70% of the data will be used for training (the \"train\" subset).\n",
    "\n",
    "The second number, 0.15, specifies that 15% of the data will be used for validation (the \"valid\" subset).\n",
    "\n",
    "The remaining 15% not specified is implicitly used for testing (the \"test\" subset).\n",
    "\n",
    "The seed=1234 parameter is used to set the random seed for reproducibility, ensuring that the same split is obtained when the code is executed multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data as described above\n",
    "train, valid, test = df_h.split_frame([0.7, 0.15], seed=1234)\n",
    "\n",
    "# Prepare predictors and response columns\n",
    "train_X = df_h.columns\n",
    "train_y = \"MIS_Status\"\n",
    "train_X.remove(train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we already imported the H2O GLM estimator, we will just instantiate our model. For simplicity, the name of our model will be glm. To build a GLM, you just need to define the family, and you are ready to go. However, we will set a random seed for reproducibility purposes, and also a model id to be able to retrieve the model later on if we need to access it. You can instantiate your GLM, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm = H2OGeneralizedLinearEstimator(family = \"binomial\", seed = 42, model_id = 'default_glm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glm Model Build progress: || (done) 100%\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 973 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style='margin: 1em 0 1em 0;'>Model Details\n",
       "=============\n",
       "H2OGeneralizedLinearEstimator : Generalized Linear Modeling\n",
       "Model Key: default_glm\n",
       "</pre>\n",
       "<div style='margin: 1em 0 1em 0;'>\n",
       "<style>\n",
       "\n",
       "#h2o-table-2.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-2 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-2 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-2 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-2 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-2 .h2o-table th,\n",
       "#h2o-table-2 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-2 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-2\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption>GLM Model: summary</caption>\n",
       "    <thead><tr><th></th>\n",
       "<th>family</th>\n",
       "<th>link</th>\n",
       "<th>regularization</th>\n",
       "<th>number_of_predictors_total</th>\n",
       "<th>number_of_active_predictors</th>\n",
       "<th>number_of_iterations</th>\n",
       "<th>training_frame</th></tr></thead>\n",
       "    <tbody><tr><td></td>\n",
       "<td>binomial</td>\n",
       "<td>logit</td>\n",
       "<td>Elastic Net (alpha = 0.5, lambda = 2.505E-4 )</td>\n",
       "<td>27</td>\n",
       "<td>22</td>\n",
       "<td>4</td>\n",
       "<td>py_3_sid_9cdc</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n",
       "</div>\n",
       "<div style='margin: 1em 0 1em 0;'><pre style='margin: 1em 0 1em 0;'>ModelMetricsBinomialGLM: glm\n",
       "** Reported on train data. **\n",
       "\n",
       "MSE: 0.12059947193887977\n",
       "RMSE: 0.34727434679066027\n",
       "LogLoss: 0.38760076479874667\n",
       "AUC: 0.7768657635596997\n",
       "AUCPR: 0.4455013625622545\n",
       "Gini: 0.5537315271193994\n",
       "Null degrees of freedom: 396643\n",
       "Residual degrees of freedom: 396621\n",
       "Null deviance: 367114.9304608208\n",
       "Residual deviance: 307479.03550566814\n",
       "AIC: 307525.03550566814</pre>\n",
       "<div style='margin: 1em 0 1em 0;'>\n",
       "<style>\n",
       "\n",
       "#h2o-table-3.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-3 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-3 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-3 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-3 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-3 .h2o-table th,\n",
       "#h2o-table-3 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-3 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-3\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption>Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.22939337808642146</caption>\n",
       "    <thead><tr><th></th>\n",
       "<th>0</th>\n",
       "<th>1</th>\n",
       "<th>Error</th>\n",
       "<th>Rate</th></tr></thead>\n",
       "    <tbody><tr><td>0</td>\n",
       "<td>267868.0</td>\n",
       "<td>59606.0</td>\n",
       "<td>0.182</td>\n",
       "<td> (59606.0/327474.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>29773.0</td>\n",
       "<td>39397.0</td>\n",
       "<td>0.4304</td>\n",
       "<td> (29773.0/69170.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>297641.0</td>\n",
       "<td>99003.0</td>\n",
       "<td>0.2253</td>\n",
       "<td> (89379.0/396644.0)</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n",
       "</div>\n",
       "<div style='margin: 1em 0 1em 0;'>\n",
       "<style>\n",
       "\n",
       "#h2o-table-4.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-4 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-4 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-4 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-4 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-4 .h2o-table th,\n",
       "#h2o-table-4 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-4 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-4\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption>Maximum Metrics: Maximum metrics at their respective thresholds</caption>\n",
       "    <thead><tr><th>metric</th>\n",
       "<th>threshold</th>\n",
       "<th>value</th>\n",
       "<th>idx</th></tr></thead>\n",
       "    <tbody><tr><td>max f1</td>\n",
       "<td>0.2293934</td>\n",
       "<td>0.4685294</td>\n",
       "<td>225.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1212450</td>\n",
       "<td>0.5965971</td>\n",
       "<td>301.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.3459632</td>\n",
       "<td>0.4627148</td>\n",
       "<td>165.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.5098153</td>\n",
       "<td>0.8361301</td>\n",
       "<td>100.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9407762</td>\n",
       "<td>0.9433962</td>\n",
       "<td>2.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0017627</td>\n",
       "<td>1.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9724435</td>\n",
       "<td>0.9999969</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.2414210</td>\n",
       "<td>0.3410475</td>\n",
       "<td>218.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1711148</td>\n",
       "<td>0.7037444</td>\n",
       "<td>264.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.1683076</td>\n",
       "<td>0.7060715</td>\n",
       "<td>266.0</td></tr>\n",
       "<tr><td>max tns</td>\n",
       "<td>0.9724435</td>\n",
       "<td>327473.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max fns</td>\n",
       "<td>0.9724435</td>\n",
       "<td>69159.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max fps</td>\n",
       "<td>0.0017627</td>\n",
       "<td>327474.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max tps</td>\n",
       "<td>0.0017627</td>\n",
       "<td>69170.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max tnr</td>\n",
       "<td>0.9724435</td>\n",
       "<td>0.9999969</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max fnr</td>\n",
       "<td>0.9724435</td>\n",
       "<td>0.9998410</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max fpr</td>\n",
       "<td>0.0017627</td>\n",
       "<td>1.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max tpr</td>\n",
       "<td>0.0017627</td>\n",
       "<td>1.0</td>\n",
       "<td>399.0</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n",
       "</div>\n",
       "<div style='margin: 1em 0 1em 0;'>\n",
       "<style>\n",
       "\n",
       "#h2o-table-5.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-5 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-5 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-5 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-5 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-5 .h2o-table th,\n",
       "#h2o-table-5 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-5 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-5\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption>Gains/Lift Table: Avg response rate: 17.44 %, avg score: 17.44 %</caption>\n",
       "    <thead><tr><th>group</th>\n",
       "<th>cumulative_data_fraction</th>\n",
       "<th>lower_threshold</th>\n",
       "<th>lift</th>\n",
       "<th>cumulative_lift</th>\n",
       "<th>response_rate</th>\n",
       "<th>score</th>\n",
       "<th>cumulative_response_rate</th>\n",
       "<th>cumulative_score</th>\n",
       "<th>capture_rate</th>\n",
       "<th>cumulative_capture_rate</th>\n",
       "<th>gain</th>\n",
       "<th>cumulative_gain</th>\n",
       "<th>kolmogorov_smirnov</th></tr></thead>\n",
       "    <tbody><tr><td>1</td>\n",
       "<td>0.0100014</td>\n",
       "<td>0.7349622</td>\n",
       "<td>4.2382335</td>\n",
       "<td>4.2382335</td>\n",
       "<td>0.7390976</td>\n",
       "<td>0.8012626</td>\n",
       "<td>0.7390976</td>\n",
       "<td>0.8012626</td>\n",
       "<td>0.0423883</td>\n",
       "<td>0.0423883</td>\n",
       "<td>323.8233490</td>\n",
       "<td>323.8233490</td>\n",
       "<td>0.0392278</td></tr>\n",
       "<tr><td>2</td>\n",
       "<td>0.0200003</td>\n",
       "<td>0.6544053</td>\n",
       "<td>3.7072206</td>\n",
       "<td>3.9727605</td>\n",
       "<td>0.6464952</td>\n",
       "<td>0.6935386</td>\n",
       "<td>0.6928022</td>\n",
       "<td>0.7474074</td>\n",
       "<td>0.0370681</td>\n",
       "<td>0.0794564</td>\n",
       "<td>270.7220555</td>\n",
       "<td>297.2760491</td>\n",
       "<td>0.0720146</td></tr>\n",
       "<tr><td>3</td>\n",
       "<td>0.0300017</td>\n",
       "<td>0.5961965</td>\n",
       "<td>3.3087710</td>\n",
       "<td>3.7514120</td>\n",
       "<td>0.5770103</td>\n",
       "<td>0.6245402</td>\n",
       "<td>0.6542017</td>\n",
       "<td>0.7064482</td>\n",
       "<td>0.0330924</td>\n",
       "<td>0.1125488</td>\n",
       "<td>230.8770962</td>\n",
       "<td>275.1412049</td>\n",
       "<td>0.0999829</td></tr>\n",
       "<tr><td>4</td>\n",
       "<td>0.0400006</td>\n",
       "<td>0.5473008</td>\n",
       "<td>3.1577885</td>\n",
       "<td>3.6030249</td>\n",
       "<td>0.5506808</td>\n",
       "<td>0.5705676</td>\n",
       "<td>0.6283247</td>\n",
       "<td>0.6724824</td>\n",
       "<td>0.0315744</td>\n",
       "<td>0.1441232</td>\n",
       "<td>215.7788491</td>\n",
       "<td>260.3024867</td>\n",
       "<td>0.1261156</td></tr>\n",
       "<tr><td>5</td>\n",
       "<td>0.0500020</td>\n",
       "<td>0.5066561</td>\n",
       "<td>2.9401661</td>\n",
       "<td>3.4704397</td>\n",
       "<td>0.5127300</td>\n",
       "<td>0.5267305</td>\n",
       "<td>0.6052034</td>\n",
       "<td>0.6433290</td>\n",
       "<td>0.0294058</td>\n",
       "<td>0.1735290</td>\n",
       "<td>194.0166071</td>\n",
       "<td>247.0439739</td>\n",
       "<td>0.1496187</td></tr>\n",
       "<tr><td>6</td>\n",
       "<td>0.1000015</td>\n",
       "<td>0.3876609</td>\n",
       "<td>2.5479511</td>\n",
       "<td>3.0092070</td>\n",
       "<td>0.4443324</td>\n",
       "<td>0.4408256</td>\n",
       "<td>0.5247699</td>\n",
       "<td>0.5420799</td>\n",
       "<td>0.1273963</td>\n",
       "<td>0.3009253</td>\n",
       "<td>154.7951096</td>\n",
       "<td>200.9207046</td>\n",
       "<td>0.2433634</td></tr>\n",
       "<tr><td>7</td>\n",
       "<td>0.1500010</td>\n",
       "<td>0.3145150</td>\n",
       "<td>2.1466170</td>\n",
       "<td>2.7216819</td>\n",
       "<td>0.3743445</td>\n",
       "<td>0.3486125</td>\n",
       "<td>0.4746290</td>\n",
       "<td>0.4775918</td>\n",
       "<td>0.1073298</td>\n",
       "<td>0.4082550</td>\n",
       "<td>114.6616993</td>\n",
       "<td>172.1681861</td>\n",
       "<td>0.3128032</td></tr>\n",
       "<tr><td>8</td>\n",
       "<td>0.2000005</td>\n",
       "<td>0.2644560</td>\n",
       "<td>1.7658122</td>\n",
       "<td>2.4827175</td>\n",
       "<td>0.3079367</td>\n",
       "<td>0.2881698</td>\n",
       "<td>0.4329564</td>\n",
       "<td>0.4302369</td>\n",
       "<td>0.0882897</td>\n",
       "<td>0.4965447</td>\n",
       "<td>76.5812227</td>\n",
       "<td>148.2717465</td>\n",
       "<td>0.3591812</td></tr>\n",
       "<tr><td>9</td>\n",
       "<td>0.2999995</td>\n",
       "<td>0.1997678</td>\n",
       "<td>1.3652010</td>\n",
       "<td>2.1102151</td>\n",
       "<td>0.2380748</td>\n",
       "<td>0.2293917</td>\n",
       "<td>0.3679964</td>\n",
       "<td>0.3632891</td>\n",
       "<td>0.1365187</td>\n",
       "<td>0.6330635</td>\n",
       "<td>36.5200987</td>\n",
       "<td>111.0215103</td>\n",
       "<td>0.4034147</td></tr>\n",
       "<tr><td>10</td>\n",
       "<td>0.4000010</td>\n",
       "<td>0.1564369</td>\n",
       "<td>1.0614268</td>\n",
       "<td>1.8480147</td>\n",
       "<td>0.1851002</td>\n",
       "<td>0.1770859</td>\n",
       "<td>0.3222718</td>\n",
       "<td>0.3167377</td>\n",
       "<td>0.1061443</td>\n",
       "<td>0.7392077</td>\n",
       "<td>6.1426766</td>\n",
       "<td>84.8014713</td>\n",
       "<td>0.4108550</td></tr>\n",
       "<tr><td>11</td>\n",
       "<td>0.5</td>\n",
       "<td>0.1228382</td>\n",
       "<td>0.7986202</td>\n",
       "<td>1.6381379</td>\n",
       "<td>0.1392699</td>\n",
       "<td>0.1390702</td>\n",
       "<td>0.2856718</td>\n",
       "<td>0.2812046</td>\n",
       "<td>0.0798612</td>\n",
       "<td>0.8190690</td>\n",
       "<td>-20.1379831</td>\n",
       "<td>63.8137921</td>\n",
       "<td>0.3864636</td></tr>\n",
       "<tr><td>12</td>\n",
       "<td>0.5999990</td>\n",
       "<td>0.0957864</td>\n",
       "<td>0.6103864</td>\n",
       "<td>1.4668474</td>\n",
       "<td>0.1064441</td>\n",
       "<td>0.1088342</td>\n",
       "<td>0.2558008</td>\n",
       "<td>0.2524764</td>\n",
       "<td>0.0610380</td>\n",
       "<td>0.8801070</td>\n",
       "<td>-38.9613622</td>\n",
       "<td>46.6847437</td>\n",
       "<td>0.3392732</td></tr>\n",
       "<tr><td>13</td>\n",
       "<td>0.7000005</td>\n",
       "<td>0.0740264</td>\n",
       "<td>0.4845958</td>\n",
       "<td>1.3265238</td>\n",
       "<td>0.0845078</td>\n",
       "<td>0.0845272</td>\n",
       "<td>0.2313300</td>\n",
       "<td>0.2284833</td>\n",
       "<td>0.0484603</td>\n",
       "<td>0.9285673</td>\n",
       "<td>-51.5404179</td>\n",
       "<td>32.6523756</td>\n",
       "<td>0.2768453</td></tr>\n",
       "<tr><td>14</td>\n",
       "<td>0.7999995</td>\n",
       "<td>0.0569197</td>\n",
       "<td>0.3572394</td>\n",
       "<td>1.2053644</td>\n",
       "<td>0.0622983</td>\n",
       "<td>0.0652598</td>\n",
       "<td>0.2102012</td>\n",
       "<td>0.2080806</td>\n",
       "<td>0.0357236</td>\n",
       "<td>0.9642909</td>\n",
       "<td>-64.2760602</td>\n",
       "<td>20.5364357</td>\n",
       "<td>0.1989935</td></tr>\n",
       "<tr><td>15</td>\n",
       "<td>0.8999985</td>\n",
       "<td>0.0410503</td>\n",
       "<td>0.2490989</td>\n",
       "<td>1.0991135</td>\n",
       "<td>0.0434399</td>\n",
       "<td>0.0490687</td>\n",
       "<td>0.1916723</td>\n",
       "<td>0.1904127</td>\n",
       "<td>0.0249096</td>\n",
       "<td>0.9892005</td>\n",
       "<td>-75.0901059</td>\n",
       "<td>9.9113537</td>\n",
       "<td>0.1080435</td></tr>\n",
       "<tr><td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.1079932</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0188327</td>\n",
       "<td>0.0301829</td>\n",
       "<td>0.1743881</td>\n",
       "<td>0.1743895</td>\n",
       "<td>0.0107995</td>\n",
       "<td>1.0</td>\n",
       "<td>-89.2006838</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n",
       "</div></div>\n",
       "<div style='margin: 1em 0 1em 0;'><pre style='margin: 1em 0 1em 0;'>ModelMetricsBinomialGLM: glm\n",
       "** Reported on validation data. **\n",
       "\n",
       "MSE: 0.12231119069488473\n",
       "RMSE: 0.3497301684082812\n",
       "LogLoss: 0.3920168718665924\n",
       "AUC: 0.7738999913401364\n",
       "AUCPR: 0.43815403112953155\n",
       "Gini: 0.5477999826802729\n",
       "Null degrees of freedom: 84950\n",
       "Residual degrees of freedom: 84928\n",
       "Null deviance: 79045.0523443093\n",
       "Residual deviance: 66604.45056387779\n",
       "AIC: 66650.45056387779</pre>\n",
       "<div style='margin: 1em 0 1em 0;'>\n",
       "<style>\n",
       "\n",
       "#h2o-table-6.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-6 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-6 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-6 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-6 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-6 .h2o-table th,\n",
       "#h2o-table-6 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-6 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-6\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption>Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.2239578972008154</caption>\n",
       "    <thead><tr><th></th>\n",
       "<th>0</th>\n",
       "<th>1</th>\n",
       "<th>Error</th>\n",
       "<th>Rate</th></tr></thead>\n",
       "    <tbody><tr><td>0</td>\n",
       "<td>56619.0</td>\n",
       "<td>13383.0</td>\n",
       "<td>0.1912</td>\n",
       "<td> (13383.0/70002.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>6390.0</td>\n",
       "<td>8559.0</td>\n",
       "<td>0.4275</td>\n",
       "<td> (6390.0/14949.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>63009.0</td>\n",
       "<td>21942.0</td>\n",
       "<td>0.2328</td>\n",
       "<td> (19773.0/84951.0)</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n",
       "</div>\n",
       "<div style='margin: 1em 0 1em 0;'>\n",
       "<style>\n",
       "\n",
       "#h2o-table-7.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-7 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-7 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-7 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-7 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-7 .h2o-table th,\n",
       "#h2o-table-7 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-7 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-7\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption>Maximum Metrics: Maximum metrics at their respective thresholds</caption>\n",
       "    <thead><tr><th>metric</th>\n",
       "<th>threshold</th>\n",
       "<th>value</th>\n",
       "<th>idx</th></tr></thead>\n",
       "    <tbody><tr><td>max f1</td>\n",
       "<td>0.2239579</td>\n",
       "<td>0.4640156</td>\n",
       "<td>226.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1220999</td>\n",
       "<td>0.5966502</td>\n",
       "<td>299.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.3626617</td>\n",
       "<td>0.4579851</td>\n",
       "<td>155.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.5361307</td>\n",
       "<td>0.8339984</td>\n",
       "<td>89.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.8771121</td>\n",
       "<td>0.8072289</td>\n",
       "<td>8.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0016651</td>\n",
       "<td>1.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9643321</td>\n",
       "<td>0.9999857</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.2239579</td>\n",
       "<td>0.3317908</td>\n",
       "<td>226.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1697117</td>\n",
       "<td>0.7021874</td>\n",
       "<td>262.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.1654844</td>\n",
       "<td>0.7035843</td>\n",
       "<td>265.0</td></tr>\n",
       "<tr><td>max tns</td>\n",
       "<td>0.9643321</td>\n",
       "<td>70001.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max fns</td>\n",
       "<td>0.9643321</td>\n",
       "<td>14949.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max fps</td>\n",
       "<td>0.0016651</td>\n",
       "<td>70002.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max tps</td>\n",
       "<td>0.0016651</td>\n",
       "<td>14949.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max tnr</td>\n",
       "<td>0.9643321</td>\n",
       "<td>0.9999857</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max fnr</td>\n",
       "<td>0.9643321</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max fpr</td>\n",
       "<td>0.0016651</td>\n",
       "<td>1.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max tpr</td>\n",
       "<td>0.0016651</td>\n",
       "<td>1.0</td>\n",
       "<td>399.0</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n",
       "</div>\n",
       "<div style='margin: 1em 0 1em 0;'>\n",
       "<style>\n",
       "\n",
       "#h2o-table-8.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-8 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-8 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-8 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-8 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-8 .h2o-table th,\n",
       "#h2o-table-8 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-8 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-8\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption>Gains/Lift Table: Avg response rate: 17.60 %, avg score: 17.43 %</caption>\n",
       "    <thead><tr><th>group</th>\n",
       "<th>cumulative_data_fraction</th>\n",
       "<th>lower_threshold</th>\n",
       "<th>lift</th>\n",
       "<th>cumulative_lift</th>\n",
       "<th>response_rate</th>\n",
       "<th>score</th>\n",
       "<th>cumulative_response_rate</th>\n",
       "<th>cumulative_score</th>\n",
       "<th>capture_rate</th>\n",
       "<th>cumulative_capture_rate</th>\n",
       "<th>gain</th>\n",
       "<th>cumulative_gain</th>\n",
       "<th>kolmogorov_smirnov</th></tr></thead>\n",
       "    <tbody><tr><td>1</td>\n",
       "<td>0.0100058</td>\n",
       "<td>0.7370418</td>\n",
       "<td>4.0982449</td>\n",
       "<td>4.0982449</td>\n",
       "<td>0.7211765</td>\n",
       "<td>0.8003784</td>\n",
       "<td>0.7211765</td>\n",
       "<td>0.8003784</td>\n",
       "<td>0.0410061</td>\n",
       "<td>0.0410061</td>\n",
       "<td>309.8244856</td>\n",
       "<td>309.8244856</td>\n",
       "<td>0.0376205</td></tr>\n",
       "<tr><td>2</td>\n",
       "<td>0.0200115</td>\n",
       "<td>0.6538840</td>\n",
       "<td>3.6436272</td>\n",
       "<td>3.8709360</td>\n",
       "<td>0.6411765</td>\n",
       "<td>0.6937579</td>\n",
       "<td>0.6811765</td>\n",
       "<td>0.7470681</td>\n",
       "<td>0.0364573</td>\n",
       "<td>0.0774634</td>\n",
       "<td>264.3627156</td>\n",
       "<td>287.0936006</td>\n",
       "<td>0.0697207</td></tr>\n",
       "<tr><td>3</td>\n",
       "<td>0.0300055</td>\n",
       "<td>0.5974369</td>\n",
       "<td>3.3199408</td>\n",
       "<td>3.6874150</td>\n",
       "<td>0.5842167</td>\n",
       "<td>0.6256958</td>\n",
       "<td>0.6488819</td>\n",
       "<td>0.7066424</td>\n",
       "<td>0.0331795</td>\n",
       "<td>0.1106429</td>\n",
       "<td>231.9940802</td>\n",
       "<td>268.7415046</td>\n",
       "<td>0.0978575</td></tr>\n",
       "<tr><td>4</td>\n",
       "<td>0.0400113</td>\n",
       "<td>0.5484272</td>\n",
       "<td>3.0151850</td>\n",
       "<td>3.5193081</td>\n",
       "<td>0.5305882</td>\n",
       "<td>0.5718428</td>\n",
       "<td>0.6192998</td>\n",
       "<td>0.6729326</td>\n",
       "<td>0.0301692</td>\n",
       "<td>0.1408121</td>\n",
       "<td>201.5185041</td>\n",
       "<td>251.9308101</td>\n",
       "<td>0.1223269</td></tr>\n",
       "<tr><td>5</td>\n",
       "<td>0.0500053</td>\n",
       "<td>0.5088514</td>\n",
       "<td>2.8647876</td>\n",
       "<td>3.3884965</td>\n",
       "<td>0.5041225</td>\n",
       "<td>0.5278542</td>\n",
       "<td>0.5962806</td>\n",
       "<td>0.6439374</td>\n",
       "<td>0.0286307</td>\n",
       "<td>0.1694428</td>\n",
       "<td>186.4787628</td>\n",
       "<td>238.8496453</td>\n",
       "<td>0.1449435</td></tr>\n",
       "<tr><td>6</td>\n",
       "<td>0.1000106</td>\n",
       "<td>0.3880252</td>\n",
       "<td>2.5350181</td>\n",
       "<td>2.9617573</td>\n",
       "<td>0.4460923</td>\n",
       "<td>0.4417800</td>\n",
       "<td>0.5211864</td>\n",
       "<td>0.5428587</td>\n",
       "<td>0.1267643</td>\n",
       "<td>0.2962071</td>\n",
       "<td>153.5018073</td>\n",
       "<td>196.1757263</td>\n",
       "<td>0.2380945</td></tr>\n",
       "<tr><td>7</td>\n",
       "<td>0.1500041</td>\n",
       "<td>0.3140925</td>\n",
       "<td>2.0672956</td>\n",
       "<td>2.6636502</td>\n",
       "<td>0.3637862</td>\n",
       "<td>0.3486823</td>\n",
       "<td>0.4687279</td>\n",
       "<td>0.4781434</td>\n",
       "<td>0.1033514</td>\n",
       "<td>0.3995585</td>\n",
       "<td>106.7295582</td>\n",
       "<td>166.3650164</td>\n",
       "<td>0.3028470</td></tr>\n",
       "<tr><td>8</td>\n",
       "<td>0.2000094</td>\n",
       "<td>0.2650680</td>\n",
       "<td>1.6989303</td>\n",
       "<td>2.4224560</td>\n",
       "<td>0.2989642</td>\n",
       "<td>0.2885290</td>\n",
       "<td>0.4262845</td>\n",
       "<td>0.4307370</td>\n",
       "<td>0.0849555</td>\n",
       "<td>0.4845140</td>\n",
       "<td>69.8930318</td>\n",
       "<td>142.2456008</td>\n",
       "<td>0.3452609</td></tr>\n",
       "<tr><td>9</td>\n",
       "<td>0.3000082</td>\n",
       "<td>0.1996907</td>\n",
       "<td>1.4161649</td>\n",
       "<td>2.0870388</td>\n",
       "<td>0.2492054</td>\n",
       "<td>0.2294982</td>\n",
       "<td>0.3672605</td>\n",
       "<td>0.3636600</td>\n",
       "<td>0.1416148</td>\n",
       "<td>0.6261288</td>\n",
       "<td>41.6164908</td>\n",
       "<td>108.7038803</td>\n",
       "<td>0.3957640</td></tr>\n",
       "<tr><td>10</td>\n",
       "<td>0.4000071</td>\n",
       "<td>0.1560117</td>\n",
       "<td>1.0796836</td>\n",
       "<td>1.8352074</td>\n",
       "<td>0.1899941</td>\n",
       "<td>0.1767959</td>\n",
       "<td>0.3229452</td>\n",
       "<td>0.3169454</td>\n",
       "<td>0.1079671</td>\n",
       "<td>0.7340959</td>\n",
       "<td>7.9683590</td>\n",
       "<td>83.5207411</td>\n",
       "<td>0.4054339</td></tr>\n",
       "<tr><td>11</td>\n",
       "<td>0.5000059</td>\n",
       "<td>0.1223423</td>\n",
       "<td>0.8141109</td>\n",
       "<td>1.6309929</td>\n",
       "<td>0.1432607</td>\n",
       "<td>0.1384901</td>\n",
       "<td>0.2870091</td>\n",
       "<td>0.2812552</td>\n",
       "<td>0.0814101</td>\n",
       "<td>0.8155061</td>\n",
       "<td>-18.5889139</td>\n",
       "<td>63.0992909</td>\n",
       "<td>0.3828756</td></tr>\n",
       "<tr><td>12</td>\n",
       "<td>0.6000047</td>\n",
       "<td>0.0954449</td>\n",
       "<td>0.6615905</td>\n",
       "<td>1.4694290</td>\n",
       "<td>0.1164214</td>\n",
       "<td>0.1085546</td>\n",
       "<td>0.2585784</td>\n",
       "<td>0.2524723</td>\n",
       "<td>0.0661583</td>\n",
       "<td>0.8816643</td>\n",
       "<td>-33.8409498</td>\n",
       "<td>46.9429011</td>\n",
       "<td>0.3418083</td></tr>\n",
       "<tr><td>13</td>\n",
       "<td>0.7000035</td>\n",
       "<td>0.0737534</td>\n",
       "<td>0.4829811</td>\n",
       "<td>1.3285103</td>\n",
       "<td>0.0849912</td>\n",
       "<td>0.0840837</td>\n",
       "<td>0.2337806</td>\n",
       "<td>0.2284172</td>\n",
       "<td>0.0482975</td>\n",
       "<td>0.9299619</td>\n",
       "<td>-51.7018865</td>\n",
       "<td>32.8510255</td>\n",
       "<td>0.2790662</td></tr>\n",
       "<tr><td>14</td>\n",
       "<td>0.8000024</td>\n",
       "<td>0.0567782</td>\n",
       "<td>0.3465156</td>\n",
       "<td>1.2057627</td>\n",
       "<td>0.0609770</td>\n",
       "<td>0.0650276</td>\n",
       "<td>0.2121805</td>\n",
       "<td>0.2079938</td>\n",
       "<td>0.0346511</td>\n",
       "<td>0.9646130</td>\n",
       "<td>-65.3484449</td>\n",
       "<td>20.5762724</td>\n",
       "<td>0.1997634</td></tr>\n",
       "<tr><td>15</td>\n",
       "<td>0.9000012</td>\n",
       "<td>0.0411062</td>\n",
       "<td>0.2535316</td>\n",
       "<td>1.0999607</td>\n",
       "<td>0.0446145</td>\n",
       "<td>0.0491226</td>\n",
       "<td>0.1935623</td>\n",
       "<td>0.1903417</td>\n",
       "<td>0.0253529</td>\n",
       "<td>0.9899659</td>\n",
       "<td>-74.6468351</td>\n",
       "<td>9.9960655</td>\n",
       "<td>0.1091768</td></tr>\n",
       "<tr><td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0000070</td>\n",
       "<td>0.1003423</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0176574</td>\n",
       "<td>0.0301928</td>\n",
       "<td>0.1759720</td>\n",
       "<td>0.1743270</td>\n",
       "<td>0.0100341</td>\n",
       "<td>1.0</td>\n",
       "<td>-89.9657659</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n",
       "</div></div>\n",
       "<div style='margin: 1em 0 1em 0;'>\n",
       "<style>\n",
       "\n",
       "#h2o-table-9.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-9 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-9 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-9 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-9 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-9 .h2o-table th,\n",
       "#h2o-table-9 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-9 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-9\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption>Scoring History: </caption>\n",
       "    <thead><tr><th></th>\n",
       "<th>timestamp</th>\n",
       "<th>duration</th>\n",
       "<th>iterations</th>\n",
       "<th>negative_log_likelihood</th>\n",
       "<th>objective</th>\n",
       "<th>training_rmse</th>\n",
       "<th>training_logloss</th>\n",
       "<th>training_r2</th>\n",
       "<th>training_auc</th>\n",
       "<th>training_pr_auc</th>\n",
       "<th>training_lift</th>\n",
       "<th>training_classification_error</th>\n",
       "<th>validation_rmse</th>\n",
       "<th>validation_logloss</th>\n",
       "<th>validation_r2</th>\n",
       "<th>validation_auc</th>\n",
       "<th>validation_pr_auc</th>\n",
       "<th>validation_lift</th>\n",
       "<th>validation_classification_error</th></tr></thead>\n",
       "    <tbody><tr><td></td>\n",
       "<td>2023-11-05 21:14:49</td>\n",
       "<td> 0.000 sec</td>\n",
       "<td>0</td>\n",
       "<td>183557.4652304</td>\n",
       "<td>0.4627764</td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td></tr>\n",
       "<tr><td></td>\n",
       "<td>2023-11-05 21:14:49</td>\n",
       "<td> 0.141 sec</td>\n",
       "<td>1</td>\n",
       "<td>156656.8094107</td>\n",
       "<td>0.3957819</td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td></tr>\n",
       "<tr><td></td>\n",
       "<td>2023-11-05 21:14:49</td>\n",
       "<td> 0.172 sec</td>\n",
       "<td>2</td>\n",
       "<td>153809.5910401</td>\n",
       "<td>0.3887093</td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td></tr>\n",
       "<tr><td></td>\n",
       "<td>2023-11-05 21:14:49</td>\n",
       "<td> 0.220 sec</td>\n",
       "<td>3</td>\n",
       "<td>153734.0005282</td>\n",
       "<td>0.3885901</td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td>\n",
       "<td></td></tr>\n",
       "<tr><td></td>\n",
       "<td>2023-11-05 21:14:49</td>\n",
       "<td> 0.251 sec</td>\n",
       "<td>4</td>\n",
       "<td>153739.5177528</td>\n",
       "<td>0.3885909</td>\n",
       "<td>0.3472743</td>\n",
       "<td>0.3876008</td>\n",
       "<td>0.1623693</td>\n",
       "<td>0.7768658</td>\n",
       "<td>0.4455014</td>\n",
       "<td>4.2382335</td>\n",
       "<td>0.2253381</td>\n",
       "<td>0.3497302</td>\n",
       "<td>0.3920169</td>\n",
       "<td>0.1565087</td>\n",
       "<td>0.7739000</td>\n",
       "<td>0.4381540</td>\n",
       "<td>4.0982449</td>\n",
       "<td>0.2327577</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n",
       "</div>\n",
       "<div style='margin: 1em 0 1em 0;'>\n",
       "<style>\n",
       "\n",
       "#h2o-table-10.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-10 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-10 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-10 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-10 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-10 .h2o-table th,\n",
       "#h2o-table-10 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-10 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-10\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption>Variable Importances: </caption>\n",
       "    <thead><tr><th>variable</th>\n",
       "<th>relative_importance</th>\n",
       "<th>scaled_importance</th>\n",
       "<th>percentage</th></tr></thead>\n",
       "    <tbody><tr><td>Log_DisbursementGross</td>\n",
       "<td>1.0742711</td>\n",
       "<td>1.0</td>\n",
       "<td>0.1794571</td></tr>\n",
       "<tr><td>GrAppv</td>\n",
       "<td>0.7878199</td>\n",
       "<td>0.7333529</td>\n",
       "<td>0.1316054</td></tr>\n",
       "<tr><td>Log_GrAppv</td>\n",
       "<td>0.7507896</td>\n",
       "<td>0.6988828</td>\n",
       "<td>0.1254195</td></tr>\n",
       "<tr><td>DisbursementGross</td>\n",
       "<td>0.7272646</td>\n",
       "<td>0.6769843</td>\n",
       "<td>0.1214896</td></tr>\n",
       "<tr><td>Log_SBA_Appv</td>\n",
       "<td>0.5599474</td>\n",
       "<td>0.5212347</td>\n",
       "<td>0.0935393</td></tr>\n",
       "<tr><td>Bank</td>\n",
       "<td>0.5510707</td>\n",
       "<td>0.5129717</td>\n",
       "<td>0.0920564</td></tr>\n",
       "<tr><td>City</td>\n",
       "<td>0.3853748</td>\n",
       "<td>0.3587314</td>\n",
       "<td>0.0643769</td></tr>\n",
       "<tr><td>UrbanRural</td>\n",
       "<td>0.3724908</td>\n",
       "<td>0.3467381</td>\n",
       "<td>0.0622246</td></tr>\n",
       "<tr><td>SBA_Appv</td>\n",
       "<td>0.1293647</td>\n",
       "<td>0.1204209</td>\n",
       "<td>0.0216104</td></tr>\n",
       "<tr><td>RevLineCr</td>\n",
       "<td>0.1193446</td>\n",
       "<td>0.1110936</td>\n",
       "<td>0.0199365</td></tr>\n",
       "<tr><td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td></tr>\n",
       "<tr><td>BankState</td>\n",
       "<td>0.0316091</td>\n",
       "<td>0.0294238</td>\n",
       "<td>0.0052803</td></tr>\n",
       "<tr><td>FranchiseCode</td>\n",
       "<td>0.0257639</td>\n",
       "<td>0.0239827</td>\n",
       "<td>0.0043039</td></tr>\n",
       "<tr><td>Gauren_SBA_Appv</td>\n",
       "<td>0.0088713</td>\n",
       "<td>0.0082580</td>\n",
       "<td>0.0014819</td></tr>\n",
       "<tr><td>Log_BalanceGross</td>\n",
       "<td>0.0040791</td>\n",
       "<td>0.0037971</td>\n",
       "<td>0.0006814</td></tr>\n",
       "<tr><td>JobPerLoan</td>\n",
       "<td>0.0000534</td>\n",
       "<td>0.0000497</td>\n",
       "<td>0.0000089</td></tr>\n",
       "<tr><td>RetainedJob</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>BalanceGross</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>TotalJobs</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>IncomeToLoanRatio</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>EmployeesToLoanRatio</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n",
       "<pre style='font-size: smaller; margin-bottom: 1em;'>[27 rows x 4 columns]</pre></div><pre style=\"font-size: smaller; margin: 1em 0 0 0;\">\n",
       "\n",
       "[tips]\n",
       "Use `model.explain()` to inspect the model.\n",
       "--\n",
       "Use `h2o.display.toggle_user_tips()` to switch on/off this section.</pre>"
      ],
      "text/plain": [
       "Model Details\n",
       "=============\n",
       "H2OGeneralizedLinearEstimator : Generalized Linear Modeling\n",
       "Model Key: default_glm\n",
       "\n",
       "\n",
       "GLM Model: summary\n",
       "    family    link    regularization                                 number_of_predictors_total    number_of_active_predictors    number_of_iterations    training_frame\n",
       "--  --------  ------  ---------------------------------------------  ----------------------------  -----------------------------  ----------------------  ----------------\n",
       "    binomial  logit   Elastic Net (alpha = 0.5, lambda = 2.505E-4 )  27                            22                             4                       py_3_sid_9cdc\n",
       "\n",
       "ModelMetricsBinomialGLM: glm\n",
       "** Reported on train data. **\n",
       "\n",
       "MSE: 0.12059947193887977\n",
       "RMSE: 0.34727434679066027\n",
       "LogLoss: 0.38760076479874667\n",
       "AUC: 0.7768657635596997\n",
       "AUCPR: 0.4455013625622545\n",
       "Gini: 0.5537315271193994\n",
       "Null degrees of freedom: 396643\n",
       "Residual degrees of freedom: 396621\n",
       "Null deviance: 367114.9304608208\n",
       "Residual deviance: 307479.03550566814\n",
       "AIC: 307525.03550566814\n",
       "\n",
       "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.22939337808642146\n",
       "       0       1      Error    Rate\n",
       "-----  ------  -----  -------  ------------------\n",
       "0      267868  59606  0.182    (59606.0/327474.0)\n",
       "1      29773   39397  0.4304   (29773.0/69170.0)\n",
       "Total  297641  99003  0.2253   (89379.0/396644.0)\n",
       "\n",
       "Maximum Metrics: Maximum metrics at their respective thresholds\n",
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.229393     0.468529  225\n",
       "max f2                       0.121245     0.596597  301\n",
       "max f0point5                 0.345963     0.462715  165\n",
       "max accuracy                 0.509815     0.83613   100\n",
       "max precision                0.940776     0.943396  2\n",
       "max recall                   0.00176269   1         399\n",
       "max specificity              0.972444     0.999997  0\n",
       "max absolute_mcc             0.241421     0.341047  218\n",
       "max min_per_class_accuracy   0.171115     0.703744  264\n",
       "max mean_per_class_accuracy  0.168308     0.706072  266\n",
       "max tns                      0.972444     327473    0\n",
       "max fns                      0.972444     69159     0\n",
       "max fps                      0.00176269   327474    399\n",
       "max tps                      0.00176269   69170     399\n",
       "max tnr                      0.972444     0.999997  0\n",
       "max fnr                      0.972444     0.999841  0\n",
       "max fpr                      0.00176269   1         399\n",
       "max tpr                      0.00176269   1         399\n",
       "\n",
       "Gains/Lift Table: Avg response rate: 17.44 %, avg score: 17.44 %\n",
       "group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n",
       "-------  --------------------------  -----------------  --------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n",
       "1        0.0100014                   0.734962           4.23823   4.23823            0.739098         0.801263   0.739098                    0.801263            0.0423883       0.0423883                  323.823   323.823            0.0392278\n",
       "2        0.0200003                   0.654405           3.70722   3.97276            0.646495         0.693539   0.692802                    0.747407            0.0370681       0.0794564                  270.722   297.276            0.0720146\n",
       "3        0.0300017                   0.596196           3.30877   3.75141            0.57701          0.62454    0.654202                    0.706448            0.0330924       0.112549                   230.877   275.141            0.0999829\n",
       "4        0.0400006                   0.547301           3.15779   3.60302            0.550681         0.570568   0.628325                    0.672482            0.0315744       0.144123                   215.779   260.302            0.126116\n",
       "5        0.050002                    0.506656           2.94017   3.47044            0.51273          0.52673    0.605203                    0.643329            0.0294058       0.173529                   194.017   247.044            0.149619\n",
       "6        0.100002                    0.387661           2.54795   3.00921            0.444332         0.440826   0.52477                     0.54208             0.127396        0.300925                   154.795   200.921            0.243363\n",
       "7        0.150001                    0.314515           2.14662   2.72168            0.374344         0.348613   0.474629                    0.477592            0.10733         0.408255                   114.662   172.168            0.312803\n",
       "8        0.200001                    0.264456           1.76581   2.48272            0.307937         0.28817    0.432956                    0.430237            0.0882897       0.496545                   76.5812   148.272            0.359181\n",
       "9        0.299999                    0.199768           1.3652    2.11022            0.238075         0.229392   0.367996                    0.363289            0.136519        0.633063                   36.5201   111.022            0.403415\n",
       "10       0.400001                    0.156437           1.06143   1.84801            0.1851           0.177086   0.322272                    0.316738            0.106144        0.739208                   6.14268   84.8015            0.410855\n",
       "11       0.5                         0.122838           0.79862   1.63814            0.13927          0.13907    0.285672                    0.281205            0.0798612       0.819069                   -20.138   63.8138            0.386464\n",
       "12       0.599999                    0.0957864          0.610386  1.46685            0.106444         0.108834   0.255801                    0.252476            0.061038        0.880107                   -38.9614  46.6847            0.339273\n",
       "13       0.700001                    0.0740264          0.484596  1.32652            0.0845078        0.0845272  0.23133                     0.228483            0.0484603       0.928567                   -51.5404  32.6524            0.276845\n",
       "14       0.799999                    0.0569197          0.357239  1.20536            0.0622983        0.0652598  0.210201                    0.208081            0.0357236       0.964291                   -64.2761  20.5364            0.198993\n",
       "15       0.899998                    0.0410503          0.249099  1.09911            0.0434399        0.0490687  0.191672                    0.190413            0.0249096       0.989201                   -75.0901  9.91135            0.108044\n",
       "16       1                           4.27023e-11        0.107993  1                  0.0188327        0.0301829  0.174388                    0.17439             0.0107995       1                          -89.2007  0                  0\n",
       "\n",
       "ModelMetricsBinomialGLM: glm\n",
       "** Reported on validation data. **\n",
       "\n",
       "MSE: 0.12231119069488473\n",
       "RMSE: 0.3497301684082812\n",
       "LogLoss: 0.3920168718665924\n",
       "AUC: 0.7738999913401364\n",
       "AUCPR: 0.43815403112953155\n",
       "Gini: 0.5477999826802729\n",
       "Null degrees of freedom: 84950\n",
       "Residual degrees of freedom: 84928\n",
       "Null deviance: 79045.0523443093\n",
       "Residual deviance: 66604.45056387779\n",
       "AIC: 66650.45056387779\n",
       "\n",
       "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.2239578972008154\n",
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  -----------------\n",
       "0      56619  13383  0.1912   (13383.0/70002.0)\n",
       "1      6390   8559   0.4275   (6390.0/14949.0)\n",
       "Total  63009  21942  0.2328   (19773.0/84951.0)\n",
       "\n",
       "Maximum Metrics: Maximum metrics at their respective thresholds\n",
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.223958     0.464016  226\n",
       "max f2                       0.1221       0.59665   299\n",
       "max f0point5                 0.362662     0.457985  155\n",
       "max accuracy                 0.536131     0.833998  89\n",
       "max precision                0.877112     0.807229  8\n",
       "max recall                   0.00166505   1         399\n",
       "max specificity              0.964332     0.999986  0\n",
       "max absolute_mcc             0.223958     0.331791  226\n",
       "max min_per_class_accuracy   0.169712     0.702187  262\n",
       "max mean_per_class_accuracy  0.165484     0.703584  265\n",
       "max tns                      0.964332     70001     0\n",
       "max fns                      0.964332     14949     0\n",
       "max fps                      0.00166505   70002     399\n",
       "max tps                      0.00166505   14949     399\n",
       "max tnr                      0.964332     0.999986  0\n",
       "max fnr                      0.964332     1         0\n",
       "max fpr                      0.00166505   1         399\n",
       "max tpr                      0.00166505   1         399\n",
       "\n",
       "Gains/Lift Table: Avg response rate: 17.60 %, avg score: 17.43 %\n",
       "group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n",
       "-------  --------------------------  -----------------  --------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n",
       "1        0.0100058                   0.737042           4.09824   4.09824            0.721176         0.800378   0.721176                    0.800378            0.0410061       0.0410061                  309.824   309.824            0.0376205\n",
       "2        0.0200115                   0.653884           3.64363   3.87094            0.641176         0.693758   0.681176                    0.747068            0.0364573       0.0774634                  264.363   287.094            0.0697207\n",
       "3        0.0300055                   0.597437           3.31994   3.68742            0.584217         0.625696   0.648882                    0.706642            0.0331795       0.110643                   231.994   268.742            0.0978575\n",
       "4        0.0400113                   0.548427           3.01519   3.51931            0.530588         0.571843   0.6193                      0.672933            0.0301692       0.140812                   201.519   251.931            0.122327\n",
       "5        0.0500053                   0.508851           2.86479   3.3885             0.504122         0.527854   0.596281                    0.643937            0.0286307       0.169443                   186.479   238.85             0.144943\n",
       "6        0.100011                    0.388025           2.53502   2.96176            0.446092         0.44178    0.521186                    0.542859            0.126764        0.296207                   153.502   196.176            0.238094\n",
       "7        0.150004                    0.314092           2.0673    2.66365            0.363786         0.348682   0.468728                    0.478143            0.103351        0.399558                   106.73    166.365            0.302847\n",
       "8        0.200009                    0.265068           1.69893   2.42246            0.298964         0.288529   0.426285                    0.430737            0.0849555       0.484514                   69.893    142.246            0.345261\n",
       "9        0.300008                    0.199691           1.41616   2.08704            0.249205         0.229498   0.36726                     0.36366             0.141615        0.626129                   41.6165   108.704            0.395764\n",
       "10       0.400007                    0.156012           1.07968   1.83521            0.189994         0.176796   0.322945                    0.316945            0.107967        0.734096                   7.96836   83.5207            0.405434\n",
       "11       0.500006                    0.122342           0.814111  1.63099            0.143261         0.13849    0.287009                    0.281255            0.0814101       0.815506                   -18.5889  63.0993            0.382876\n",
       "12       0.600005                    0.0954449          0.661591  1.46943            0.116421         0.108555   0.258578                    0.252472            0.0661583       0.881664                   -33.8409  46.9429            0.341808\n",
       "13       0.700004                    0.0737534          0.482981  1.32851            0.0849912        0.0840837  0.233781                    0.228417            0.0482975       0.929962                   -51.7019  32.851             0.279066\n",
       "14       0.800002                    0.0567782          0.346516  1.20576            0.060977         0.0650276  0.212181                    0.207994            0.0346511       0.964613                   -65.3484  20.5763            0.199763\n",
       "15       0.900001                    0.0411062          0.253532  1.09996            0.0446145        0.0491226  0.193562                    0.190342            0.0253529       0.989966                   -74.6468  9.99607            0.109177\n",
       "16       1                           6.97019e-06        0.100342  1                  0.0176574        0.0301928  0.175972                    0.174327            0.0100341       1                          -89.9658  0                  0\n",
       "\n",
       "Scoring History: \n",
       "    timestamp            duration    iterations    negative_log_likelihood    objective    training_rmse        training_logloss     training_r2          training_auc        training_pr_auc     training_lift      training_classification_error    validation_rmse     validation_logloss    validation_r2        validation_auc      validation_pr_auc    validation_lift    validation_classification_error\n",
       "--  -------------------  ----------  ------------  -------------------------  -----------  -------------------  -------------------  -------------------  ------------------  ------------------  -----------------  -------------------------------  ------------------  --------------------  -------------------  ------------------  -------------------  -----------------  ---------------------------------\n",
       "    2023-11-05 21:14:49  0.000 sec   0             183557                     0.462776\n",
       "    2023-11-05 21:14:49  0.141 sec   1             156657                     0.395782\n",
       "    2023-11-05 21:14:49  0.172 sec   2             153810                     0.388709\n",
       "    2023-11-05 21:14:49  0.220 sec   3             153734                     0.38859\n",
       "    2023-11-05 21:14:49  0.251 sec   4             153740                     0.388591     0.34727434679066027  0.38760076479874667  0.16236930409227046  0.7768657635596997  0.4455013625622545  4.238233490486189  0.22533808654612197              0.3497301684082812  0.3920168718665924    0.15650872440673047  0.7738999913401364  0.43815403112953155  4.098244856039948  0.2327577073842568\n",
       "\n",
       "Variable Importances: \n",
       "variable               relative_importance    scaled_importance      percentage\n",
       "---------------------  ---------------------  ---------------------  ---------------------\n",
       "Log_DisbursementGross  1.0742710828781128     1.0                    0.17945711926553892\n",
       "GrAppv                 0.7878198623657227     0.7333529450081168     0.13160540691605582\n",
       "Log_GrAppv             0.7507895827293396     0.6988828003429787     0.12541949405378375\n",
       "DisbursementGross      0.7272646427154541     0.6769842866541813     0.12148964987099521\n",
       "Log_SBA_Appv           0.5599473714828491     0.5212347054736658     0.0935392787055257\n",
       "Bank                   0.5510706901550293     0.5129717246773866     0.09205642797527898\n",
       "City                   0.3853748142719269     0.3587314416389738     0.06437691110650404\n",
       "UrbanRural             0.3724907636642456     0.34673814607975306    0.06222462883494611\n",
       "SBA_Appv               0.1293647140264511     0.12042092176572985    0.021610391719378712\n",
       "RevLineCr              0.11934459209442139    0.11109355356999986    0.019936529092644\n",
       "---                    ---                    ---                    ---\n",
       "BankState              0.03160914033651352    0.029423802651215834   0.005280310861624921\n",
       "FranchiseCode          0.025763940066099167   0.023982717655467556   0.004303869422608988\n",
       "Gauren_SBA_Appv        0.008871283382177353   0.008257956044399915   0.0014819490027494534\n",
       "Log_BalanceGross       0.004079081583768129   0.0037970691464948828  0.0006814110906820303\n",
       "JobPerLoan             5.3359181038104e-05    4.967012692471231e-05  8.913657891462553e-06\n",
       "RetainedJob            0.0                    0.0                    0.0\n",
       "BalanceGross           0.0                    0.0                    0.0\n",
       "TotalJobs              0.0                    0.0                    0.0\n",
       "IncomeToLoanRatio      0.0                    0.0                    0.0\n",
       "EmployeesToLoanRatio   0.0                    0.0                    0.0\n",
       "[27 rows x 4 columns]\n",
       "\n",
       "\n",
       "[tips]\n",
       "Use `model.explain()` to inspect the model.\n",
       "--\n",
       "Use `h2o.display.toggle_user_tips()` to switch on/off this section."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time glm.train(x = train_X, y = train_y, training_frame = train, validation_frame = valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the summary results, we can see the GLM performance. We will focus on the Area Under the Curve (AUC), and since we have a very imbalanced dataset, we will be looking at the F1 score. Additionally, we will also take a quick look at the misclassification error and logloss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the report, we can look at the metrics on the training and validation data, and we see that the training AUCPR was AUCPR: 0.4455013625622545 while the validation AUCPR: 0.43815403112953155"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```AUCPR (Area Under the Precision-Recall Curve) is a metric that measures the trade-off between precision and recall for a binary classification model.\n",
    "\n",
    "AUCPR summarizes how well a model is at ranking the positive class instances higher than the negative class instances.\n",
    "\n",
    "Precision is a measure of the model's ability to correctly identify positive instances among the instances it predicts as positive. It's the ratio of true positive predictions to the total positive predictions\n",
    "\n",
    "Recall (or Sensitivity) is a measure of the model's ability to identify all the positive instances correctly. It's the ratio of true positive predictions to the total actual positive instances.\n",
    "\n",
    "The reported values represent the AUCPR score for your model on both the training and validation datasets:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```  \n",
    "\n",
    "Training AUCPR: 0.4455013625622545> This indicates that on the training dataset, the model achieved an AUCPR of approximately 0.4455. This score reflects how well the model performs on the data it was trained on. It suggests that the model is reasonably effective at ranking and identifying positive instances relative to negative instances within the training dataset.\n",
    "\n",
    "<Validation AUCPR: 0.43815403112953155> This indicates that on the validation dataset, the model achieved an AUCPR of approximately 0.4382. The validation dataset is a separate dataset that the model did not see during training. The AUCPR score on the validation dataset measures the model's generalization performance. While slightly lower than the training AUCPR, this score still suggests that the model maintains its ability to rank positive instances effectively when applied to new, unseen data.\n",
    "\n",
    "The goal is to have a model that generalizes well, which means it performs consistently on both the training and validation datasets. In this case, the AUCPR scores are reasonably close, indicating that the model is performing consistently, and it is not exhibiting significant overfitting (a situation where the model fits the training data too closely but performs poorly on new data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the report, we can also see the max F1 score as well as all the metrics for our model with their respective thresholds. For the default GLM, we obtained a training F1 score of 0.2293934 and a validation F1 score of 0.2239579.\n",
    "\n",
    "F1 Score: The F1 score is a metric that combines both precision and recall into a single value, providing a balanced measure of a model's performance. It is especially useful when dealing with imbalanced datasets\n",
    "\n",
    "Training F1 Score: This is the F1 score calculated on the training dataset. It measures how well the model performs on the data it was trained on. A training F1 score of 0.2293934 means that the model achieved an F1 score of approximately 0.2294 when tested on the training data.\n",
    "\n",
    "Validation F1 Score: This is the F1 score calculated on the validation dataset. The validation dataset is a separate set of data that the model did not see during training. The validation F1 score of 0.2239579 indicates that when the model is applied to new, unseen data (the validation dataset), it achieved an F1 score of approximately 0.2240. This demonstrates how well the model generalizes to data it has not been trained on.\n",
    "\n",
    "Thresholds: In binary classification, different threshold values can be used to determine whether a prediction is classified as the positive or negative class. The explanation suggests that the F1 scores reported are associated with specific threshold values. Different thresholds can affect the trade-off between precision and recall, and they are used to fine-tune the model's performance.\n",
    "\n",
    "``` In summary, the explanation indicates that a machine learning model was evaluated using F1 scores on both the training and validation datasets. The training F1 score represents its performance on the training data, while the validation F1 score reflects how well the model generalizes to new, unseen data. The thresholds mentioned suggest that different threshold values were applied to calculate these scores, influencing the precision and recall trade-off.```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the Scoring history for any of our models, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<h2o.plot._plot_result._MObject at 0x2105aa54f70>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glm.plot(metric='negative_log_likelihood')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can also generate a variable importance plot to see how each of our features contribute to the linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` As we can see after 2 iterations, the scores dont really improve after this time We can also use the default number of iterations and use early stopping; that way, the model will stop training when it is no longer improving. We will use early stopping when we start tuning our models.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABQIAAANMCAYAAAAHW9DnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAACWP0lEQVR4nOzdeXxM1+P/8fdEVlntuxARgtgp0qJFVVH7GlspLaWUKvppK5ZWP62tFEUtbe1rSylFpVVLrbFTW4qifIpYSkRyf3/45X4zshMJ7uv5eMzjkZl7zrln7ty5M/POuffYDMMwBAAAAAAAAOCp5pDZHQAAAAAAAADw6BEEAgAAAAAAABZAEAgAAAAAAABYAEEgAAAAAAAAYAEEgQAAAAAAAIAFEAQCAAAAAAAAFkAQCAAAAAAAAFgAQSAAAAAAAABgAQSBAAAAAAAAgAUQBAIAUsVms8lmsyk0NPSRtD979mxzHREREQ/cTpcuXWSz2VSkSJF06xsA4PGV3OdTWFiYuTwsLCzD+5ZeateuLZvNptq1a1ty/Xg8REREmO+n2bNnP5J1hIaGmutIDPsi8PAIAgEgg7z++uvmF5uff/45TXV/+ukns27fvn0fUQ+RnooUKUIgiSda/B9jqQ1QUtrv//33Xy1btkw9e/ZUlSpVlC1bNjk5OSlHjhyqXr26QkNDdeHChTT1MyYmRosXL1aHDh0UEBAgb29vubm5qUiRImrQoIG++OILXb16NU1tptatW7f0zTffqGPHjgoMDFSOHDnk5OSkbNmyqWTJkmrTpo2++OKLND8nIL3FD3C6dOmS2d2xnLhjY9ztm2++SVW97t2729XjtQOQHggCASCDdOrUyfx7zpw5aar77bffJtoO8DRjdOfTZd++fcqTJ49atGihL7/8Ujt37tTVq1d19+5dXb58Wdu2bdOwYcNUokQJLVy4MFVt/vbbbypbtqxat26tuXPn6tixY7p27Zpu376tP//8U2vWrFGfPn3k7++v6dOnp+vzmTFjhooVK6bOnTtrzpw5OnLkiC5fvqy7d+/q6tWrOnr0qBYtWqQ+ffqoYMGC6tixo/7888907QOAJ1Nqvgfevn1bixcvzoDeALAax8zuAABYRXBwsIoVK6YTJ05o6dKlmjRpktzc3FKsd/PmTS1fvlySVLp0aVWqVOlRdzVRhmFkynoBPB2uXbumGzduSLp3PGzUqJEqV66sHDly6NKlS1q2bJmmT5+ua9euKSQkRF5eXmrQoEGS7S1ZskQdOnRQVFSUpHuni4WEhKhkyZJycXHRn3/+qRUrVmj+/Pn6559/1KNHDx09elSjR49+qOcRGxurXr16aerUqZLunZbasGFDvfTSSypZsqSyZcum69ev66+//tLGjRu1cuVK/f3335ozZ45Kly6twYMHP9T6kTa1a9fm8ysdPMmnVT9OXF1ddfv2bW3YsEHnzp1T/vz5kyy7cuVKRUZGmnVwD/si8PAIAgEgA3Xs2FGhoaG6du2avv/+e7Vt2zbFOsuWLdPNmzfN+gDwJHJwcFDr1q01dOhQlSpVKsHyF198UQ0aNFCzZs0UExOjPn366NixY4leJ2rPnj0KCQnRnTt35OzsrFmzZql9+/Z2ZapUqaKWLVuqf//+aty4sc6ePasxY8aoWLFi6tmz5wM/j9DQUDMEDAgI0KJFi1SuXLlEy7Zv3163b9/WzJkz9cEHHzzwOgE8HSpXrqwTJ07o/Pnzmjdvnt55550ky8adDdKkSZNUj5IGgNTg1GAAyEAdO3Y0f9Sm9vTguC+CDg4O6tChwyPrGwA8SjVq1NDChQsTDQHjNGnSRM2bN5cknThxQnv27ElQJjY2Vh07dtSdO3ckSTNnzkwQAsZXvnx5bdiwQe7u7pKkAQMG6PTp0w/0HLZv366RI0dKkgoVKqQtW7YkGQLGcXV1Va9evbRv3z5VrVr1gdYL4OmQJUsWhYSESLK/7Mv9/ve//2nNmjWSuCQMgPRHEAgAGcjPz0/BwcGSpLVr1+rixYvJlj937pw2bNggSXrhhRdUoEABc9m2bdv0/vvvq3bt2sqbN6+cnZ3l5eWlUqVKqWfPnjp06FCybd9//bXz589r0KBBKl26tDw9PRNMEJDSrMEnT57UmDFj1LhxYxUpUkRubm5yc3OTr6+v2rRpY36hTa2oqCiNHj1aFStWlLe3t7y8vPTMM89o8uTJiomJSVNbiYmMjNSoUaMUHBysXLlyydnZWfny5VPjxo21ZMmSR34qWdyFw+Mu/L17926FhISoUKFCcnNzk7+/v/r376///e9/dvW2bNmiVq1aqXDhwnJ1dVWxYsU0aNAgXb9+Pcl13T/D3tGjR9WjRw8VLVpUrq6uypcvn1q3bq1t27alqu+//fabOnbsqCJFisjV1VU+Pj6qUKGC3n//fV26dCnJevfP3hkbG6uZM2fq+eefV548eeTg4KAuXbqYk1R8/fXXkqQ///zT7mLpic0meOfOHa1cuVK9e/dOMAnFM888o9DQ0ATb8n73vyZHjx5V9+7dVaRIEbm4uChPnjxq1qxZqrdTRESEBg0apEqVKpmTSOTMmVPPPfecQkNDdfLkySTrZvb+mZmef/558+8TJ04kWL5y5UodPHhQktSgQQPzR3VyAgICzBF5t27d0ueff/5AfRs1apS57SdPnqwcOXKkum6BAgX0wgsvJFvm+PHjevvttxUUFGROeuLn56cuXbpo586dSdZLbGbcRYsWqU6dOsqVK5fc3NxUokQJvfvuu7p8+XKq+vvdd9/ZHWt8fHxUuXJlDRs2TFeuXEmyXlo/W65cuaJZs2apQ4cOKlWqlDw8POTs7Ky8efOqfv36mjZtmhn6PojkZg2OPyFOam5JzZJ6+/ZtffHFF6pTp475eZw7d27VrVtXM2bM0N27d1Ps57Zt29SqVSvlzZtXrq6uKlq0qHk6++MguZlaE5tJdt26dWrcuLHy5s0rFxcXFS1aVD179tTZs2dTtb6NGzeqc+fO8vPzU9asWeXl5aWgoCANHDhQ586dS7bugQMHNHLkSNWvX18FCxaUi4uLPDw8VLx4cXXu3DnFY/j9s9ZGRkZqxIgRqlChgnx8fB56xty4szv27dunffv2JVpm/vz5io6OVu7cufXiiy+muu0H/XyOExMTo8mTJ+uZZ56Rl5eXvL29VbFiRY0ePdq8DENqPegxJCUZuS/+888/evfdd1WiRAm5ubkpT548qlevnnnJntmzZ5vri4iIeODnBGQ4AwCQoaZNm2ZIMiQZn3/+ebJlP/vsM7PsN998Yz4+a9Ys8/GkblmyZDEmTZqUZNudO3c2JBm+vr7G1q1bjZw5cyZoY+PGjWb5uMeGDh2aoK2TJ0+m2B9JRocOHYzo6OhE+xP/Oe3evduoVKlSku3UrFnTuH79eorPKynr1683cuTIkWxfX3755STXkRq+vr7J9iNueefOnY1vvvnGcHZ2TrQfAQEBxvnz5w3DuLc/2Gy2RMtVrFgxyf7WqlXLkGTUqlXLWL16teHu7p5oGw4ODsa4ceOSfE4xMTHGm2++mex28/b2Nn766adE62/cuNEs9+OPPxp169ZNUL9z587G0KFDU7U/xRf3uid3y5Ejh/Hbb7+l+Jp17tzZWLZsmZE1a9Yk31sLFixIsp2418rJySnZ/tSqVSvRug+7f8bfzp07d062n8mJ/zrEPxYkJ6X9PjXGjBljrnfp0qUJljdr1sxcvnbt2lS3e+XKFcPV1dXcF2JjY9PUrytXrhgODg6GJKNYsWJprp+SlPYZm81mfPDBB4nWjf+ab9iwwejQoUOS7fj7+5vHlMRcvnzZeOGFF5Ld/3Lnzm1s3bo10fpp/WyJ22eSu1WoUCHZPseVS+zzKf62uX8/Tu2xJu42a9asBO2Hh4en+ByqVKliXLhwIcn+jx071ty37r+5u7sbq1atsjuOP4hTp0491HEhufXHb3vWrFnG4MGDk9wWuXLlMg4dOpTkem7dumW0bds22e3p7u5urFixItH68V/v5G6DBw9Osg/x94s//vjDKFKkSKr2heTE7SNx269s2bKGJOOdd95JtHyVKlUMSUbfvn0Nw/i/fTyp1+5hP58NwzCuX79uPPfcc0nWr1ixorF79+4Ut8HDHkPib//EZNS+uG/fPiNPnjxJ1u/Ro4fdd9dTp04l2RbwuCEIBIAMdvXqVfPHaOXKlZMtG/dF0cPDw7hx44b5+PTp041s2bIZXbp0MWbOnGls2rTJ2L17t/HDDz8Yw4cPN3942Ww2Y8OGDYm2HfdjLUeOHEb+/PkNDw8P4z//+Y8RFhZmbN++3ZgxY4Zx5MgRs3xyP7SOHTtmODs7G40bNzYmTJhgrF+/3ti9e7exfv16Y/LkyUbp0qXN+h9++GGi/Yn/ZSruC3CbNm2M1atXGzt37jTmzZtnPi7JaNq0abLPK6kg4rfffjN/bOfJk8cYOXKksXLlSmPXrl3GypUr7X5AN2/ePIlXJmWpDQLLly9vODs7G6VKlTJmzpxp7Nixw/j555/t+hESEmIsXbrUkGRUq1bNmDt3rrFz505jzZo1xssvv2yWGzRoUKLrivvSXLx4ccPHx8fw9vY2Pv74Y2PLli3Gli1bjI8++sjw8vIy21m+fHmi7QwcONAsU7RoUePLL780tm/fbmzcuNF4++23ze3q7OxshIeHJ6gf/wda3L79yiuvGMuWLTN27dplrF692liwYIHx999/G/v37zeaNGliSDLy589v7N+/P8EtvpCQEMPPz88YMGCAsXDhQmPr1q3Gjh07jCVLlhhvvPGGGbTmypXL+Pvvv5N9TSpWrGi4uroaRYsWNb744gtj27ZtxtatW43Q0FDzvevl5WVcvHgx0XaGDx9uPk8fHx/jvffeM9atW2fs3r3b+Pnnn43Ro0cbNWrUMGrXrp2gbnrsn096EPjKK6+Y603sR1rc8S1r1qzG3bt309T2iy++aLZ94MCBNNVdsWKFWbdXr15pqpuSTz/91O69MWXKFGP9+vXGzp07jblz5xrVq1c3lyf2D6T4r3mNGjXMY2T891bDhg3NMm3btk20H7dv3zYqVqxoSPcC744dOxrz5883tm3bZmzatMn46KOPzJA6W7ZsRkRERII20vrZUrBgQeOZZ54xRowYYfzwww/Gjh07jM2bNxtz5swxXnrpJbPPyQVgyX0+JRcExh1rkrrt3bvXKF++vFn//s/TY8eOGd7e3uYxYciQIcby5cuNnTt3GmvXrjXefPNNw9HR0ZBkPPPMM8adO3cS9G/ZsmV2QU38Y/PIkSMNLy8vw8fHxyhevHiK2yE5GRUExu1/tWrVMubNm2fs3LnTWL9+vdGpUyezTLVq1RJdR2xsrN1+2rhxY+Pbb781Nm/ebGzdutX4/PPPjcKFC5ufMzt27EjQxrp16wx3d3ejdevWxpdffmmEhYUZu3fvNtasWWOMGTPGLrSdOXNmov2If+wrW7as4eTkZPTp08dYt26dsXPnTmP+/PnGli1b0rT97g8C4/7Rmz9/fiMmJsau7JEjR8z179y50zCMlIPAh/18NgzD/MyVZFStWtWYP3++sXPnTmPVqlVGq1at7L6fxQVt90uPY0h6BYEPsy9euXLFyJ8/v1muY8eOxo8//mjs3LnTWLBggXlMfuaZZwgC8UQiCASATNC6dWvzi0P8H0Tx7d271yzTqVMnu2Vnz541bt68mWT7V69eNYOWZ599NtEy8UdQeXh4JPnFME5yP7Ru3LhhnDt3Lsm6sbGxRpcuXQzp3n/yr169mqDM/aMcP/744wRloqOjjfr165tlVq1aleTzSiyIuHPnjvmf/ZdeeinJbRh/1GZy/z1PTmqDwLgvq4n1pWXLluaX6ezZsxstWrRIEHzcvXvXqFatmvnDO7ERl3FfmuN+aCYWrhw4cMAMAwsUKJDgB+u+ffvMEStlypQxrly5kqCNH3/80SxTtWrVBMvvH6nx/vvvJ7pt4qRmdGec48ePJztCa9++fYaHh0ey643/mlSqVMmIjIxMUGbOnDlmmbFjxyZYvnv3bnMbBAQEGGfOnEmyT6dPn7a7n17756MIAmfOnJlsYBJ3i/vh9KBBYHh4uJElSxZDkhEUFJRg+dmzZ1P8AZec+KND5s6dm6a6I0eONOt+9dVXaV53Ug4ePGj+SB86dGii+3FMTIwZAnt4eBiXL1+2W37/e2vkyJEJ2oiNjTWDUEdHx0SD7Pfee8+Q7gXYcQHE/SIiIox8+fIZkoz27dsnWJ7Wz5Y//vgj2eUzZ84021u/fn2iZR40CEzJO++8Y9Z97733EiyPCxoqVKhgXLp0KdE24h8Xp02bZrcsKirKfM8kdWzev3+/3T9qHvcgUJLRvXv3RPfj1157zSyze/fuBMvjjm1OTk7Gjz/+mGg/Ll++bP5zMTg4OMHyS5cuJfr5FCcqKsqoV6+eeZxK7J8J8Y99Dg4OaRp5nJT7g8Bz586Zx7r7j+P/+c9/DElGqVKlzMeSe+3S4/P5hx9+MNfx8ssvJ/pdYtiwYXavc2JBYHocQ9IrCHyYfbFfv37m8vHjxydYfvfuXbvglCAQTxqCQADIBPG/cP3nP/9JtEz8HyBJ/fhJznfffWfW/9///pdgefwfa8OHD0+xveR+aKXGP//8Y37pXbJkSYLl8YPAsmXLJhnqnDlzxvzR3LBhwwTLkwuPvvnmG0OS4erqmuRorjhVq1ZN8ktqaqQ2CLTZbEmemvLzzz+b2yRr1qzGP//8k2i5+D+U9+7dm2B5/CBw9OjRSfb5v//9r1lu8eLFdst69uxpLtu2bVuSbcT/cr19+3a7ZfF/kAcEBKQ4mistQWBqxH2xL1OmTKLL4weBiW1Hw7gXpsT9cG/WrFmC5e3atTNf18R+XCQnvfbPRxEEpvX2IK/Z7du3jcqVK5ttJHbqX3h4uLk8qVHByRk3bpxZf8KECWmqG/+H4ffff59kuVu3biUblt6va9euhnRvhHhyYfaVK1cMFxeXRAOl+K95pUqVkmxnzZo1ST6H69evm6PbJk6cmNymMCZPnmwGNvFHqxtG2j9bUiNuVF7v3r0TXf4ogsDZs2fb7Wv3b9Nff/3VXL5v375k24r751+NGjXsHl+0aFGaj82PexCYL18+4/bt24m2EX+k2/0jW2NjY41ixYoZkowBAwYk25fVq1eb7aQUJCcm/nEksbAq/rGva9euaW4/MfcHgYbxfyOU4/+zNzY21iw7atQo8/HkXrv0+HyOO7vAxcXF+OuvvxKtHxMTY5QpU8Zs4/4gML2OIekVBD7ovnj79m3Dx8fHkO6NgEzKhQsXzLMECALxpGGyEADIBPXr11eePHkkSXPnzk1w4f/Y2FjNmzdPklSwYEG7i+cn5ubNm4qIiNDBgwd14MABHThwQE5OTubyvXv3Jls/NRfbT4vo6GidPXtWhw8fNvtz7tw588L6KfWnc+fOCSaDiFOwYEHzwtlhYWFpmjhkxYoVkqRatWopV65cyZatWbOmJGnr1q2pbv9BlC1bVoGBgYkuiz8bab169ZQ9e/YUyyU3AYXNZlPnzp2TXP7qq6+a2339+vV2y+Luly5dWs8880ySbXTv3j1BncS0adNGWbJkSXL5w7py5YpOnDhh957w8fGRJB06dEjR0dFJ1g0KClLZsmUTXWaz2VShQgVJCbd1bGysfvzxR0n3LmYeVy610mv/rF27tox7/+x9qAvaZ7TevXubE2J07txZjRs3TlAm/qQ4Hh4eaV5H/DrXrl1LU934646bgTgxR44cUVBQUJK3+61cuVKS1KJFiySPe5Lk4+Nj1k/uuNS+ffsk26lUqZL59/377y+//KLIyEhJUsuWLZNsX/q//S86Olq7du1KslxaP1sMw9CFCxf0xx9/mO/bAwcOmBNlpfTZkV42b96sHj16SLp3jJ4zZ06CbRr3fi1RokSir2t8cdtrx44ddhOHxB0j03Jsfty1bNlSLi4uiS4rUaKE+R68f/87dOiQOTlQavc/KeXP6KioKJ0+fVqHDh0y96f437ky+vtRfHGThixbtkz//vuvJGnTpk36888/5eDgkOp1P+znc0xMjDmRzosvvqj8+fMnWt/BwSHZ/fRRHEMexoPuizt37tTVq1clSR06dEiy/Tx58qh+/frp01kggzlmdgcAwIocHR3Vvn17jRs3ThEREfrtt9/03HPPmcs3bNhgzooXEhIiB4eE/7f53//+p7Fjx2rp0qU6duxYsrOIJjdbqoeHh/z8/B7i2dwTHR2tadOm6dtvv9WePXuSneUxpdlbq1SpkuzyqlWratWqVbp586ZOnjyp4sWLp6qPcSHD2rVrU/2j6sKFC6kq96ACAgKSXBYXXKWlXHKzBxctWlQ5c+ZMcnmuXLlUpEgRnTp1Svv37zcfj4qK0rFjxyQp2R8ZklShQgU5OTkpOjpaBw4cSLJcUkHbw9i/f7/GjRunH3/8MdnXLTY2VleuXFHu3LkTXV6yZMlk1xMXyN6/rU+dOmX+eIj/fk6tx3H/jLNx48ZEZ2i8X5EiRfTnn3+muf1Ro0bpq6++knTv/T9p0qREy3l6epp/37hxI83riV/Hy8srTXXjr/vmzZtpXndi/vzzT3MmzyFDhmjIkCGpqpfc657c/hv/nwn377/xZyXOly9fqvqRXF/S8tmyatUqTZkyRb/++muyx7CUPjvSw+nTp9W8eXPduXNHuXPn1ooVKxINfuO219GjR1P9fo2Ojtbly5fNY0/ccTYtx+bHXUrHz2zZsunGjRvJ7n/Vq1dP9foS2/9u3rypCRMmaMGCBTp48GCy/zBMaZ96FJ9VcZo3b66ePXvqxo0bWr58uUJCQvTNN99IuvcPnUKFCqXYRnp8Pp84ccIMIlPz/Ssp6X0MeVgPui/G3zbx/3mSmMqVK+v7779/8E4CmYQRgQCQSTp16mT+/e2339oti38/frk4u3btUsmSJTVq1Cj98ccfyYaAknTr1q0kl8UPkR7U5cuXVb16dfXu3Vu///57siFgSv2RlGRAEyduNGXculPr4sWLqS4bJ6W+PqysWbMmuSx+AJzacsn94Elpu0r/t23jb9crV66kug0nJydz5Gdyr022bNlS7EtazJgxQxUrVtSsWbNS9aMiudc1uW0t/d/2vn9bx/9BmZYfQXEex/0zI0ydOlXvvfeepHs/3FavXp3kiLv4YcmD/Hj8+++/zb/j9tPUil8+LrxLTPny5c0RmXG3WrVqJVr2QV5zSeaP9sQ86LEivfuSms8WwzD02muvqVGjRlq1alWyIaD06Pf3GzduqHHjxrp48aKcnZ21bNky+fr6Jlo2PbZX3DEyLcfmx92DHj/Ta/+LiIhQUFCQ3nvvPe3bty/FswZS2qfS+7MqvqxZs6p58+aS7n3vu337tpYsWSLp/0YLpiQ9Pp/j/52W71/3exTHs4fxoPti/G2a0uj8lJYDjytGBAJAJilfvryCgoK0f/9+LV68WBMnTpSLi4tu3rypZcuWSbr3n8hSpUrZ1btz545at26tf/75R05OTurTp4+aNGmigIAAZcuWzTwN4uTJkypWrJgkJRsUpsfpmX379jVP7WjatKm6du2qsmXLKnfu3HJ1dTVHSxQuXFhnzpxJMbh8VKdAxX3Za9CggT799NNHso7HWXps1/R6bdLztOAjR47ojTfe0N27d5U7d24NHDhQL7zwgooUKSJPT0/zNPmZM2eqW7dukpJ/T2QWK+6f8+fPV69evSRJvr6+WrduXbIjo/Lnz6+cOXPqf//7n/kjPy370u7du82/459Snxrxy+/ZsydNdZMS/wfohx9+qFatWqWqXnKnJqdHX3bv3m13eYnkFCxYMNHHU/O6zJw5UzNmzJB07zOxX79+euaZZ1SgQAFlzZrVbKNTp0769ttvH+n71jAMdejQQfv27ZMkffnllwoODk6yfNz2KleunObMmZPq9cSd5hzfk3La76MUf/9buXKlihQpkqp69wdXHTt21KlTp2Sz2fTqq6+qbdu2CgwMVK5cueTs7CybzabY2Fhz30ppn3qUl7CI6+8333yj9evXa+rUqYqMjFTWrFnVokWLNLeV2Z/x6X0MAfDoEAQCQCbq1KmTBg4cqKtXr2rlypVq2bKlli9fbp52lthowJ9//tm8nsnkyZP12muvJdp2WkbKPYxr165p4cKFku6dxpzcD6L4/2VNzt9//53sqbDxR/Ukdd28xOTIkUPnzp3TnTt3VKZMmVTXe1rE324plYm/XeOPiEipjbt37+qff/5J0MajNHv2bN29e1dZsmTRL7/8kuTpQI/6PRE/wDp//nya61tt/1yxYoU6deqk2NhY5cuXTxs2bEjxB6HNZtOzzz6r7777Tv/++682bNhgXjM0JZGRkdq0aZOke9v6/n+ypOS5556Tg4ODYmNjtXbtWhmG8dA/vOOPMnRycsrU1z1+X3LlypUhP86nT58uSfL399eWLVvk5uaWaLmM+Dx77733zFP8+vfvr1dffTXZ8nHb68aNGw/8usUdW9NybH5axd//fHx8HmibHjlyRL/99puke6/nyJEjEy2XUd+PUuOFF15QgQIF9Ndff2nw4MGS7v1DNf6lCJKTHp/PaWkjueWZcQx5FOJvj0uXLiX7fTS50eHA44xTgwEgE4WEhJj/bY4L0OJOC3ZyclK7du0S1Dl48KD5d5s2bZJsO/61Wh6lY8eOmRMvJNefI0eOpPqaXjt27EjV8qxZs6bp+oZxkzfs3LkzxdOXn0anTp0yfwQk5tKlS4qIiJAkux9hLi4u5nUYf//992TXsWfPHnN/eNhQI7UhS9x7oly5csleE+hRvyeKFi1qng7566+/prm+lfbPDRs2qHXr1rp7965y5MihdevWmSOYU9KlSxfz7wkTJqR6ndOmTTNPA0xuQqKk+Pj4mBOYHD9+XGvWrElT/cT4+fnJ29tb0r0JKjJT/MltMqovce/dV155JckQ0DAMu5Gcj8LcuXP1ySefSLo3Ivezzz5LsU78SYMe9BpncZOMpOXY/LRKj/3vcft+lBrxJwW5ffu2pNSfFiylz+dzsWLFzPdfar9/JSYzjiGPQunSpc2/U5rI5HHal4C0IAgEgEyUL18+1a1bV5K0evVqHThwQBs2bJAkvfTSS4leeyT+jINJXbA+NjbWHGnxqKWmP9K906xSK7lTwP766y/99NNPku5dTDstp+288sorku6NDJo1a1aq6z0tDMMwL0SemNmzZ5vbPW6/jBN3/+DBg9q+fXuSbcRN+JBYG2nl6uoq6d7F0JMTtw8mt/+dP3/enOXzUXFwcFDDhg0l3Zs9Ma2nj1pl/9yyZYuaNGmiqKgoeXt7a+3atXY/vFLSuHFjc6btVatWacGCBSnWOX78uIYPHy5JcnNzU9++fR+o70OGDDEDxJ49ez70yKIsWbLo5ZdfliT99NNPOnz48EO19zDq1q1rXlNrwoQJGXL6fGreu99///0DjbBNrW3btpmXDAgMDNSCBQsSnaDrfnHvV8Mw9Pnnnz/QuuOOkWk5Nj+tKlasaI4gmzZtmhmKpcWj+D6SETp27CgXFxe5uLioUKFCqlevXprqP+zns6OjozkZ1E8//ZTk+y02NlZff/11sv3I6GPIo1C5cmXzHzTJneXy999/a+3atRnVLSBdEQQCQCaLO/03Ojpabdu2Na+xkthpwZLsZsidPXt2omWGDBnyyEdQxPH39zd/GH/99deJfvFbuXKlvvjii1S3GR4enuiIjLt376p79+7maKmePXumqa+dO3c2Z+F75513Uhy19dtvv+mXX35J0zoedyNGjNDRo0cTPH748GF99NFHku4F1E2aNLFb3rNnT/PHcY8ePXTt2rUEbfz000/m9b6qVq2a4uyDKYmbcOPixYvJTiIQ9544duyYtmzZkmD5v//+q/bt22fIxBrvvPOOHBwcZBiG2rZtq7NnzyZZ9v5l6bV/hoWFyWazyWaz2Y2eexyEh4erYcOGunnzptzd3bVq1aoUZ2W8n4ODg7799lvz+lNdunQxL0+QmH379qlOnTrmiOQxY8aocOHCD9T/Z555xpzZ988//9Szzz6b7OzY0r3jVnKjoYcMGaIsWbIoNjZWLVu2THafiYmJ0dy5c5Mt86B8fHzUu3dvSffC2rfffluxsbFJlv/777/tgoUHEffeXblyZaKh6okTJ/Tmm28+1DqSc+bMGTVr1kxRUVHKnj27Vq5cmerZpF988UVzBtXPPvtMixYtSrb8/v37tXLlSrvHmjZtah7nkjo2Hzp0yDw2P80cHBzMSYNOnjypTp06JftPoGvXriX4XpGa70dTpkx57GZ5LVOmjG7fvq3bt2/r9OnTab4uYXp8Psd9n4qKitLrr7+e6CQro0aNMme6TkxmHEMeBVdXV/M7+I4dOxIN+mNjY/X6668/UGANPA64RiAAZLJmzZrJ09NT169fN09ryZYtm3kK2v3q16+v3Llz6+LFi3r//fcVERGhZs2aKWfOnDp+/LimT5+uDRs2KDg4OENOzciRI4defvllrVq1SmvWrNGLL76onj17ytfXVxcvXtTSpUs1e/Zs+fn56erVq6m6nkrlypU1aNAghYeHq1OnTsqdO7eOHTumsWPHmv/tbty4sRo1apSmvrq4uGjRokWqXbu2bty4oRdeeEFt27ZV06ZNVbRoUcXGxur8+fPatWuXli9frv3792vixIlJzvj5pPH399elS5dUrVo1DRo0yBwBEBYWpk8++USRkZGSpIkTJ8rZ2dmublBQkAYMGKDPPvtMe/fuVcWKFTVo0CBVqFBBN2/e1MqVKzVhwgTFxMTI2dlZU6dOfej+1qhRQ9K9L9xvvPGG+vTpY3cdPn9/f0n3RlNMnDhRsbGxatiwoQYOHKhnn31Wrq6u2rVrl8aNG6djx45lyHuifPnyGjZsmD744AP98ccfCgoK0ptvvqnnn39eOXLk0NWrVxUeHq5ly5YpS5Ys2rhxo1n3ad8/T5w4ofr16+vq1auSpJEjR8rb2zvZIC137tyJzmJZqVIlffvtt2ZY0LZtW02dOlUhISEqWbKknJ2ddfr0aa1YsULz5s0zRwoNGDAgzf9AuN/w4cP1999/a8aMGTp8+LDKlSunRo0aqUGDBipRooSyZcumO3fu6K+//tL27du1YMEC87TOxE5/DQoK0ujRo/X222/r0KFDKlOmjHr06KEXXnhBefLk0e3btxUREaGtW7dqyZIlOn/+vPbv3/9Irr81fPhw/fLLL/r999/1+eefKywsTN27d1f58uXl7u6uK1eu6ODBg1q/fr1+/PFHBQUFJXmd2tSIu07uuXPnVL16dQ0aNMgMRX7++WeNHz9eUVFRqlix4iP551ZISIh5Wu/QoUN169atZPfHggUL2s2GPG/ePFWtWlWXL19WmzZtNGfOHLVp00bFixdXlixZdPHiRe3Zs0crV67Utm3bNGDAALvPdmdnZ02cOFEtW7bUlStX7I7NhmEoLCxM//3vfyXdO94dP348XZ738ePHkwzK4qtatWqar6X5MN544w2tW7dOy5cv1+LFi7V79269/vrrqlq1qry9vXXt2jUdOXJEYWFhWrFihVxdXc3gSbp3amqZMmV04MABTZ06VVeuXFHHjh2VL18+nT17VnPmzNGSJUsy7PtRRkmPz+fGjRurcePGWrlypVauXKng4GC9/fbbKl68uC5evKjZs2dr4cKFqly5crKnw2b0MeRRCQ0N1eLFi3XhwgX169dPu3btUkhIiHLlyqXjx4/r888/15YtW1S1alXzeymT/uCJYgAAMt2rr75qSDJvr7/+erLl16xZY7i6utrViX+rXbu2ceDAAfP+rFmzErTRuXNnQ5Lh6+ubqj7GtTV06NAEy06fPm0ULlw4yf4ULlzYOHjwoOHr62tIMjp37pygjVmzZpnld+/ebVSoUCHJ9oKDg41r164l2s/UPK+tW7cahQoVSrL9+Levv/46VdvnfnHPNal+JLct4ktuu8c5depUsq91rVq1DElGrVq1jB9++MHImjVros/VwcHBGD16dJLriYmJMXr16pXs9vL29jbWrl2baP2NGzea5TZu3Jjs845bX7Vq1ZJcV3zDhg1Ltl8DBgyw28dOnTqVYH2pfU1Ss4999NFHhqOjY7J9qlWrVqJ1H3b/jL+dU3ouyRk6dGiaXi/DSH6/j7/9U3tLbr83DMP45ZdfjMDAwBTbyZ49uzF16tS0b4RkTJ061ciTJ0+qnoejo6PRtm1bIyIiIsn2pk2bluR7M/7N2dnZOHbsmF3dtLy3Utq2165dM5o3b56q5/X8888nqJ+Wz5Y7d+4YL774YpLtu7m5GYsWLUqxzeSeU3LbJm5/Te0tsePr0aNHjTJlyqSq/rBhwxLt/2effWbYbLZE62TNmtX44Ycf7I7jDyL+50Rqb+PGjTPrJ7f+lD6D4kvpOHvnzh2jZ8+eSW6P+LeiRYsmqL9nzx4jW7ZsSdYJCgoyzp07l+w+E//Yl17inveDvn4pHdMf9vPZMO6994ODg5OsX6FCBWPXrl0pvtYPewxJaftn1L4YHh5u5MqVK8m+d+nSxZgxY4Z5/8KFC8muD3iccGowADwGOnfubHc/qdOC49SvX187d+5Uhw4dlD9/fjk5OSlXrlyqVauWpk2bpg0bNsjd3f1RdtlOoUKFtHv3bg0cOFABAQFycXGRt7e3ypUrp6FDhyo8PDxNowqyZcumLVu2aNSoUSpfvrw8PT3l4eGhKlWqaOLEifrll19SPaNeYqpVq6Zjx47pyy+/VMOGDZU/f345OzvL1dVVhQoV0osvvqiPPvpIR44cSfG1eNI0bNhQO3fu1KuvvipfX185Ozsrd+7catGihX777TcNGDAgyboODg6aNGmSfv31V4WEhKhw4cJycXGRl5eXypcvr/fee0/Hjh1L9SyuKXFwcNBPP/2k999/X+XKlZOHh0eS/3H/8MMPtWrVKr344ovKli2bnJ2dVbBgQTVv3lw//fSTRo8enS59Sq333ntPhw4dUr9+/VSmTBl5eXnJ0dHRfJ+OHDnSnBjoflbeP9OqZs2a2r9/vxYsWKB27drJ399fnp6e5raqX7++JkyYoBMnTqhHjx7puu4ePXro1KlTmj17ttq3b6+AgABly5ZNjo6O8vHxUfHixdWqVSuNHz9eZ86c0fz58+Xr65tke927d9fJkyc1bNgwBQcHK2fOnHJ0dJS7u7sCAgLUokULffnll/rrr7/M0bCPgqenp5YuXapNmzbptddeU4kSJeTp6SlHR0dlz55dVapU0ZtvvqnVq1dr3bp1D7UuJycnrVq1ShMmTFDlypWVNWtWubm5yd/fX2+88YZ2796tVq1apdMzezQCAgIUHh6uefPmqUWLFipcuLDc3Nzk7OysfPnyqXbt2nr//fe1a9cuffjhh4m28c477+i3335T8+bNlTt3brm4uMjX11ddu3bVzp07zWuPWoGTk5MmT56svXv3qk+fPgoKCpK3t7eyZMkib29vlS9fXt26ddOSJUsSvaZm+fLlFR4erjfeeEO+vr5ycnJS9uzZVbVqVY0ePVrbt283T8d+mqTH57Onp6fCwsI0ceJEValSRR4eHvL09FT58uU1atQobdmyxW624eTayahjyKNUrlw5HTp0SAMGDFDx4sXl4uKinDlz6vnnn9e8efM0a9Ysu9Ow464rCDwJbIbxhF7FEwAApErt2rX1yy+/qFatWgoLC8vs7gAAADzxXnvtNc2YMUMFCxbUmTNnMrs7QKoxIhAAAAAAACCVbt26ZU48U61atUzuDZA2BIEAAAAAAAD/34kTJ5TUyZMxMTHq2bOn/ve//0lKeIkf4HHHrMEAAAAAAAD/34gRI7R9+3a1bdtWzzzzjHLnzq1bt25p3759mj59ujmTed26dS11HU88HQgCAQAAAAAA4jl8+LCGDh2a5PLg4GAtWLAgyYnMgMcVQSAAAAAAAMD/N2TIEAUEBGj9+vWKiIjQpUuXFB0drRw5cqhy5cpq06aN2rZtKwcHrraGJw+zBgMAAAAAAAAWQHwNyzAMQ7dv307yoq8AAAAAAABPM4JAWEZUVJRGjhypqKiozO4KAAAAAABAhiMIBAAAAAAAACyAIBAAAAAAAACwAIJAAAAAAAAAwAIIAgEAAAAAAAALIAgEAAAAAAAALIAgEAAAAAAAALAAgkAAAAAAAADAAggCAQAAAAAAAAsgCAQAAAAAAAAsgCAQAAAAAAAAsACCQAAAAAAAAMACCAIBAAAAAAAACyAIBAAAAAAAACyAIBAAAAAAAACwAIJAAAAAAAAAwAIIAgEAAAAAAAALIAgEAAAAAAAALIAgEAAAAAAAALAAgkAAAAAAAADAAggCAQAAAAAAAAsgCAQAAAAAAAAsgCAQAAAAAAAAsACCQAAAAAAAAMACCAIBAAAAAAAACyAIBAAAAAAAACyAIBAAAAAAAACwAIJAAAAAAAAAwAIIAgEAAAAAAAALIAgEAAAAAAAALIAgEAAAAAAAALAAgkAAAAAAAADAAggCAQAAAAAAAAsgCAQAAAAAAAAsgCAQAAAAAAAAsACCQAAAAAAAAMACCAIBAAAAAAAACyAIBAAAAAAAACyAIBAAAAAAAACwAIJAAAAAAAAAwAIIAgEAAAAAAAALIAgEAAAAAAAALMAxszsAZLSg0LWKZtcHAAAAAMDSIj5pmNldyHCMCAQAAAAAAAAsgCAQAAAAAAAAsACCQAAAAAAAAMACCAIBAAAAAAAACyAIBAAAAAAAACyAIBAAAAAAAACwAIJAAAAAAAAAwAIIAgEAAAAAAAALIAgEAAAAAAAALIAgEAAAAAAAALAAgkAAAAAAAADAAggCAQAAAAAAAAsgCAQAAAAAAAAsgCAQAAAAAAAAsACCQAAAAAAAAMACCAIBAAAAAAAACyAIBAAAAAAAACyAIBAAAAAAAACwAIJAAAAAAAAAwAIIAgEAAAAAAAALIAgEAAAAAAAALIAgEAAAAAAAALCApy4IjIiIkM1mU3h4eLqWBQAAAAAAAJ5kaQoCu3TpoqZNmz6iriSvdu3astlsstlscnFxUYECBdS4cWMtW7bMrlyhQoV0/vx5lSlTJlP6+TSYPXu2fHx8El12/Phxde3aVYULFzZfhzp16mju3Lm6e/duxnYUAAAAAAAAqfZEjQjs3r27zp8/rxMnTmjp0qUqVaqU2rZtqx49ephlsmTJorx588rR0THT+nnnzp1MW/ejtH37dlWsWFGHDx/WpEmTdODAAYWFhem1117TlClTdPDgwSTrRkdHZ2BPAQAAAAAAcL90CwJ/+eUXVa1aVS4uLsqXL58GDx5sN0Ls+vXrCgkJkbu7u/Lly6dx48apdu3a6tevX6rXkTVrVuXNm1cFCxZUtWrV9N///ldTp07V9OnTtX79ekkJT/e9cuWKQkJClCtXLrm5ual48eKaNWuWXbtHjhxRjRo15OrqqjJlyuiXX34xlyU2Ou67776TzWYz74eGhqp8+fL66quvVLRoUbm6ukqSlixZoqCgILm5uSlHjhyqW7eubt68adb76quvFBgYKFdXV5UsWVKTJ082l8U9j0WLFum5556Tm5ubqlSpoj/++EM7duxQ5cqV5eHhoQYNGujSpUt2/UtNu8uWLdPzzz+vrFmzqly5ctq6daskKSwsTK+++qoiIyPNEZihoaEyDENdunRRQECANm/erMaNG6t48eIqXry42rVrp99++01ly5a1W8fChQtVq1Ytubq6au7cuYqNjdXw4cNVsGBBubi4qHz58lqzZo3Ztzt37qh3797Kly+fXF1d5evrq1GjRkmSDMNQaGioORIxf/78euutt1Kx1wAAAAAAAECS0mXY3F9//aWXX35ZXbp00TfffKMjR46oe/fucnV1VWhoqCSpf//+2rx5s1asWKE8efLoww8/1O7du1W+fPmHWnfnzp01YMAALVu2THXr1k2w/IMPPtChQ4f0448/KmfOnDp+/Lhu3bplV2bgwIEaP368SpUqpbFjx6px48Y6deqUcuTIkep+HD9+XEuXLtWyZcuUJUsWnT9/Xu3atdOnn36qZs2a6fr169q0aZMMw5AkzZ07Vx9++KG++OILVahQQXv27FH37t3l7u6uzp07m+0OHTpU48ePV+HChdW1a1e1b99enp6e+vzzz5U1a1a1bt1aH374oaZMmZKmdv/zn/9o9OjRKl68uP7zn/+oXbt2On78uGrUqKHx48frww8/1NGjRyVJHh4eCg8P1+HDhzV//nw5OCSeH8cPRyVp8ODBGjNmjCpUqCBXV1d9/vnnGjNmjKZOnaoKFSpo5syZeuWVV3Tw4EEVL15cEyZM0IoVK7Ro0SIVLlxYZ86c0ZkzZyRJS5cu1bhx47RgwQKVLl1aFy5c0N69e5N9TaKiohQVFWV3HwAAAAAAwKrSJQicPHmyChUqpC+++EI2m00lS5bUuXPnNGjQIH344Ye6efOmvv76a82bN0916tSRJM2aNUv58+d/6HU7ODgoICBAERERiS4/ffq0KlSooMqVK0uSihQpkqBM79691aJFC0nSlClTtGbNGs2YMUPvvvtuqvtx584dffPNN8qVK5ckaffu3bp7966aN28uX19fSVJQUJBZfujQoRozZoyaN28uSSpatKgOHTqkqVOn2gV277zzjurXry9J6tu3r9q1a6cNGzYoODhYktStWzfNnj37gdpt2LChJGnYsGEqXbq0jh8/rpIlS8rb21s2m0158+Y1y//xxx+SpBIlSpiPXbx4UX5+fub9Tz/9VL169TLv9+vXz+yHJI0ePVqDBg1S27ZtJUn//e9/tXHjRo0fP16TJk3S6dOnVbx4cT377LOy2WzmdpPuvY558+ZV3bp15eTkpMKFC6tq1arJviajRo3SsGHDzPvOzs4aOHBgsnUAAAAAAACeVulyavDhw4dVvXp1uxFhwcHBunHjhs6ePauTJ08qOjraLrjx9va2C5UehmEYCUajxenZs6cWLFig8uXL691339WWLVsSlKlevbr5t6OjoypXrqzDhw+nqQ++vr5mCChJ5cqVU506dRQUFKRWrVpp+vTpunLliiTp5s2bOnHihLp16yYPDw/zNnLkSJ04ccKu3bjTbSUpT548kuwDxTx58ujixYsP1W6+fPkkyWwntXLkyKHw8HCFh4fLx8cnwbUR48JXSbp27ZrOnTtnBphxgoODzW3dpUsXhYeHq0SJEnrrrbf0008/meVatWqlW7duyc/PT927d9fy5ctTnJxkyJAhioyMNG9nz55N0/MDAAAAAAB4mjxRk4UkJiYmRseOHVPRokUTXd6gQQP9+eefevvtt3Xu3DnVqVNH77zzTqrbd3BwME/njZPYxBfu7u5297NkyaJ169bpxx9/VKlSpTRx4kSVKFFCp06d0o0bNyRJ06dPN4O08PBwHThwQNu2bbNrx8nJyfw7Luy8/7HY2FhJeuh249pJTPHixSXJPF047jn6+/vL398/0clZ7t8mKalYsaJOnTqlESNG6NatW2rdurVatmwp6d5s0EePHtXkyZPl5uamXr16qWbNmslOQuLi4iIvLy/z5unpmab+AAAAAAAAPE3SJQgMDAzU1q1b7QKzzZs3y9PTUwULFpSfn5+cnJy0Y8cOc3lkZKR5uunD+Prrr3XlyhXz1N7E5MqVS507d9acOXM0fvx4TZs2zW55/JDs7t272rVrlwIDA826169ft5vkI24ikpTYbDYFBwdr2LBh2rNnj5ydnbV8+XLlyZNH+fPn18mTJ80gLe6WVKCZGunVrrOzs2JiYuweq1ChgkqWLKnRo0cnGxgmxcvLS/nz59fmzZvtHt+8ebNKlSplV65NmzaaPn26Fi5cqKVLl+ry5cuSJDc3NzVu3FgTJkxQWFiYtm7dqv3796e5LwAAAAAAAFaU5msERkZGJgjCevToofHjx6tPnz7q3bu3jh49qqFDh6p///5ycHCQp6enOnfurIEDByp79uzKnTu3hg4dKgcHhyRP6U3Mv//+qwsXLuju3bs6e/asli9frnHjxqlnz556/vnnE63z4YcfqlKlSipdurSioqL0ww8/mCFfnEmTJql48eIKDAzUuHHjdOXKFXXt2lWS9Mwzzyhr1qx677339NZbb+n333+3uyZfUn7//Xdt2LBBL774onLnzq3ff/9dly5dMtc9bNgwvfXWW/L29tZLL72kqKgo7dy5U1euXFH//v1TvU3ulx7tFilSRDdu3NCGDRtUrlw5Zc2aVVmzZtWsWbNUr149BQcHa8iQIQoMDFR0dLR+/fVXXbp0SVmyZEm23YEDB2ro0KEqVqyYypcvr1mzZik8PFxz586VJI0dO1b58uVThQoV5ODgoMWLFytv3rzy8fHR7NmzFRMTY74ec+bMkZubm911BAEAAAAAAJC0NAeBYWFhqlChgt1j3bp10+rVqzVw4ECVK1dO2bNnV7du3fT++++bZcaOHas33nhDjRo1kpeXl959912dOXNGrq6uqV739OnTNX36dDk7OytHjhyqVKmSFi5cqGbNmiVZx9nZWUOGDFFERITc3Nz03HPPacGCBXZlPvnkE33yyScKDw+Xv7+/VqxYoZw5c0qSsmfPrjlz5mjgwIGaPn266tSpo9DQUPXo0SPZvnp5eenXX3/V+PHjde3aNfn6+mrMmDFq0KCBJOm1115T1qxZ9dlnn2ngwIFyd3dXUFCQ+vXrl+rtkZj0aLdGjRp644031KZNG/3zzz8aOnSoQkNDVa1aNe3atUsff/yx3nzzTV24cEHu7u4qV66cxo0bZ4anSXnrrbcUGRmpAQMG6OLFiypVqpRWrFhhnnbs6empTz/9VMeOHVOWLFlUpUoVrV69Wg4ODvLx8dEnn3yi/v37KyYmRkFBQVq5cmWaZnYGAAAAAACwMptx/wXwMsjNmzdVoEABjRkzRt26dcuMLsBibt++rZEjR2rh3SqKTp8JswEAAAAAwBMq4pOGmd2FDJdhaciePXt05MgRVa1aVZGRkRo+fLgkqUmTJhnVBQAAAAAAAMCyMnRY1OjRo3X06FE5OzurUqVK2rRpk3LmzKlNmzaZp8wmJm42XAAAAAAAAAAPJsOCwAoVKmjXrl2JLqtcuXKqZ+IFAAAAAAAAkHaPxYXS3Nzc5O/vn9ndAAAAAAAAAJ5aDpndAQAAAAAAAACPHkEgAAAAAAAAYAEEgQAAAAAAAIAFEAQCAAAAAAAAFkAQCAAAAAAAAFgAQSAAAAAAAABgAQSBAAAAAAAAgAUQBAIAAAAAAAAWQBAIAAAAAAAAWABBIAAAAAAAAGABBIEAAAAAAACABRAEAgAAAAAAABZAEAgAAAAAAABYAEEgAAAAAAAAYAEEgQAAAAAAAIAFEAQCAAAAAAAAFkAQCAAAAAAAAFiAY2Z3AMho+0Pry9XVNbO7AQAAAAAAkKEYEQgAAAAAAABYAEEgAAAAAAAAYAEEgQAAAAAAAIAFEAQCAAAAAAAAFkAQCAAAAAAAAFgAQSAAAAAAAABgAQSBAAAAAAAAgAUQBAIAAAAAAAAWQBAIAAAAAAAAWABBIAAAAAAAAGABBIEAAAAAAACABRAEAgAAAAAAABZAEAgAAAAAAABYgGNmdwDIaEGhaxXNrg8AAAA8kSI+aZjZXQCAJxYjAgEAAAAAAAALIAgEAAAAAAAALIAgEAAAAAAAALAAgkAAAAAAAADAAggCAQAAAAAAAAsgCAQAAAAAAAAsgCAQAAAAAAAAsACCQAAAAAAAAMACCAIBAAAAAAAACyAIBAAAAAAAACyAIBAAAAAAAACwAIJAAAAAAAAAwAIIAgEAAAAAAAALIAgEAAAAAAAALIAgEAAAAAAAALAAgkAAAAAAAADAAggCAQAAAAAAAAsgCAQAAAAAAAAsgCAQAAAAAAAAsACCQAAAAAAAAMACCAIBAAAAAAAACyAIBAAAAAAAACyAIBAAAAAAAACwAIJA6MKFC+rbt6/8/f3l6uqqPHnyKDg4WFOmTNG///6bqjbq16+vLFmyaMeOHY+4twAAAAAAAHgQjpndAWSukydPKjg4WD4+Pvr4448VFBQkFxcX7d+/X9OmTVOBAgX0yiuvJKgXHR0tJycnSdLp06e1ZcsW9e7dWzNnzlSVKlUy+mkAAAAAAAAgBYwItLhevXrJ0dFRO3fuVOvWrRUYGCg/Pz81adJEq1atUuPGjSVJNptNU6ZM0SuvvCJ3d3d99NFHZhuzZs1So0aN1LNnT82fP1+3bt2yW0ft2rXVu3dv9e7dW97e3sqZM6c++OADGYZhlilSpIhGjBihdu3ayd3dXQUKFNCkSZPM5e3bt1ebNm3s2o2OjlbOnDn1zTffPIpNAwAAAAAA8FQhCLSwf/75Rz/99JPefPNNubu7J1rGZrOZf4eGhqpZs2bav3+/unbtKkkyDEOzZs1Shw4dVLJkSfn7+2vJkiUJ2vn666/l6Oio7du36/PPP9fYsWP11Vdf2ZX57LPPVK5cOe3Zs0eDBw9W3759tW7dOklSSEiIVq5cqRs3bpjl165dq3///VfNmjVLtO9RUVG6du2aebt+/XraNhAAAAAAAMBThCDQwo4fPy7DMFSiRAm7x3PmzCkPDw95eHho0KBB5uPt27fXq6++Kj8/PxUuXFiStH79ev3777+qX7++JKlDhw6aMWNGgnUVKlRI48aNU4kSJRQSEqI+ffpo3LhxdmWCg4M1ePBgBQQEqE+fPmrZsqVZpn79+nJ3d9fy5cvN8vPmzdMrr7wiT0/PRJ/fqFGj5O3tbd4KFiz4AFsJAAAAAADg6UAQiAS2b9+u8PBwlS5dWlFRUebjlStXTlB25syZatOmjRwd711usl27dtq8ebNOnDhhV65atWp2owurV6+uY8eOKSYmxu6x+KpXr67Dhw9LkhwdHdW6dWvNnTtXknTz5k19//33CgkJSfJ5DBkyRJGRkebt7Nmzqd0EAAAAAAAATx2CQAvz9/eXzWbT0aNH7R738/OTv7+/3Nzc7B6///Thy5cva/ny5Zo8ebIcHR3l6OioAgUK6O7du5o5c2a69zckJEQbNmzQxYsX9d1338nNzU0vvfRSkuVdXFzk5eVl3pIaOQgAAAAAAGAFBIEWliNHDtWrV09ffPGFbt68meb6c+fOVcGCBbV3716Fh4ebtzFjxmj27Nl2o/1+//13u7rbtm1T8eLFlSVLFrvH7i8TGBho3q9Ro4YKFSqkhQsXau7cuWrVqpU5czEAAAAAAACS55jZHUDmmjx5soKDg1W5cmWFhoaqbNmycnBw0I4dO3TkyBFVqlQpybozZsxQy5YtVaZMGbvHCxUqpCFDhmjNmjVq2LChJOn06dPq37+/Xn/9de3evVsTJ07UmDFj7Opt3rxZn376qZo2bap169Zp8eLFWrVqlV2Z9u3b68svv9Qff/yhjRs3ptNWAAAAAAAAePoRBFpcsWLFtGfPHn388ccaMmSIzp49KxcXF5UqVUrvvPOOevXqlWi9Xbt2ae/evZo+fXqCZd7e3qpTp45mzJhhBoGdOnXSrVu3VLVqVWXJkkV9+/ZVjx497OoNGDBAO3fu1LBhw+Tl5aWxY8eak5DECQkJ0UcffSRfX18FBwen01YAAAAAAAB4+hEEQvny5dPEiRM1ceLEJMsYhmF3v1KlSgkei2/16tV2952cnDR+/HhNmTIlyTpeXl5atGhRsn0NDAxMdr0AAAAAAABIHNcIBAAAAAAAACyAIBAAAAAAAACwAE4NxiMXFhaWYpmIiIhH3g8AAAAAAAArY0QgAAAAAAAAYAEEgQAAAAAAAIAFEAQCAAAAAAAAFkAQCAAAAAAAAFgAQSAAAAAAAABgAQSBAAAAAAAAgAUQBAIAAAAAAAAWQBAIAAAAAAAAWABBIAAAAAAAAGABBIEAAAAAAACABRAEAgAAAAAAABZAEAgAAAAAAABYAEEgAAAAAAAAYAEEgQAAAAAAAIAFEAQCAAAAAAAAFkAQCAAAAAAAAFgAQSAAAAAAAABgAQSBAAAAAAAAgAU4ZnYHgIy2P7S+XF1dM7sbAAAAAAAAGYoRgQAAAAAAAIAFEAQCAAAAAAAAFkAQCAAAAAAAAFgAQSAAAAAAAABgAQSBAAAAAAAAgAUQBAIAAAAAAAAWQBAIAAAAAAAAWABBIAAAAAAAAGABBIEAAAAAAACABRAEAgAAAAAAABZAEAgAAAAAAABYAEEgAAAAAAAAYAEEgQAAAAAAAIAFOGZ2B4CMFhS6VtHs+gAAAECqRXzSMLO7AABIB4wIBAAAAAAAACyAIBAAAAAAAACwAIJAAAAAAAAAwAIIAgEAAAAAAAALIAgEAAAAAAAALIAgEAAAAAAAALAAgkAAAAAAAADAAggCAQAAAAAAAAsgCAQAAAAAAAAsgCAQAAAAAAAAsACCQAAAAAAAAMACCAIBAAAAAAAACyAIBAAAAAAAACyAIBAAAAAAAACwAIJAAAAAAAAAwAIIAgEAAAAAAAALIAgEAAAAAAAALIAgEAAAAAAAALAAgkAAAAAAAADAAggCAQAAAAAAAAsgCAQAAAAAAAAsgCAQAAAAAAAAsACCQAAAAAAAAMACCAIzUZcuXdS0adNMW//x48fVtWtXFS5cWC4uLipQoIDq1KmjuXPn6u7du6lup2TJknJxcdGFCxceYW8BAAAAAADwMAgCLWr79u2qWLGiDh8+rEmTJunAgQMKCwvTa6+9pilTpujgwYNJ1o2Ojjb//u2333Tr1i21bNlSX3/9dUZ0HQAAAAAAAA+AIPAx9csvv6hq1apycXFRvnz5NHjwYLtRetevX1dISIjc3d2VL18+jRs3TrVr11a/fv1SbNswDHXp0kUBAQHavHmzGjdurOLFi6t48eJq166dfvvtN5UtW1aSFBERIZvNpoULF6pWrVpydXXV3LlzzbZmzJih9u3bq2PHjpo5c2aCdRUpUkQjRoxQu3bt5O7urgIFCmjSpEl2ZWw2m6ZMmaIGDRrIzc1Nfn5+WrJkibm8Ro0aGjRokF2dS5cuycnJSb/++muqticAAAAAAIDVEQQ+hv766y+9/PLLqlKlivbu3aspU6ZoxowZGjlypFmmf//+2rx5s1asWKF169Zp06ZN2r17d6raDw8P1+HDh/XOO+/IwSHxXcBms9ndHzx4sPr27avDhw+rfv36ku6FkYsXL1aHDh1Ur149RUZGatOmTQna+uyzz1SuXDnt2bPHbGfdunV2ZT744AO1aNFCe/fuVUhIiNq2bavDhw9LkkJCQrRgwQIZhmGWX7hwofLnz6/nnnsuyecZFRWla9eumbfr16+navsAAAAAAAA8jQgCH0OTJ09WoUKF9MUXX6hkyZJq2rSphg0bpjFjxig2NlbXr1/X119/rdGjR6tOnToqU6aMZs2apZiYmFS1/8cff0iSSpQoYT528eJFeXh4mLfJkyfb1enXr5+aN2+uokWLKl++fJKkBQsWqHjx4ipdurSyZMmitm3basaMGQnWFxwcrMGDBysgIEB9+vRRy5YtNW7cOLsyrVq10muvvaaAgACNGDFClStX1sSJEyVJrVu31rlz5/Tbb7+Z5efNm6d27dolCCzjGzVqlLy9vc1bwYIFU7V9AAAAAAAAnkYEgY+hw4cPq3r16nYhV3BwsG7cuKGzZ8/q5MmTio6OVtWqVc3l3t7edsFeWuXIkUPh4eEKDw+Xj4+P7ty5Y7e8cuXKCerMnDlTHTp0MO936NBBixcvTjDyrnr16gnux432S02ZXLly6cUXXzRPST516pS2bt2qkJCQZJ/TkCFDFBkZad7Onj2bbHkAAAAAAICnGUGgBRUvXlySdPToUfOxLFmyyN/fX/7+/nJ0dExQx93d3e7+oUOHtG3bNr377rtydHSUo6OjqlWrpn///VcLFixI9z6HhIRoyZIlio6O1rx58xQUFKSgoKBk67i4uMjLy8u8eXp6pnu/AAAAAAAAnhQEgY+hwMBAbd261e6aeJs3b5anp6cKFiwoPz8/OTk5aceOHebyyMhI85TflFSoUEElS5bU6NGjFRsb+0B9nDFjhmrWrKm9e/eaIwnDw8PVv3//BKcHb9u2LcH9wMDANJVp0qSJbt++rTVr1mjevHkpjgYEAAAAAACAvYRDv5ChIiMjFR4ebvdYjx49NH78ePXp00e9e/fW0aNHNXToUPXv318ODg7y9PRU586dNXDgQGXPnl25c+fW0KFD5eDgkOw18+LYbDbNmjVL9erVU3BwsIYMGaLAwEBFR0fr119/1aVLl5QlS5Yk60dHR+vbb7/V8OHDVaZMGbtlr732msaOHauDBw+qdOnSku6FmJ9++qmaNm2qdevWafHixVq1apVdvcWLF6ty5cp69tlnNXfuXG3fvt0uUHR3d1fTpk31wQcf6PDhw2rXrl2KzxMAAAAAAAD/hyAwk4WFhalChQp2j3Xr1k2rV6/WwIEDVa5cOWXPnl3dunXT+++/b5YZO3as3njjDTVq1EheXl569913debMGbm6uqZqvdWqVdOuXbv08ccf680339SFCxfk7u6ucuXKady4ceratWuSdVesWKF//vlHzZo1S7AsMDBQgYGBmjFjhsaOHStJGjBggHbu3Klhw4bJy8tLY8eONWcejjNs2DAtWLBAvXr1Ur58+TR//nyVKlXKrkxISIhefvll1axZU4ULF07V8wQAAAAAAMA9NiP++ad4Yt28eVMFChTQmDFj1K1bt8zujqlIkSLq16+f+vXrl2QZm82m5cuXq2nTpo+0L7dv39bIkSO18G4VRZOBAwAAAKkW8UnDzO4CACAdkIY8ofbs2aMjR46oatWqioyM1PDhwyXdu5YeAAAAAAAAcD+CwCfY6NGjdfToUTk7O6tSpUratGmTcubMqU2bNqlBgwZJ1rtx40YG9hIAAAAAAACPA4LAJ1SFChW0a9euRJdVrlw5wQQkmSUiIiLFMpydDgAAAAAA8OgRBD6F3Nzc5O/vn9ndAAAAAAAAwGPEIbM7AAAAAAAAAODRIwgEAAAAAAAALIAgEAAAAAAAALAAgkAAAAAAAADAAggCAQAAAAAAAAsgCAQAAAAAAAAsgCAQAAAAAAAAsACCQAAAAAAAAMACCAIBAAAAAAAACyAIBAAAAAAAACyAIBAAAAAAAACwAIJAAAAAAAAAwAIIAgEAAAAAAAALIAgEAAAAAAAALIAgEAAAAAAAALAAgkAAAAAAAADAAggCAQAAAAAAAAtwzOwOABltf2h9ubq6ZnY3AAAAAAAAMhQjAgEAAAAAAAALIAgEAAAAAAAALIAgEAAAAAAAALAAgkAAAAAAAADAAggCAQAAAAAAAAsgCAQAAAAAAAAsgCAQAAAAAAAAsACCQAAAAAAAAMACCAIBAAAAAAAACyAIBAAAAAAAACyAIBAAAAAAAACwAIJAAAAAAAAAwAIIAgEAAAAAAAALcMzsDgAZLSh0raLZ9QEAAPCUifikYWZ3AQDwmGNEIAAAAAAAAGABBIEAAAAAAACABRAEAgAAAAAAABZAEAgAAAAAAABYAEEgAAAAAAAAYAEEgQAAAAAAAIAFEAQCAAAAAAAAFkAQCAAAAAAAAFgAQSAAAAAAAABgAQSBAAAAAAAAgAUQBAIAAAAAAAAWQBAIAAAAAAAAWABBIAAAAAAAAGABBIEAAAAAAACABRAEAgAAAAAAABZAEAgAAAAAAABYAEEgAAAAAAAAYAEEgQAAAAAAAIAFEAQCAAAAAAAAFkAQCAAAAAAAAFgAQSAAAAAAAABgAQSBAAAAAAAAgAU8lUGgzWbTd999l+5lAQAAAAAAgCfVExUEdunSRTabTTabTU5OTsqTJ4/q1aunmTNnKjY21ix3/vx5NWjQIBN7+mQLCwuTzWbT1atXEyy7cOGC+vbtK39/f7m6uipPnjwKDg7WlClT9O+//2Z8ZwEAAAAAAJAqjpndgbR66aWXNGvWLMXExOjvv//WmjVr1LdvXy1ZskQrVqyQo6Oj8ubNm6l9jImJkc1mk4PDE5WzpujkyZMKDg6Wj4+PPv74YwUFBcnFxUX79+/XtGnTVKBAAb3yyiuJ1o2OjpaTk1MG9xgAAAAAAABxnrikysXFRXnz5lWBAgVUsWJFvffee/r+++/1448/avbs2ZLsT/e9c+eOevfurXz58snV1VW+vr4aNWqUXZtxIwjd3Nzk5+enJUuWmMsSGx0XHh4um82miIgISdLs2bPl4+OjFStWqFSpUnJxcdHp06cVFhamqlWryt3dXT4+PgoODtaff/5ptvP999+rYsWKcnV1lZ+fn4YNG6a7d++ay202m6ZOnapGjRopa9asCgwM1NatW3X8+HHVrl1b7u7uqlGjhk6cOGH3fFLT7ldffaVmzZopa9asKl68uFasWCFJioiI0PPPPy9JypYtm2w2m7p06SJJ6tWrlxwdHbVz5061bt1agYGB8vPzU5MmTbRq1So1btzYbh1TpkzRK6+8Ind3d3300UeSpClTpqhYsWJydnZWiRIl9O2335p1DMNQaGioChcuLBcXF+XPn19vvfWWuXzy5MkqXry4ORKxZcuWSe8oAAAAAAAAsPPEBYGJeeGFF1SuXDktW7YswbIJEyZoxYoVWrRokY4ePaq5c+eqSJEidmU++OADtWjRQnv37lVISIjatm2rw4cPp6kP//77r/773//qq6++0sGDB5U9e3Y1bdpUtWrV0r59+7R161b16NFDNptNkrRp0yZ16tRJffv21aFDhzR16lTNnj3bDMzijBgxQp06dVJ4eLhKliyp9u3b6/XXX9eQIUO0c+dOGYah3r17m+VT2+6wYcPUunVr7du3Ty+//LJCQkJ0+fJlFSpUSEuXLpUkHT16VOfPn9fnn3+uf/75Rz/99JPefPNNubu7J7oN4p5bnNDQUDVr1kz79+9X165dtXz5cvXt21cDBgzQgQMH9Prrr+vVV1/Vxo0bJUlLly7VuHHjNHXqVB07dkzfffedgoKCJEk7d+7UW2+9peHDh+vo0aNas2aNatasmexrEhUVpWvXrpm369evp/QyAgAAAAAAPLWeiiBQkkqWLGmO0Ivv9OnTKl68uJ599ln5+vrq2WefVbt27ezKtGrVSq+99poCAgI0YsQIVa5cWRMnTkzT+qOjozV58mTVqFFDJUqU0N27dxUZGalGjRqpWLFiCgwMVOfOnVW4cGFJ94K4wYMHq3PnzvLz81O9evU0YsQITZ061a7dV199Va1bt1ZAQIAGDRqkiIgIhYSEqH79+goMDFTfvn0VFhZmlk9tu126dFG7du3k7++vjz/+WDdu3ND27duVJUsWZc+eXZKUO3du5c2bV97e3jp+/LgMw1CJEiXs2smZM6c8PDzk4eGhQYMG2S1r3769Xn31Vfn5+alw4cIaPXq0unTpol69eikgIED9+/dX8+bNNXr0aPO1yps3r+rWravChQuratWq6t69u7nM3d1djRo1kq+vrypUqGA3WjAxo0aNkre3t3krWLBgKl9NAAAAAACAp89TEwQahpFgRJp0L/AKDw9XiRIl9NZbb+mnn35KUKZ69eoJ7qd1RKCzs7PKli1r3s+ePbu6dOmi+vXrq3Hjxvr88891/vx5c/nevXs1fPhwM0Tz8PBQ9+7ddf78ebtJN+K3mSdPHkkyR8nFPXb79m1du3btgdt1d3eXl5eXLl68mKbnLEnbt29XeHi4SpcuraioKLtllStXtrt/+PBhBQcH2z0WHBxsbutWrVrp1q1b8vPzU/fu3bV8+XLzlOZ69erJ19dXfn5+6tixo+bOnZvi5CRDhgxRZGSkeTt79myanx8AAAAAAMDT4qkJAg8fPqyiRYsmeLxixYo6deqURowYoVu3bql169ZpurZc3IQfhmGYj0VHRyco5+bmliCInDVrlrZu3aoaNWpo4cKFCggI0LZt2yRJN27c0LBhwxQeHm7e9u/fr2PHjsnV1dVsI/4EG3HtJ/ZY3KzJD9JuXDvxZ16+n7+/v2w2m44ePWr3uJ+fn/z9/eXm5pagTlKnECelUKFCOnr0qCZPniw3Nzf16tVLNWvWVHR0tDw9PbV7927Nnz9f+fLl04cffqhy5colOrNxHBcXF3l5eZk3T0/PNPUHAAAAAADgafJUBIE///yz9u/frxYtWiS63MvLS23atNH06dO1cOFCLV26VJcvXzaXx4Vz8e8HBgZKknLlyiVJdqP5wsPDU923ChUqaMiQIdqyZYvKlCmjefPmSboXUB49elT+/v4Jbg8z23B6tOvs7Czp3uzHcXLkyKF69erpiy++0M2bNx+ob4GBgdq8ebPdY5s3b1apUqXM+25ubmrcuLEmTJigsLAwbd26Vfv375ckOTo6qm7duvr000+1b98+RURE6Oeff36gvgAAAAAAAFiNY2Z3IK2ioqJ04cIFxcTE6O+//9aaNWs0atQoNWrUSJ06dUpQfuzYscqXL58qVKggBwcHLV68WHnz5pWPj49ZZvHixapcubKeffZZzZ07V9u3b9eMGTMk3RsJV6hQIYWGhuqjjz7SH3/8oTFjxqTYz1OnTmnatGl65ZVXlD9/fh09elTHjh0z+/jhhx+qUaNGKly4sFq2bCkHBwft3btXBw4c0MiRIx94+6RHu76+vrLZbPrhhx/08ssvy83NTR4eHpo8ebKCg4NVuXJlhYaGqmzZsnJwcNCOHTt05MgRVapUKdl2Bw4cqNatW6tChQqqW7euVq5cqWXLlmn9+vWS7s2+HBMTo2eeeUZZs2bVnDlz5ObmJl9fX/3www86efKkatasqWzZsmn16tWKjY1NcM1CAAAAAAAAJO6JGxG4Zs0a5cuXT0WKFNFLL72kjRs3asKECfr++++VJUuWBOU9PT316aefqnLlyqpSpYoiIiK0evVqu9Fxw4YN04IFC1S2bFl98803mj9/vjlKzcnJSfPnz9eRI0dUtmxZ/fe//01VoJY1a1YdOXJELVq0UEBAgHr06KE333xTr7/+uiSpfv36+uGHH/TTTz+pSpUqqlatmsaNGydfX9+H2j7p0W6BAgXMSUfy5MljzkpcrFgx7dmzR3Xr1tWQIUNUrlw5c2KVd955RyNGjEi23aZNm+rzzz/X6NGjVbp0aU2dOlWzZs1S7dq1JUk+Pj6aPn26goODVbZsWa1fv14rV65Ujhw55OPjo2XLlumFF15QYGCgvvzyS82fP1+lS5d+4G0FAAAAAABgJTYj/sXvgKfY7du3NXLkSC28W0XRT95gWAAAACBZEZ80zOwuAAAec0/ciEAAAAAAAAAAaUcQCAAAAAAAAFgAQSAAAAAAAABgAQSBAAAAAAAAgAUQBAIAAAAAAAAWQBAIAAAAAAAAWABBIAAAAAAAAGABBIEAAAAAAACABRAEAgAAAAAAABZAEAgAAAAAAABYAEEgAAAAAAAAYAEEgQAAAAAAAIAFEAQCAAAAAAAAFkAQCAAAAAAAAFgAQSAAAAAAAABgAQSBAAAAAAAAgAUQBAIAAAAAAAAWQBAIAAAAAAAAWABBIAAAAAAAAGABBIEAAAAAAACABRAEAgAAAAAAABbgmNkdADLa/tD6cnV1zexuAAAAAAAAZChGBAIAAAAAAAAWQBAIAAAAAAAAWABBIAAAAAAAAGABBIEAAAAAAACABRAEAgAAAAAAABZAEAgAAAAAAABYAEEgAAAAAAAAYAEEgQAAAAAAAIAFEAQCAAAAAAAAFkAQCAAAAAAAAFgAQSAAAAAAAABgAQSBAAAAAAAAgAUQBAIAAAAAAAAW4JjZHQAyWlDoWkWz6wMAgHQW8UnDzO4CAABAshgRCAAAAAAAAFgAQSAAAAAAAABgAQSBAAAAAAAAgAUQBAIAAAAAAAAWQBAIAAAAAAAAWABBIAAAAAAAAGABBIEAAAAAAACABRAEAgAAAAAAABZAEAgAAAAAAABYAEEgAAAAAAAAYAEEgQAAAAAAAIAFEAQCAAAAAAAAFkAQCAAAAAAAAFgAQSAAAAAAAABgAQSBAAAAAAAAgAUQBAIAAAAAAAAWQBAIAAAAAAAAWABBIAAAAAAAAGABBIEAAAAAAACABRAEAgAAAAAAABZAEAgAAAAAAABYAEEgAAAAAAAAYAEEgQAAAAAAAIAFEAQ+gC5duqhp06aZsu5Tp06pffv2yp8/v1xdXVWwYEE1adJER44cMcvYbDbz5ujoqMKFC6t///6KiopK0N6tW7eUPXt25cyZM9HlqTFq1ChlyZJFn3322QM/LwAAAAAAADxaBIFPkOjoaNWrV0+RkZFatmyZjh49qoULFyooKEhXr161Kztr1iydP39ep06d0uTJk/Xtt99q5MiRCdpcunSpSpcurZIlS+q77757oH7NnDlT7777rmbOnPlA9QEAAAAAAPDoEQSms19++UVVq1aVi4uL8uXLp8GDB+vu3bvm8uvXryskJETu7u7Kly+fxo0bp9q1a6tfv34ptn3w4EGdOHFCkydPVrVq1eTr66vg4GCNHDlS1apVsyvr4+OjvHnzqlChQmrUqJGaNGmi3bt3J2hzxowZ6tChgzp06KAZM2Y80PO9deuWhg8frmvXrmnLli12y0NDQ1W+fHlNnTpVhQoVUtasWdW6dWtFRkaaZeJGWA4bNky5cuWSl5eX3njjDd25c0eSNG3aNOXPn1+xsbF2bTdp0kRdu3ZNc58BAAAAAACsiCAwHf311196+eWXVaVKFe3du1dTpkzRjBkz7Ebi9e/fX5s3b9aKFSu0bt06bdq0KdGALjG5cuWSg4ODlixZopiYmFT3648//tDPP/+sZ555xu7xEydOaOvWrWrdurVat26tTZs26c8//0x1u9K9ILFdu3ZycnJSu3btEg0Tjx8/rkWLFmnlypVas2aN9uzZo169etmV2bBhgw4fPqywsDDNnz9fy5Yt07BhwyRJrVq10j///KONGzea5S9fvqw1a9YoJCQkyb5FRUXp2rVr5u369etpem4AAAAAAABPE4LAdDR58mQVKlRIX3zxhUqWLGmOchszZoxiY2N1/fp1ff311xo9erTq1KmjMmXKaNasWakO9QoUKKAJEyboww8/VLZs2fTCCy9oxIgROnnyZIKy7dq1k4eHh1xdXVWiRAmVLl1aQ4YMsSszc+ZMNWjQQNmyZVP27NlVv359zZo1K9XP99q1a1qyZIk6dOggSerQoYMWLVqkGzdu2JW7ffu2vvnmG5UvX141a9bUxIkTtWDBAl24cMEs4+zsrJkzZ6p06dJq2LChhg8frgkTJig2NlbZsmVTgwYNNG/ePLP8kiVLlDNnTj3//PNJ9m/UqFHy9vY2bwULFkz1cwMAAAAAAHjaEASmo8OHD6t69eqy2WzmY8HBwbpx44bOnj2rkydPKjo6WlWrVjWXe3t7q0SJEqlex5tvvqkLFy5o7ty5ql69uhYvXqzSpUtr3bp1duXGjRun8PBw7d27Vz/88IP++OMPdezY0VweExOjr7/+2gzxpHtB3uzZsxOcgpuU+fPnq1ixYipXrpwkqXz58vL19dXChQvtyhUuXFgFChQw71evXl2xsbE6evSo+Vi5cuWUNWtWuzI3btzQmTNnJEkhISFaunSpOaHJ3Llz1bZtWzk4JL0LDxkyRJGRkebt7NmzqXpeAAAAAAAATyOCwCeQp6enGjdurI8++kh79+7Vc889l2AikLx588rf318lSpRQw4YNNWzYMC1cuFDHjx+XJK1du1Z//fWX2rRpI0dHRzk6Oqpt27b6888/tWHDhlT1Y8aMGTp48KBZ39HRUYcOHXokk4Y0btxYhmFo1apVOnPmjDZt2pTsacGS5OLiIi8vL/Pm6emZ7v0CAAAAAAB4UhAEpqPAwEBt3bpVhmGYj23evFmenp4qWLCg/Pz85OTkpB07dpjLIyMj9ccffzzwOm02m0qWLKmbN28mWy5LliySpFu3bkm6F+K1bdtW4eHhdre2bdumatKQ/fv3a+fOnQoLC7OrHxYWpq1bt+rIkSNm2dOnT+vcuXPm/W3btsnBwcFuJOTevXvNvsWV8fDwUKFChSRJrq6uat68uebOnav58+erRIkSqlixYiq2EAAAAAAAACTJMbM78KSKjIxUeHi43WM9evTQ+PHj1adPH/Xu3VtHjx7V0KFD1b9/fzk4OMjT01OdO3fWwIEDlT17duXOnVtDhw6Vg4OD3enESQkPD9fQoUPVsWNHlSpVSs7Ozvrll180c+ZMDRo0yK7s1atXdeHCBcXGxurYsWMaPny4AgICFBgYqEuXLmnlypVasWKFypQpY1evU6dOatasmS5fvqzs2bMn2ZcZM2aoatWqqlmzZoJlVapU0YwZM/TZZ59Juhfide7cWaNHj9a1a9f01ltvqXXr1sqbN69Z586dO+rWrZvef/99RUREaOjQoerdu7fdqb8hISFq1KiRDh48aHdKMwAAAAAAAFJGEPiAwsLCVKFCBbvHunXrptWrV2vgwIEqV66csmfPboZbccaOHas33nhDjRo1kpeXl959912dOXNGrq6uKa6zYMGCKlKkiIYNG6aIiAjZbDbz/ttvv21X9tVXX5V0b8Rg3rx5VbNmTX388cdydHTUN998I3d3d9WpUyfBOurUqSM3NzfNmTNHb731VqL9uHPnjubMmZMgfIzTokULjRkzRh9//LEkyd/fX82bN9fLL7+sy5cvq1GjRpo8eXKC9RYvXlw1a9ZUVFSU2rVrp9DQULsyL7zwgrJnz66jR4+qffv2KW4vAAAAAAAA/B+bEf88VmS4mzdvqkCBAhozZoy6deuW2d1Jd6Ghofruu+8SjJ6Mr0uXLrp69aq+++67R9qX27dva+TIkVp4t4qiycABAEA6i/ikYWZ3AQAAIFmkIRlsz549OnLkiKpWrarIyEgNHz5cktSkSZNM7hkAAAAAAACeZkwWkglGjx6tcuXKqW7durp586Y2bdqknDlzatOmTfLw8EjylpHmzp2bZD9Kly6doX0BAAAAAADAw+PU4MfIrVu39NdffyW53N/fP8P6cv36df3999+JLnNycpKvr2+G9SW9cGowAAB4lDg1GAAAPO5IQx4jbm5uGRr2JcfT01Oenp6Z3Q0AAAAAAACkE04NBgAAAAAAACyAIBAAAAAAAACwAIJAAAAAAAAAwAIIAgEAAAAAAAALIAgEAAAAAAAALIAgEAAAAAAAALAAgkAAAAAAAADAAggCAQAAAAAAAAsgCAQAAAAAAAAsgCAQAAAAAAAAsACCQAAAAAAAAMACCAIBAAAAAAAACyAIBAAAAAAAACyAIBAAAAAAAACwAIJAAAAAAAAAwAIIAgEAAAAAAAALIAgEAAAAAAAALMAxszsAZLT9ofXl6uqa2d0AAAAAAADIUIwIBAAAAAAAACyAIBAAAAAAAACwAIJAAAAAAAAAwAIIAgEAAAAAAAALIAgEAAAAAAAALIAgEAAAAAAAALAAgkAAAAAAAADAAggCAQAAAAAAAAsgCAQAAAAAAAAsgCAQAAAAAAAAsACCQAAAAAAAAMACCAIBAAAAAAAACyAIBAAAAAAAACzAMbM7AGS0oNC1imbXBwAAaRTxScPM7gIAAMBDYUQgAAAAAAAAYAEEgQAAAAAAAIAFEAQCAAAAAAAAFkAQCAAAAAAAAFgAQSAAAAAAAABgAQSBAAAAAAAAgAUQBAIAAAAAAAAWQBAIAAAAAAAAWABBIAAAAAAAAGABBIEAAAAAAACABRAEAgAAAAAAABZAEAgAAAAAAABYAEEgAAAAAAAAYAEEgQAAAAAAAIAFEAQCAAAAAAAAFkAQCAAAAAAAAFgAQSAAAAAAAABgAQSBAAAAAAAAgAUQBAIAAAAAAAAWQBAIAAAAAAAAWABBIAAAAAAAAGABBIEAAAAAAACABRAE4olQu3Zt9evXL7O7AQAAAAAA8MQiCMRD6dKli2w2m3nLkSOHXnrpJe3bty+zuwYAAAAAAIB4CALx0F566SWdP39e58+f14YNG+To6KhGjRpldrcAAAAAAAAQD0EgHpqLi4vy5s2rvHnzqnz58ho8eLDOnDmjS5cuSZIGDRqkgIAAZc2aVX5+fvrggw8UHR1t1g8NDVX58uX17bffqkiRIvL29lbbtm11/fr1JNe5atUqeXt7a+7cuY/8+QEAAAAAADwNHDO7A3i63LhxQ3PmzJG/v79y5MghSfL09NTs2bOVP39+7d+/X927d5enp6feffdds96JEyf03Xff6YcfftCVK1fUunVrffLJJ/roo48SrGPevHl64403NG/evGRHHkZFRSkqKsruPgAAAAAAgFUxIhAP7YcffpCHh4c8PDzk6empFStWaOHChXJwuLd7vf/++6pRo4aKFCmixo0b65133tGiRYvs2oiNjdXs2bNVpkwZPffcc+rYsaM2bNiQYF2TJk1Sr169tHLlyhRPPx41apS8vb3NW8GCBdPvSQMAAAAAADxhCALx0J5//nmFh4crPDxc27dvV/369dWgQQP9+eefkqSFCxcqODhYefPmlYeHh95//32dPn3aro0iRYrI09PTvJ8vXz5dvHjRrsySJUv09ttva926dapVq1aK/RoyZIgiIyPN29mzZ9Ph2QIAAAAAADyZCALx0Nzd3eXv7y9/f39VqVJFX331lW7evKnp06dr69atCgkJ0csvv6wffvhBe/bs0X/+8x/duXPHrg0nJye7+zabTbGxsXaPVahQQbly5dLMmTNlGEaK/XJxcZGXl5d5ix80AgAAAAAAWA3XCES6s9lscnBw0K1bt7Rlyxb5+vrqP//5j7k8bqRgWhUrVkxjxoxR7dq1lSVLFn3xxRfp1WUAAAAAAICnHkEgHlpUVJQuXLggSbpy5Yq++OIL3bhxQ40bN9a1a9d0+vRpLViwQFWqVNGqVau0fPnyB15XQECANm7cqNq1a8vR0VHjx49Pp2cBAAAAAADwdCMIxENbs2aN8uXLJ+neDMElS5bU4sWLVbt2bUnS22+/rd69eysqKkoNGzbUBx98oNDQ0AdeX4kSJfTzzz+bIwPHjBmTDs8CAAAAAADg6WYzUnOxNeApcPv2bY0cOVIL71ZRNBk4AABIo4hPGmZ2FwAAAB4Kk4UAAAAAAAAAFkAQCAAAAAAAAFgAQSAAAAAAAABgAQSBAAAAAAAAgAUQBAIAAAAAAAAWQBAIAAAAAAAAWABBIAAAAAAAAGABBIEAAAAAAACABRAEAgAAAAAAABZAEAgAAAAAAABYAEEgAAAAAAAAYAEEgQAAAAAAAIAFEAQCAAAAAAAAFkAQCAAAAAAAAFgAQSAAAAAAAABgAQSBAAAAAAAAgAUQBAIAAAAAAAAWQBAIAAAAAAAAWABBIAAAAAAAAGABBIEAAAAAAACABRAEAgAAAAAAABbgmNkdADLa/tD6cnV1zexuAAAAAAAAZChGBAIAAAAAAAAWQBAIAAAAAAAAWABBIAAAAAAAAGABBIEAAAAAAACABRAEAgAAAAAAABZAEAgAAAAAAABYAEEgAAAAAAAAYAEEgQAAAAAAAIAFEAQCAAAAAAAAFkAQCAAAAAAAAFgAQSAAAAAAAABgAQSBAAAAAAAAgAUQBAIAAAAAAAAW4JjZHQAyWlDoWkWz6wPAEynik4aZ3QUAAADgicWIQAAAAAAAAMACCAIBAAAAAAAACyAIBAAAAAAAACyAIBAAAAAAAACwAIJAAAAAAAAAwAIIAgEAAAAAAAALIAgEAAAAAAAALIAgEAAAAAAAALAAgkAAAAAAAADAAggCAQAAAAAAAAsgCAQAAAAAAAAsgCAQAAAAAAAAsACCQAAAAAAAAMACCAIBAAAAAAAACyAIBAAAAAAAACyAIBAAAAAAAACwAIJAAAAAAAAAwAIIAgEAAAAAAAALIAgEAAAAAAAALIAgEAAAAAAAALAAgkAAAAAAAADAAggCAQAAAAAAAAsgCESms9ls+u677zK7GwAAAAAAAE81gkA8chcuXFCfPn3k5+cnFxcXFSpUSI0bN9aGDRskSefPn1eDBg0kSREREbLZbAoPD8/EHgMAAAAAADx9HDO7A3i6RUREKDg4WD4+Pvrss88UFBSk6OhorV27Vm+++aaOHDmivHnzZnY3AQAAAAAAnnqMCMQj1atXL9lsNm3fvl0tWrRQQECASpcurf79+2vbtm2S7E8NLlq0qCSpQoUKstlsql27tn799Vc5OTnpwoULdm3369dPzz33XIY+HwAAAAAAgCcVQSAemcuXL2vNmjV688035e7unmC5j49Pgse2b98uSVq/fr3Onz+vZcuWqWbNmvLz89O3335rlouOjtbcuXPVtWvXJNcfFRWla9eumbfr168//JMCAAAAAAB4QhEE4pE5fvy4DMNQyZIlU10nV65ckqQcOXIob968yp49uySpW7dumjVrlllu5cqVun37tlq3bp1kW6NGjZK3t7d5K1iw4AM+EwAAAAAAgCcfQSAeGcMw0q2tLl266Pjx4+bpxLNnz1br1q0THWkYZ8iQIYqMjDRvZ8+eTbf+AAAAAAAAPGmYLASPTPHixWWz2XTkyJGHbit37txq3LixZs2apaJFi+rHH39UWFhYsnVcXFzk4uJi3r99+/ZD9wMAAAAAAOBJxYhAPDLZs2dX/fr1NWnSJN28eTPB8qtXryZ4zNnZWZIUExOTYNlrr72mhQsXatq0aSpWrJiCg4PTvc8AAAAAAABPK4JAPFKTJk1STEyMqlatqqVLl+rYsWM6fPiwJkyYoOrVqyconzt3brm5uWnNmjX6+++/FRkZaS6rX7++vLy8NHLkSL366qsZ+TQAAAAAAACeeASBeKT8/Py0e/duPf/88xowYIDKlCmjevXqacOGDZoyZUqC8o6OjpowYYKmTp2q/Pnzq0mTJuYyBwcHdenSRTExMerUqVNGPg0AAAAAAIAnns1IzxkdgEesW7duunTpklasWJHmurdv39bIkSO18G4VRXN5TAB4IkV80jCzuwAAAAA8sUhD8ESIjIzU/v37NW/evAcKAQEAAAAAAKyOIBBPhCZNmmj79u164403VK9evczuDgAAAAAAwBOHIBBPhLCwsMzuAgAAAAAAwBONyUIAAAAAAAAACyAIBAAAAAAAACyAIBAAAAAAAACwAIJAAAAAAAAAwAIIAgEAAAAAAAALIAgEAAAAAAAALIAgEAAAAAAAALAAgkAAAAAAAADAAggCAQAAAAAAAAsgCAQAAAAAAAAsgCAQAAAAAAAAsACCQAAAAAAAAMACCAIBAAAAAAAACyAIBAAAAAAAACyAIBAAAAAAAACwAIJAAAAAAAAAwAIIAgEAAAAAAAALIAgEAAAAAAAALMAxszsAZLT9ofXl6uqa2d0AAAAAAADIUIwIBAAAAAAAACyAIBAAAAAAAACwAIJAAAAAAAAAwAIIAgEAAAAAAAALIAgEAAAAAAAALIAgEAAAAAAAALAAgkAAAAAAAADAAggCAQAAAAAAAAsgCAQAAAAAAAAsgCAQAAAAAAAAsACCQAAAAAAAAMACCAIBAAAAAAAACyAIBAAAAAAAACzAMbM7AGS0oNC1imbXB4DHQsQnDTO7CwAAAIBlMCIQAAAAAAAAsACCQAAAAAAAAMACCAIBAAAAAAAACyAIBAAAAAAAACyAIBAAAAAAAACwAIJAAAAAAAAAwAIIAgEAAAAAAAALIAgEAAAAAAAALIAgEAAAAAAAALAAgkAAAAAAAADAAggCAQAAAAAAAAsgCAQAAAAAAAAsgCAQAAAAAAAAsACCQAAAAAAAAMACCAIBAAAAAAAACyAIBAAAAAAAACyAIBAAAAAAAACwAIJAAAAAAAAAwAIIAgEAAAAAAAALIAgEAAAAAAAALIAgEAAAAAAAALAAgkAAAAAAAADAAggCnzJFihTR+PHjM7sbj1xoaKjKly+f2d0AAAAAAAB4YhAEPoZq166tfv36JXh89uzZ8vHxyfD+JCY0NFQ2m002m01ZsmRRoUKF1KNHD12+fDmzuwYAAAAAAIBEOGZ2B5A+7ty5I2dn5wxdZ+nSpbV+/XrFxMTo8OHD6tq1qyIjI7Vw4cIHbjMzngcAAAAAAIAVMCLwCdWlSxc1bdpUH330kfLnz68SJUqYy65fv6527drJ3d1dBQoU0KRJk+zqjh07VkFBQXJ3d1ehQoXUq1cv3bhxw1weN/Jw7dq1CgwMlIeHh1566SWdP3/erh1HR0flzZtXBQoUUN26ddWqVav/1969R3s95/sDf+5ue5dKjluhERKZlOOeZsxkmKzCMO6SHBxrkMilQuQSueQyBlOHnNJCJobjtlyHcYmZgzpuySHJrTEcu9JdfX9/zM/3/PaUYe9f9q4+j8dan7V8Pp/35/19ffbqtbb13O/P55vHH3+8fH5lKxsPPPDAHHvsseX9Dh065JJLLskxxxyT1q1b58QTT0ySDBkyJJ06dUqLFi2y5ZZb5vzzz8/SpUv/f39sAAAAAIUlCFyDPfnkk5k+fXoef/zxPPjgg+XjV111Vbp165YpU6Zk6NChOe2002oEdI0aNcr111+fN954I+PHj88f/vCHDB48uMbcCxYsyKhRozJhwoQ888wzmTVrVs4666xvrGXmzJl59NFH67Sab9SoUeV6zz///CRJq1atMm7cuLz55pv59a9/nZtvvjnXXnttreZdvHhx5s6dW97mzZtX69oAAAAA1hYeDV6DrbPOOrnllltWCN969OiRoUOHJkk6deqU559/Ptdee2322WefJKmxSq9Dhw4ZMWJEfvWrX+Wmm24qH1+6dGlGjx6drbbaKkkyYMCAXHzxxTU+57XXXkvLli2zbNmyLFq0KMnfVhvW1l577ZUzzzyzxrFhw4bVqPGss87KxIkTVwgs/5GRI0fmoosuKu83a9YsZ599dq3rAwAAAFgbWBG4Btt+++1XugKve/fuK+xPmzatvP/EE0/kZz/7WTbddNO0atUq/fr1y+eff54FCxaUx7Ro0aIcAiZJu3bt8umnn9aYd5tttsnUqVPzn//5nxkyZEh69eqVU089tdb3sfPOO69w7K677kqPHj3Stm3btGzZMsOGDcusWbNqNe8555yTOXPmlLcPP/yw1rUBAAAArC0Egauh1q1bZ86cOSscr66uzrrrrlveX2eddWo998yZM7Pffvula9euueeee/Lyyy+X3yG4ZMmS8rimTZvWuK6ioiKlUqnGsWbNmqVjx47p0qVLLr/88jRu3LjGCrxGjRqtcM3K3vP39/fxwgsvpG/fvundu3cefPDBTJkyJeedd16N+r6LysrKtG7dury1atWqVtcDAAAArE0EgauhbbbZJq+88soKx1955ZV06tTpW69/8cUXV9jv3LlzkuTll1/O8uXLc/XVV2f33XdPp06d8vHHH6+SuocNG5ZRo0aV59twww1rfMHIsmXL8vrrr3/rPJMnT87mm2+e8847LzvvvHO23nrrvP/++6ukRgAAAICiEgSuhk466aS8/fbbGThwYF599dVMnz4911xzTe68884V3qW3Ms8//3yuvPLKvP3227nxxhszadKknHbaaUmSjh07ZunSpfnNb36TGTNmZMKECRk9evQqqbt79+7p2rVrLrvssiR/e/ffQw89lIceeihvvfVWTjrppFRXV3/rPFtvvXVmzZqViRMn5t13383111+fe++9d5XUCAAAAFBUgsDV0JZbbplnnnkmb731Vvbee+/stttu+d3vfpdJkyZl3333/dbrzzzzzLz00kv553/+54wYMSLXXHNNevXqlSTp1q1brrnmmlxxxRXp0qVLbr/99owcOXKV1T5o0KDccsst+eCDD3Lcccelf//+OeaYY/KTn/wkW265ZXr27PmtcxxwwAEZNGhQBgwYkB122CGTJ08uf5swAAAAAHVTUfr7l7jBWmrRokUZMWJE7vpqlyz1hdkAq4WZl/dp6BIAAKAwrAgEAAAAgAIQBAIAAABAAQgCAQAAAKAABIEAAAAAUACCQAAAAAAoAEEgAAAAABSAIBAAAAAACkAQCAAAAAAFIAgEAAAAgAIQBAIAAABAAQgCAQAAAKAABIEAAAAAUACCQAAAAAAoAEEgAAAAABSAIBAAAAAACkAQCAAAAAAFIAgEAAAAgAIQBAIAAABAAQgCAQAAAKAABIEAAAAAUACCQAAAAAAogCYNXQDUt9cu7JWqqqqGLgMAAACgXlkRCAAAAAAFIAgEAAAAgAIQBAIAAABAAQgCAQAAAKAABIEAAAAAUACCQAAAAAAoAEEgAAAAABSAIBAAAAAACkAQCAAAAAAFIAgEAAAAgAIQBAIAAABAAQgCAQAAAKAABIEAAAAAUABNGroAqG/bX/holvqnXwgzL+/T0CUAAADAasOKQAAAAAAoAEEgAAAAABSAIBAAAAAACkAQCAAAAAAFIAgEAAAAgAIQBAIAAABAAQgCAQAAAKAABIEAAAAAUACCQAAAAAAoAEEgAAAAABSAIBAAAAAACkAQCAAAAAAFIAgEAAAAgAIQBAIAAABAAQgCAQAAAKAABIEAAAAAUACCQAAAAAAoAEEgAAAAABSAIBAAAAAACkAQCAAAAAAFIAgEAAAAgAIQBAIAAABAAQgCAQAAAKAABIFroL/+9a856aST8oMf/CCVlZVp27ZtevXqleeffz5J0qFDh1RUVKSioiKNGzfOJptskuOPPz5ffPHFSufbdtttU1lZmdmzZ9epnjvvvDONGzfOKaecUud7AgAAAOD7JQhcAx188MGZMmVKxo8fn7fffjv3339/fvrTn+bzzz8vj7n44ovzySefZNasWbn99tvzzDPPZODAgSvM9dxzz2XhwoU55JBDMn78+DrVM3bs2AwePDh33nlnFi1aVOf7AgAAAOD7Iwhcw1RXV+fZZ5/NFVdckZ49e2bzzTfPrrvumnPOOScHHHBAeVyrVq3Stm3bbLrppunZs2f69++fV155ZYX5xo4dm6OOOir9+vXLrbfeWut63nvvvUyePDlDhw5Np06d8vvf/77G+XHjxqVNmza57777svXWW6eqqiq9evXKBx98UB5z4YUXZocddsiYMWPSvn37tGjRIocddljmzJmTJHnsscdSVVWV6urqGnOfdtpp2WuvvWpdMwAAAEARCQLXMC1btkzLli1z3333ZfHixd/pmo8++igPPPBAdttttxrH582bl0mTJuXoo4/OPvvskzlz5uTZZ5+tVT3//u//nj59+mTdddfN0UcfnbFjx64wZsGCBbn00ktz22235fnnn091dXWOOOKIGmPeeeed/O53v8sDDzyQRx55JFOmTMnJJ5+cJPnZz36WNm3a5J577imPX7ZsWe6666707dv3G2tbvHhx5s6dW97mzZtXq3sDAAAAWJsIAtcwTZo0ybhx4zJ+/Pi0adMmPXr0yLnnnptXX321xrghQ4akZcuWad68eTbbbLNUVFTkmmuuqTFm4sSJ2XrrrfPDH/4wjRs3zhFHHLHSIO+bLF++POPGjcvRRx+dJDniiCPy3HPP5b333qsxbunSpbnhhhvSvXv37LTTThk/fnwmT56cP//5z+UxixYtym233ZYddtghe+65Z37zm99k4sSJmT17drm2O+64ozz+ySefTHV1dQ4++OBvrG/kyJFZd911y9tmm232ne8NAAAAYG0jCFwDHXzwwfn4449z//33Z999983TTz+dHXfcMePGjSuPOfvsszN16tS8+uqrefLJJ5Mkffr0ybJly8pjbr311nKIlyRHH310Jk2a9J1Xzj3++OOZP39+evfunSTZYIMNss8++6zwiHGTJk2yyy67lPe33XbbtGnTJtOmTSsf+8EPfpBNN920vN+9e/csX74806dPT5L07ds3Tz/9dD7++OMkye23354+ffqkTZs231jfOeeckzlz5pS3Dz/88DvdFwAAAMDaSBC4hqqqqso+++yT888/P5MnT86xxx6b4cOHl89vsMEG6dixY7beeuvstddeue666zJ58uQ89dRTSZI333wzL774YgYPHpwmTZqkSZMm2X333bNgwYJMnDjxO9UwduzY/M///E+aN29enuPhhx/O+PHjs3z58lV6v7vssku22mqrTJw4MQsXLsy99977Dx8LTpLKysq0bt26vLVq1WqV1gQAAACwJhEEriW22267zJ8//xvPN27cOEmycOHCJH8L8fbcc8/813/9V6ZOnVrezjjjjO/0ePDnn3+e//iP/8jEiRNrXD9lypR88cUXeeyxx8pjv/rqq7z00kvl/enTp6e6ujqdO3cuH5s1a1Z5tV+SvPjii2nUqFG22Wab8rG+ffvm9ttvzwMPPJBGjRqlT58+3+EnAwAAAECSNGnoAqidzz//PIceemiOO+64dO3aNa1atcpLL72UK6+8Mr/4xS/K4+bNm5fZs2enVCrlgw8+yODBg7Phhhtmjz32yNKlSzNhwoRcfPHF6dKlS435TzjhhFxzzTV544038sMf/vAb65gwYULWX3/9HHbYYamoqKhxrnfv3hk7dmz23XffJEnTpk1z6qmn5vrrr0+TJk0yYMCA7L777tl1113L11RVVaV///4ZNWpU5s6dm4EDB+awww5L27Zty2P69u2bCy+8MJdeemkOOeSQVFZW/n/9LAEAAACKxIrANUzLli2z22675dprr82ee+6ZLl265Pzzz8+//uu/5oYbbiiPu+CCC9KuXbtssskm2W+//bLOOuvksccey/rrr5/7778/n3/+eQ466KAV5u/cuXM6d+78rasCb7311hx00EErhIDJ395heP/99+ezzz5LkrRo0SJDhgzJUUcdlR49eqRly5a56667alzTsWPH/PKXv0zv3r3z85//PF27ds1NN920wphdd901r7766rc+FgwAAABATRWlUqnU0EWw9ho3blxOP/30VFdXf+OYCy+8MPfdd1+mTp36vdayaNGijBgxInd9tUuWWgxbCDMv9/g4AAAAfM2KQAAAAAAoAEEgK/Xss8+mZcuW37gBAAAAsGbxaDArtXDhwnz00UffeL5jx471WM2q4dHg4vFoMAAAAPwvaQgr1bx58zUy7AMAAABg5TwaDAAAAAAFIAgEAAAAgAIQBAIAAABAAQgCAQAAAKAABIEAAAAAUACCQAAAAAAoAEEgAAAAABSAIBAAAAAACkAQCAAAAAAFIAgEAAAAgAIQBAIAAABAAQgCAQAAAKAABIEAAAAAUACCQAAAAAAoAEEgAAAAABSAIBAAAAAACkAQCAAAAAAF0KShC4D69tqFvVJVVdXQZQAAAADUKysCAQAAAKAABIEAAAAAUACCQAAAAAAoAEEgAAAAABSAIBAAAAAACkAQCAAAAAAFIAgEAAAAgAIQBAIAAABAAQgCAQAAAKAABIEAAAAAUACCQAAAAAAoAEEgAAAAABSAIBAAAAAACqBJQxcA9W37Cx/NUv/012gzL+/T0CUAAADAGseKQAAAAAAoAEEgAAAAABSAIBAAAAAACkAQCAAAAAAFIAgEAAAAgAIQBAIAAABAAQgCAQAAAKAABIEAAAAAUACCQAAAAAAoAEEgAAAAABSAIBAAAAAACkAQCAAAAAAFIAgEAAAAgAIQBAIAAABAAQgCAQAAAKAABIEAAAAAUACCQAAAAAAoAEEgAAAAABSAIBAAAAAACkAQCAAAAAAFIAgEAAAAgAIQBAIAAABAAQgCqbWnn346FRUVqa6ubuhSAAAAAPiOBIFroGOPPTYVFRWpqKhI06ZNs8UWW2Tw4MFZtGjRKvuMDh065LrrrlvpuT322COffPJJ1l133VX2eV976qmn0rt376y//vpp0aJFtttuu5x55pn56KOPVvlnAQAAABSJIHANte++++aTTz7JjBkzcu2112bMmDEZPnx4vXx2s2bN0rZt21RUVKzSeceMGZO99947bdu2zT333JM333wzo0ePzpw5c3L11Vev9Jply5Zl+fLlq7QOAAAAgLWRIHANVVlZmbZt26Z9+/Y58MADs/fee+fxxx9PkixfvjwjR47MFltskebNm6dbt265++67y+c222yz/Pa3v60x35QpU9KoUaO8//773/rZf/9o8Lhx49KmTZs8+uij6dy5c1q2bFkOKv9ft9xySzp37pyqqqpsu+22uemmm8rnPvzwwwwcODADBw7Mrbfemp/+9Kfp0KFD9txzz9xyyy254IILanzW/fffn+222y6VlZWZNWtWnX+OAAAAAEUhCFwLvP7665k8eXKaNWuWJBk5cmRuu+22jB49Om+88UYGDRqUo48+On/84x/TqFGjHHnkkbnjjjtqzHH77benR48e2XzzzetUw4IFCzJq1KhMmDAhzzzzTGbNmpWzzjqrxvwXXHBBLr300kybNi2XXXZZzj///IwfPz5JMmnSpCxZsiSDBw9e6fxt2rSp8VlXXHFFbrnllrzxxhvZaKONVnrN4sWLM3fu3PI2b968Ot0bAAAAwNqgSUMXQN08+OCDadmyZb766qssXrw4jRo1yg033JDFixfnsssuyxNPPJHu3bsnSbbccss899xzGTNmTH7yk5+kb9++ufrqqzNr1qz84Ac/yPLlyzNx4sQMGzaszvUsXbo0o0ePzlZbbZUkGTBgQC6++OLy+eHDh+fqq6/OL3/5yyTJFltskTfffDNjxoxJ//7989///d9p3bp12rVr950+66abbkq3bt3+4biRI0fmoosuKu83a9YsZ599dl1uDwAAAGCNZ0XgGqpnz56ZOnVq/vSnP6V///75l3/5lxx88MF55513smDBguyzzz5p2bJlebvtttvy7rvvJkl22GGHdO7cubwq8I9//GM+/fTTHHrooXWup0WLFuUQMEnatWuXTz/9NEkyf/78vPvuuzn++ONr1DRixIhyTaVS6Tu/c7BZs2bp2rXrt44755xzMmfOnPL24Ycf1uHOAAAAANYOVgSuodZZZ5107NgxSXLrrbemW7duGTt2bLp06ZIkeeihh7LpppvWuKaysrL833379s0dd9yRoUOH5o477si+++6b9ddfv871NG3atMZ+RUVFSqVSkuTLL79Mktx8883Zbbfdaoxr3LhxkqRTp06ZM2dOPvnkk29dFdi8efPvFBpWVlbWuOdV+a3KAAAAAGsaKwLXAo0aNcq5556bYcOG1fgCjY4dO9bY2rdvX77mqKOOyuuvv56XX345d999d/r27fu91bfxxhtnk002yYwZM1aoaYsttkiSHHLIIWnWrFmuvPLKlc7x9ReTAAAAAFA3VgSuJQ499NCcffbZGTNmTM4666wMGjQoy5cvz49+9KPMmTMnzz//fFq3bp3+/fsnSTp06JA99tgjxx9/fJYtW5YDDjhghTk/+uijTJ06tcaxun6ZyEUXXZSBAwdm3XXXzb777pvFixfnpZdeyhdffJEzzjgj7du3z7XXXpsBAwZk7ty5OeaYY9KhQ4d8+OGHue2229KyZctcffXVdfpsAAAAAASBa40mTZpkwIABufLKK/Pee+9lww03zMiRIzNjxoy0adMmO+64Y84999wa1/Tt2zcnn3xyjjnmmDRv3nyFOUeNGpVRo0bVODZhwoRsttlmta7vhBNOSIsWLXLVVVfl7LPPzjrrrJPtt98+p59+ennMySefnE6dOmXUqFE56KCDsnDhwnTo0CH77bdfzjjjjFp/JgAAAAD/q6L09YvcYC23aNGijBgxInd9tUuWysDXaDMv79PQJQAAAMAaxzsCAQAAAKAABIEAAAAAUACCQAAAAAAoAEEgAAAAABSAIBAAAAAACkAQCAAAAAAFIAgEAAAAgAIQBAIAAABAAQgCAQAAAKAABIEAAAAAUACCQAAAAAAoAEEgAAAAABSAIBAAAAAACkAQCAAAAAAFIAgEAAAAgAIQBAIAAABAAQgCAQAAAKAABIEAAAAAUACCQAAAAAAoAEEgAAAAABSAIBAAAAAACqBJQxcA9e21C3ulqqqqocsAAAAAqFdWBAIAAABAAQgCAQAAAKAABIEAAAAAUACCQAAAAAAoAEEgAAAAABSAIBAAAAAACkAQCAAAAAAFIAgEAAAAgAIQBAIAAABAAQgCAQAAAKAABIEAAAAAUACCQAAAAAAoAEEgAAAAABSAIBAAAAAACkAQCAAAAAAFIAgEAAAAgAIQBAIAAABAAQgCAQAAAKAABIEAAAAAUACCQAAAAAAoAEEgAAAAABSAIBAAAAAACkAQCAAAAAAFIAgEAAAAgAIQBAIAAABAAQgCAQAAAKAABIEAAAAAUACCQAAAAAAoAEEgAAAAABSAIBAAAAAACkAQCAAAAAAFIAgEAAAAgAIQBAIAAABAAQgCAQAAAKAABIEAAAAAUACCQAAAAAAoAEEgAAAAABSAIBAAAAAACkAQCAAAAAAFIAgEAAAAgAJo0tAFQH0plUpJksWLFzdwJQAAAACrXmVlZSoqKr7xvCCQwvjyyy+TJFdddVUDVwIAAACw6g0bNixVVVXfeL6i9PUyKVjLVVdXp3379nn33XfTunXrhi4H1gjz5s3LZpttlg8//DCtWrVq6HJgjaF3oG70DtSevoG6WVt7x4pA+L8aNWqUL7/8MlVVVf8wHQf+15IlS7JkyZJUVlbqG6gFvQN1o3eg9vQN1E1Re8eXhQAAAABAAQgCAQAAAKAABIEURmVlZYYPH57KysqGLgXWGPoG6kbvQN3oHag9fQN1U9Te8WUhAAAAAFAAVgQCAAAAQAEIAgEAAACgAASBAAAAAFAAgkAAAAAAKABBIGuVG2+8MR06dEhVVVV22223/PnPf/6H4ydNmpRtt902VVVV2X777fPwww/XU6Ww+qhN39x888358Y9/nPXWWy/rrbde9t5772/tM1hb1fZ3ztcmTpyYioqKHHjggd9vgbAaqm3fVFdX55RTTkm7du1SWVmZTp06+f81Cqm2vXPddddlm222SfPmzdO+ffsMGjQoixYtqqdqoeE988wz2X///bPJJpukoqIi991337de8/TTT2fHHXdMZWVlOnbsmHHjxn3vdTYEQSBrjbvuuitnnHFGhg8fnldeeSXdunVLr1698umnn650/OTJk3PkkUfm+OOPz5QpU3LggQfmwAMPzOuvv17PlUPDqW3fPP300znyyCPz1FNP5YUXXkj79u3z85//PB999FE9Vw4Nq7a987WZM2fmrLPOyo9//ON6qhRWH7XtmyVLlmSfffbJzJkzc/fdd2f69Om5+eabs+mmm9Zz5dCwats7d9xxR4YOHZrhw4dn2rRpGTt2bO66666ce+659Vw5NJz58+enW7duufHGG7/T+Pfeey99+vRJz549M3Xq1Jx++uk54YQT8uijj37Plda/ilKpVGroImBV2G233bLLLrvkhhtuSJIsX7487du3z6mnnpqhQ4euMP7www/P/Pnz8+CDD5aP7b777tlhhx0yevToeqsbGlJt++bvLVu2LOutt15uuOGGHHPMMd93ubDaqEvvLFu2LHvuuWeOO+64PPvss6murv5Of52GtUVt+2b06NG56qqr8tZbb6Vp06b1XS6sNmrbOwMGDMi0adPy5JNPlo+deeaZ+dOf/pTnnnuu3uqG1UVFRUXuvffef/g0xpAhQ/LQQw/VWBh0xBFHpLq6Oo888kg9VFl/rAhkrbBkyZK8/PLL2XvvvcvHGjVqlL333jsvvPDCSq954YUXaoxPkl69en3jeFjb1KVv/t6CBQuydOnS/NM//dP3VSasduraOxdffHE22mijHH/88fVRJqxW6tI3999/f7p3755TTjklG2+8cbp06ZLLLrssy5Ytq6+yocHVpXf22GOPvPzyy+XHh2fMmJGHH344vXv3rpeaYU1UpHygSUMXAKvCZ599lmXLlmXjjTeucXzjjTfOW2+9tdJrZs+evdLxs2fP/t7qhNVJXfrm7w0ZMiSbbLLJCr80YW1Wl9557rnnMnbs2EydOrUeKoTVT136ZsaMGfnDH/6Qvn375uGHH84777yTk08+OUuXLs3w4cPro2xocHXpnaOOOiqfffZZfvSjH6VUKuWrr77Kr371K48Gwz/wTfnA3Llzs3DhwjRv3ryBKlv1rAgEoE4uv/zyTJw4Mffee2+qqqoauhxYbc2bNy/9+vXLzTffnA022KChy4E1xvLly7PRRhvl3/7t37LTTjvl8MMPz3nnnecVLvAtnn766Vx22WW56aab8sorr+T3v/99HnrooVxyySUNXRqwGrAikLXCBhtskMaNG+cvf/lLjeN/+ctf0rZt25Ve07Zt21qNh7VNXfrma6NGjcrll1+eJ554Il27dv0+y4TVTm175913383MmTOz//77l48tX748SdKkSZNMnz49W2211fdbNDSwuvzOadeuXZo2bZrGjRuXj3Xu3DmzZ8/OkiVL0qxZs++1Zlgd1KV3zj///PTr1y8nnHBCkmT77bfP/Pnzc+KJJ+a8885Lo0bWA8Hf+6Z8oHXr1mvVasDEikDWEs2aNctOO+1U44W4y5cvz5NPPpnu3buv9Jru3bvXGJ8kjz/++DeOh7VNXfomSa688spccskleeSRR7LzzjvXR6mwWqlt72y77bZ57bXXMnXq1PJ2wAEHlL+Vrn379vVZPjSIuvzO6dGjR955551ycJ4kb7/9dtq1aycEpDDq0jsLFixYIez7OlD3XaGwcoXKB0qwlpg4cWKpsrKyNG7cuNKbb75ZOvHEE0tt2rQpzZ49u1QqlUr9+vUrDR06tDz++eefLzVp0qQ0atSo0rRp00rDhw8vNW3atPTaa6811C1Avatt31x++eWlZs2ale6+++7SJ598Ut7mzZvXULcADaK2vfP3+vfvX/rFL35RT9XC6qG2fTNr1qxSq1atSgMGDChNnz699OCDD5Y22mij0ogRIxrqFqBB1LZ3hg8fXmrVqlXpzjvvLM2YMaP02GOPlbbaaqvSYYcd1lC3APVu3rx5pSlTppSmTJlSSlK65pprSlOmTCm9//77pVKpVBo6dGipX79+5fEzZswotWjRonT22WeXpk2bVrrxxhtLjRs3Lj3yyCMNdQvfG48Gs9Y4/PDD89e//jUXXHBBZs+enR122CGPPPJI+YWfs2bNqvGXsT322CN33HFHhg0blnPPPTdbb7117rvvvnTp0qWhbgHqXW375re//W2WLFmSQw45pMY8w4cPz4UXXlifpUODqm3vALXvm/bt2+fRRx/NoEGD0rVr12y66aY57bTTMmTIkIa6BWgQte2dYcOGpaKiIsOGDctHH32UDTfcMPvvv38uvfTShroFqHcvvfRSevbsWd4/44wzkiT9+/fPuHHj8sknn2TWrFnl81tssUUeeuihDBo0KL/+9a+z2Wab5ZZbbkmvXr3qvfbvW0WpZG0wAAAAAKzt/KkaAAAAAApAEAgAAAAABSAIBAAAAIACEAQCAAAAQAEIAgEAAACgAASBAAAAAFAAgkAAAAAAKABBIAAAAAAUgCAQAAAAAApAEAgAAAAABSAIBAAAAIACEAQCAAAAQAH8H23Cye2STcDOAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1400x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<h2o.plot._plot_result._MObject at 0x2105ad15030>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "glm.varimp_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` From the variable importance plot, we can see that the most significant feature is Log_DisbursementGross. We can also see Gr_Appv, (Log_Gr_Appv) DisbursementGross, and Log_SBA_Appv are the next most important variables. As this is understood by the fact, that if your ```\n",
    "\n",
    "``` DisbursementGross provides information about the size of the loans granted to small businesses. This information is crucial for understanding the financial impact of SBA loans on the businesses they support. The loan amount disbursed is often indicative of the level of risk associated with a borrower. Larger loan amounts may indicate higher financial stability, while smaller loans may be associated with smaller or riskier businesses.```\n",
    "\n",
    "``` SBA_Appv (Log_SBA_Appv) The variable confirms that the SBA has approved a loan for a specific borrower. It signifies that the borrower has successfully gone through the SBA's application and approval process Lenders and borrowers use this variable to determine if they are eligible for SBA loans and to understand the maximum loan amount that can be approved for their business.he variable is important for assessing the economic impact of SBA loans. By analyzing the approved loan amounts, one can estimate the potential economic impact in terms of job creation, business expansion, and overall economic growth.```\n",
    "\n",
    "``` he \"Bank\" variable helps identify the specific financial institutions that are participating in the SBA loan program. This information is crucial for understanding which banks are actively providing SBA loans to small businesses.```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Top 4 Variables with their relative importance is as follows:\n",
    "\n",
    "Log_DisbursementGross: 1.0742711\n",
    "\n",
    "Log_Gr_Appv: 0.7507896\n",
    "\n",
    "Log_SBA_Appv: 0.5599474\n",
    "\n",
    "Bank: 0.5510707"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glm prediction progress: || (done) 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class='dataframe'>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  predict</th><th style=\"text-align: right;\">      p0</th><th style=\"text-align: right;\">       p1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.970777</td><td style=\"text-align: right;\">0.029223 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.471275</td><td style=\"text-align: right;\">0.528725 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.951736</td><td style=\"text-align: right;\">0.0482639</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.951392</td><td style=\"text-align: right;\">0.0486081</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.945459</td><td style=\"text-align: right;\">0.0545409</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.92252 </td><td style=\"text-align: right;\">0.0774796</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.855034</td><td style=\"text-align: right;\">0.144966 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.865822</td><td style=\"text-align: right;\">0.134178 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.919617</td><td style=\"text-align: right;\">0.0803829</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.759493</td><td style=\"text-align: right;\">0.240507 </td></tr>\n",
       "</tbody>\n",
       "</table><pre style='font-size: smaller; margin-bottom: 1em;'>[10 rows x 3 columns]</pre>"
      ],
      "text/plain": [
       "  predict        p0         p1\n",
       "---------  --------  ---------\n",
       "        0  0.970777  0.029223\n",
       "        1  0.471275  0.528725\n",
       "        0  0.951736  0.0482639\n",
       "        0  0.951392  0.0486081\n",
       "        0  0.945459  0.0545409\n",
       "        0  0.92252   0.0774796\n",
       "        0  0.855034  0.144966\n",
       "        0  0.865822  0.134178\n",
       "        0  0.919617  0.0803829\n",
       "        1  0.759493  0.240507\n",
       "[10 rows x 3 columns]\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glm.predict(valid).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` These columns contain the predicted probabilities for each class. \"p0\" represents the probability of an observation belonging to class 0 (Not Defaulted), and \"p1\" represents the probability of it belonging to class 1 (Defaulted on Loan). These probabilities can be used to assess the model's confidence in its predictions.```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H2O Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```We Tune our model with lambda_search = True, as this will automatically tune the model. Other parameters that we can alter are max_active_predictors (feature selection parameter), nlambdas, which allows you to specify the number of lambda values, or the regularization strengths, to be used in the elastic net regularization path, and solver, which specify the algorithm or optimization method that the GLM model should use to find the solution``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```A value of alpha = 1 represents Lasso Regularization and a value of alpha = 0 produces Ridge regression```\n",
    "\n",
    "``` lambda is employed for regularization strength ```\n",
    "``` missing_value_handling parameter allows to specify how we want to handle any missing data (options are skip and MeanImputation)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glm Grid Build progress: || (done) 100%\n",
      "CPU times: total: 984 ms\n",
      "Wall time: 5min 3s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "\n",
       "#h2o-table-11.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-11 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-11 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-11 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-11 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-11 .h2o-table th,\n",
       "#h2o-table-11 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-11 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-11\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption>Hyper-Parameter Search Summary: ordered by increasing logloss</caption>\n",
       "    <thead><tr><th></th>\n",
       "<th>alpha</th>\n",
       "<th>missing_values_handling</th>\n",
       "<th>model_ids</th>\n",
       "<th>logloss</th></tr></thead>\n",
       "    <tbody><tr><td></td>\n",
       "<td>0.0</td>\n",
       "<td>Skip</td>\n",
       "<td>glm_random_grid_model_11</td>\n",
       "<td>0.3916081</td></tr>\n",
       "<tr><td></td>\n",
       "<td>0.0</td>\n",
       "<td>MeanImputation</td>\n",
       "<td>glm_random_grid_model_34</td>\n",
       "<td>0.3916081</td></tr>\n",
       "<tr><td></td>\n",
       "<td>0.44</td>\n",
       "<td>MeanImputation</td>\n",
       "<td>glm_random_grid_model_28</td>\n",
       "<td>0.3917570</td></tr>\n",
       "<tr><td></td>\n",
       "<td>0.48</td>\n",
       "<td>Skip</td>\n",
       "<td>glm_random_grid_model_2</td>\n",
       "<td>0.3917586</td></tr>\n",
       "<tr><td></td>\n",
       "<td>0.48</td>\n",
       "<td>MeanImputation</td>\n",
       "<td>glm_random_grid_model_36</td>\n",
       "<td>0.3917586</td></tr>\n",
       "<tr><td></td>\n",
       "<td>0.43</td>\n",
       "<td>MeanImputation</td>\n",
       "<td>glm_random_grid_model_10</td>\n",
       "<td>0.3917621</td></tr>\n",
       "<tr><td></td>\n",
       "<td>0.43</td>\n",
       "<td>Skip</td>\n",
       "<td>glm_random_grid_model_22</td>\n",
       "<td>0.3917621</td></tr>\n",
       "<tr><td></td>\n",
       "<td>0.4700000</td>\n",
       "<td>MeanImputation</td>\n",
       "<td>glm_random_grid_model_26</td>\n",
       "<td>0.3917632</td></tr>\n",
       "<tr><td></td>\n",
       "<td>0.4700000</td>\n",
       "<td>Skip</td>\n",
       "<td>glm_random_grid_model_43</td>\n",
       "<td>0.3917632</td></tr>\n",
       "<tr><td></td>\n",
       "<td>0.42</td>\n",
       "<td>MeanImputation</td>\n",
       "<td>glm_random_grid_model_56</td>\n",
       "<td>0.3917673</td></tr>\n",
       "<tr><td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td></tr>\n",
       "<tr><td></td>\n",
       "<td>0.1</td>\n",
       "<td>Skip</td>\n",
       "<td>glm_random_grid_model_62</td>\n",
       "<td>0.3918407</td></tr>\n",
       "<tr><td></td>\n",
       "<td>0.08</td>\n",
       "<td>Skip</td>\n",
       "<td>glm_random_grid_model_23</td>\n",
       "<td>0.3918501</td></tr>\n",
       "<tr><td></td>\n",
       "<td>0.06</td>\n",
       "<td>Skip</td>\n",
       "<td>glm_random_grid_model_45</td>\n",
       "<td>0.3918789</td></tr>\n",
       "<tr><td></td>\n",
       "<td>0.06</td>\n",
       "<td>MeanImputation</td>\n",
       "<td>glm_random_grid_model_9</td>\n",
       "<td>0.3918789</td></tr>\n",
       "<tr><td></td>\n",
       "<td>0.05</td>\n",
       "<td>MeanImputation</td>\n",
       "<td>glm_random_grid_model_4</td>\n",
       "<td>0.3919370</td></tr>\n",
       "<tr><td></td>\n",
       "<td>0.04</td>\n",
       "<td>Skip</td>\n",
       "<td>glm_random_grid_model_52</td>\n",
       "<td>0.3920156</td></tr>\n",
       "<tr><td></td>\n",
       "<td>0.03</td>\n",
       "<td>Skip</td>\n",
       "<td>glm_random_grid_model_30</td>\n",
       "<td>0.3921355</td></tr>\n",
       "<tr><td></td>\n",
       "<td>0.02</td>\n",
       "<td>Skip</td>\n",
       "<td>glm_random_grid_model_27</td>\n",
       "<td>0.3923318</td></tr>\n",
       "<tr><td></td>\n",
       "<td>0.02</td>\n",
       "<td>MeanImputation</td>\n",
       "<td>glm_random_grid_model_47</td>\n",
       "<td>0.3923318</td></tr>\n",
       "<tr><td></td>\n",
       "<td>0.24</td>\n",
       "<td>Skip</td>\n",
       "<td>glm_random_grid_model_63</td>\n",
       "<td>0.4056959</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n",
       "<pre style='font-size: smaller; margin-bottom: 1em;'>[63 rows x 5 columns]</pre>"
      ],
      "text/plain": [
       "Hyper-Parameter Search Summary: ordered by increasing logloss\n",
       "     alpha                missing_values_handling    model_ids                 logloss\n",
       "---  -------------------  -------------------------  ------------------------  -------------------\n",
       "     0.0                  Skip                       glm_random_grid_model_11  0.39160808622118365\n",
       "     0.0                  MeanImputation             glm_random_grid_model_34  0.39160808622118365\n",
       "     0.44                 MeanImputation             glm_random_grid_model_28  0.3917569991015515\n",
       "     0.48                 Skip                       glm_random_grid_model_2   0.3917586027842902\n",
       "     0.48                 MeanImputation             glm_random_grid_model_36  0.3917586027842902\n",
       "     0.43                 MeanImputation             glm_random_grid_model_10  0.39176212599720756\n",
       "     0.43                 Skip                       glm_random_grid_model_22  0.39176212599720756\n",
       "     0.47000000000000003  MeanImputation             glm_random_grid_model_26  0.3917631576492127\n",
       "     0.47000000000000003  Skip                       glm_random_grid_model_43  0.3917631576492127\n",
       "     0.42                 MeanImputation             glm_random_grid_model_56  0.3917672670241429\n",
       "---  ---                  ---                        ---                       ---\n",
       "     0.1                  Skip                       glm_random_grid_model_62  0.39184069991759385\n",
       "     0.08                 Skip                       glm_random_grid_model_23  0.39185013254495743\n",
       "     0.06                 Skip                       glm_random_grid_model_45  0.3918788835170476\n",
       "     0.06                 MeanImputation             glm_random_grid_model_9   0.3918788835170476\n",
       "     0.05                 MeanImputation             glm_random_grid_model_4   0.3919370150415301\n",
       "     0.04                 Skip                       glm_random_grid_model_52  0.39201558013903665\n",
       "     0.03                 Skip                       glm_random_grid_model_30  0.39213545399292443\n",
       "     0.02                 Skip                       glm_random_grid_model_27  0.3923318174271338\n",
       "     0.02                 MeanImputation             glm_random_grid_model_47  0.3923318174271338\n",
       "     0.24                 Skip                       glm_random_grid_model_63  0.40569593168558904\n",
       "[63 rows x 5 columns]\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glm_grid = h2o.grid.H2OGridSearch (\n",
    "    \n",
    "    H2OGeneralizedLinearEstimator(family = \"binomial\",\n",
    "                                  lambda_search = True),\n",
    "    \n",
    "    hyper_params = {\"alpha\": [x*0.01 for x in range(0, 50)],\n",
    "                    \"missing_values_handling\" : [\"Skip\", \"MeanImputation\"]},\n",
    "    \n",
    "    grid_id = \"glm_random_grid\",\n",
    "    \n",
    "    search_criteria = {\n",
    "        \"strategy\":\"RandomDiscrete\",\n",
    "        \"max_models\":300,\n",
    "        \"max_runtime_secs\":300,\n",
    "        \"seed\":42})\n",
    "\n",
    "%time glm_grid.train(x = train_X, y = train_y, training_frame = train, validation_frame = valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'glm_random_grid_model_11'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_glm_grid = glm_grid.get_grid(sort_by = 'aucpr', decreasing = True)\n",
    "sorted_glm_grid.sorted_metric_table()\n",
    "best_model_id = sorted_glm_grid.sorted_metric_table()['model_ids'][0]\n",
    "best_model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` he grid search results sorted by the AUC-PR metric in decreasing order. AUC-PR (Area Under the Precision-Recall Curve) is a metric commonly used to evaluate the performance of binary classification models, especially when dealing with imbalanced datasets. Sorting in decreasing order means that the models with the highest AUC-PR values will appear at the top of the sorted list. Higher AUC-PR values indicate better precision-recall trade-offs in the models.```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sorted_metric_table() function provides an easy way to examine and analyze the results, allowing you to identify the best-performing models based on the chosen metric (AUC-PR in this case).\n",
    "\n",
    "As in this case we can see the best model is ``` glm_random_grid_model_11``` with AUCPR values as ``` 0.439303```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "\n",
       "#h2o-table-12.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-12 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-12 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-12 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-12 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-12 .h2o-table th,\n",
       "#h2o-table-12 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-12 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-12\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption>GLM Model: summary</caption>\n",
       "    <thead><tr><th></th>\n",
       "<th>family</th>\n",
       "<th>link</th>\n",
       "<th>regularization</th>\n",
       "<th>lambda_search</th>\n",
       "<th>number_of_predictors_total</th>\n",
       "<th>number_of_active_predictors</th>\n",
       "<th>number_of_iterations</th>\n",
       "<th>training_frame</th></tr></thead>\n",
       "    <tbody><tr><td></td>\n",
       "<td>binomial</td>\n",
       "<td>logit</td>\n",
       "<td>Ridge ( lambda = 1.252E-5 )</td>\n",
       "<td>nlambda = 30, lambda.max = 12.525, lambda.min = 1.252E-5, lambda.1se = -1.0</td>\n",
       "<td>27</td>\n",
       "<td>27</td>\n",
       "<td>54</td>\n",
       "<td>py_3_sid_9cdc</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n"
      ],
      "text/plain": [
       "GLM Model: summary\n",
       "    family    link    regularization               lambda_search                                                                number_of_predictors_total    number_of_active_predictors    number_of_iterations    training_frame\n",
       "--  --------  ------  ---------------------------  ---------------------------------------------------------------------------  ----------------------------  -----------------------------  ----------------------  ----------------\n",
       "    binomial  logit   Ridge ( lambda = 1.252E-5 )  nlambda = 30, lambda.max = 12.525, lambda.min = 1.252E-5, lambda.1se = -1.0  27                            27                             54                      py_3_sid_9cdc"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_glm = sorted_glm_grid.models[0]\n",
    "tuned_glm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` Here we receive the best model for grid search along with the parameters fro the best model```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets evaluate the model on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GLM AUCPR: 0.4455 \n",
      "Tuned GLM AUCPR:0.4393\n"
     ]
    }
   ],
   "source": [
    "tuned_glm_perf = tuned_glm.model_performance(valid)\n",
    "print(\"Default GLM AUCPR: %.4f \\nTuned GLM AUCPR:%.4f\" % (glm.aucpr(), tuned_glm_perf.aucpr()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` Which suggests are default GLM model having higher AUCPR generalizes better than the tuned model```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As such we will do Scoring in Scikit-Learn Model which has a better threshold "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the H2o model in the Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Work\\\\Gre\\\\UTD\\\\Courses\\\\Fall\\\\MIS6341\\\\Softwares\\\\Python\\\\ml-fall-2023\\\\Project1\\\\artifacts_h2o\\\\glm_model_h2o.pkl\\\\glm_random_grid_model_11'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the directory path and the model file name separately\n",
    "model_directory = \"D:/Work/Gre/UTD/Courses/Fall/MIS6341/Softwares/Python/ml-fall-2023/Project1/artifacts_h2o\"\n",
    "model_filename = \"glm_model_h2o.pkl\"\n",
    "\n",
    "# Get the H2O model by its ID\n",
    "model_h2o = h2o.get_model(best_model_id)\n",
    "\n",
    "# Construct the full model path\n",
    "model_path = f\"{model_directory}/{model_filename}\"\n",
    "\n",
    "# Create an artifacts dictionary and include the model file path\n",
    "artifacts_dict = {}\n",
    "artifacts_dict[\"h2o_model_path\"] = model_path\n",
    "\n",
    "# Save the H2O model to the specified file path\n",
    "h2o.save_model(model_h2o, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def score_with_h2o_model(input_data):\n",
    "    import h2o\n",
    "    try:\n",
    "      h2o.cluster().shutdown()\n",
    "    except:\n",
    "      pass \n",
    "    from h2o.frame import H2OFrame\n",
    "    h2o.init(max_mem_size = \"4G\", nthreads=16)\n",
    "    try:\n",
    "        # Load the saved H2O model\n",
    "        model_path = \"D:/Work/Gre/UTD/Courses/Fall\\MIS6341/Softwares/Python/ml-fall-2023/Project1/artifacts_h2o/glm_model_h2o.pkl/glm_random_grid_model_11\"\n",
    "        loaded_model = h2o.load_model(model_path)\n",
    "\n",
    "        # Convert input_data to an H2O frame\n",
    "        input_h2o = h2o.H2OFrame(input_data)\n",
    "\n",
    "        # Use the loaded model for scoring\n",
    "        predictions = loaded_model.predict(input_h2o)\n",
    "\n",
    "        # Extract the predictions as a Pandas DataFrame\n",
    "        predictions_df = predictions.as_data_frame()\n",
    "\n",
    "        # Return the prediction results\n",
    "        return predictions_df\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H2O session _sid_9cdc closed.\n",
      "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "; Java HotSpot(TM) 64-Bit Server VM (build 25.361-b09, mixed mode)\n",
      "  Starting server from D:\\Work\\Gre\\UTD\\Courses\\Fall\\MIS6341\\Softwares\\Python\\ml-fall-2023\\Lib\\site-packages\\h2o\\backend\\bin\\h2o.jar\n",
      "  Ice root: C:\\Users\\Asus\\AppData\\Local\\Temp\\tmp_po7by6d\n",
      "  JVM stdout: C:\\Users\\Asus\\AppData\\Local\\Temp\\tmp_po7by6d\\h2o_Asus_started_from_python.out\n",
      "  JVM stderr: C:\\Users\\Asus\\AppData\\Local\\Temp\\tmp_po7by6d\\h2o_Asus_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "\n",
       "#h2o-table-13.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-13 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-13 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-13 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-13 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-13 .h2o-table th,\n",
       "#h2o-table-13 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-13 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-13\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption></caption>\n",
       "    <thead></thead>\n",
       "    <tbody><tr><td>H2O_cluster_uptime:</td>\n",
       "<td>02 secs</td></tr>\n",
       "<tr><td>H2O_cluster_timezone:</td>\n",
       "<td>America/Chicago</td></tr>\n",
       "<tr><td>H2O_data_parsing_timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O_cluster_version:</td>\n",
       "<td>3.42.0.3</td></tr>\n",
       "<tr><td>H2O_cluster_version_age:</td>\n",
       "<td>2 months and 14 days</td></tr>\n",
       "<tr><td>H2O_cluster_name:</td>\n",
       "<td>H2O_from_python_Asus_rhb1fn</td></tr>\n",
       "<tr><td>H2O_cluster_total_nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O_cluster_free_memory:</td>\n",
       "<td>3.548 Gb</td></tr>\n",
       "<tr><td>H2O_cluster_total_cores:</td>\n",
       "<td>16</td></tr>\n",
       "<tr><td>H2O_cluster_allowed_cores:</td>\n",
       "<td>16</td></tr>\n",
       "<tr><td>H2O_cluster_status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O_connection_url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O_connection_proxy:</td>\n",
       "<td>{\"http\": null, \"https\": null}</td></tr>\n",
       "<tr><td>H2O_internal_security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>Python_version:</td>\n",
       "<td>3.10.11 final</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n"
      ],
      "text/plain": [
       "--------------------------  -----------------------------\n",
       "H2O_cluster_uptime:         02 secs\n",
       "H2O_cluster_timezone:       America/Chicago\n",
       "H2O_data_parsing_timezone:  UTC\n",
       "H2O_cluster_version:        3.42.0.3\n",
       "H2O_cluster_version_age:    2 months and 14 days\n",
       "H2O_cluster_name:           H2O_from_python_Asus_rhb1fn\n",
       "H2O_cluster_total_nodes:    1\n",
       "H2O_cluster_free_memory:    3.548 Gb\n",
       "H2O_cluster_total_cores:    16\n",
       "H2O_cluster_allowed_cores:  16\n",
       "H2O_cluster_status:         locked, healthy\n",
       "H2O_connection_url:         http://127.0.0.1:54321\n",
       "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
       "H2O_internal_security:      False\n",
       "Python_version:             3.10.11 final\n",
       "--------------------------  -----------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Error: Argument `python_obj` should be a None | list | tuple | dict | numpy.ndarray | pandas.DataFrame | scipy.sparse.issparse, got H2OFrame      City     State    Zip       Bank    BankState    NAICS       NoEmp    NewExist    CreateJob    RetainedJob    FranchiseCode    UrbanRural    RevLineCr     LowDoc    DisbursementGross    BalanceGross     GrAppv    SBA_Appv    MIS_Status    Log_DisbursementGross    Log_GrAppv    Log_SBA_Appv    Log_BalanceGross    TotalJobs    IncomeToLoanRatio    EmployeesToLoanRatio    JobPerLoan    Gauren_SBA_Appv    DefaultRate\\n0.11465    0.184773  93001  0.0314465     0.218517   235910   0.600407     0.17044    -0.0353733     -0.0454543                1     0.0716743     0.15307   0.187063              0.358949     -0.00229552   0.394801    0.410973             0                 0.306712      0.332752        0.344279         -0.00229816   -0.0808276             0.873414                1.46094    -0.196674             0.960651        17.5096\\n0.337588   0.197662  11225  0.272221      0.220168   621111  -0.140136     0.186978   -0.022536      -0.0326467                0     0.243491      0.15307   0.187063             -0.451437     -0.00229552  -0.429073   -0.497867             0                -0.600454     -0.560494       -0.68889          -0.00229816   -0.0551827             0.906743                0.281473    0.110838             0.861822        17.5096\\n0.0615385  0.124634  54935  0.175694      0.117429   453220  -0.140136     0.186978   -0.0353733     -0.0454543                1     0.0716743     0.15307   0.0897581            -0.593428     -0.00229552  -0.573427   -0.547995             0                -0.899994     -0.851971       -0.794062         -0.00229816   -0.0808276             1.08291                 0.255725    0.147497             1.04641         17.5096\\n0.185864   0.188249  77379  0.147425      0.139976   332996   0.788908     0.17044    -0.0353733     -0.0454543                1     0.0716743     0.15307   0.187063              2.14249      -0.00229552   2.20803     2.02814              0                 1.14502       1.16566         1.10795          -0.00229816   -0.0808276             1.05638                 0.388981   -0.039853             1.0887          17.5096\\n0.275041   0.184773  90001  0             0.218517   448120   0.654264     0.17044     0.0288134     -0.0454543                1     0.243491      0.15307   0.187063              3.9295       -0.00229552   4.24307     5.44119              0                 1.59524       1.65691         1.86271          -0.00229816   -0.0166409             0.722176                0.120243   -0.00305832           0.779804        17.5096\\n0.189995   0.156142  64068  0.145683      0.126444   811192  -0.140136     0.17044    -0.0353733     -0.0454543                1     0.0716743     0.15307   0.187063             -0.143213     -0.00229552  -0.115719   -0.129536             0                -0.154566     -0.12298        -0.138729         -0.00229816   -0.0808276             1.10558                 1.08183     0.623976             0.893333        17.5096\\n0.175011   0.124634  54455  0.116923      0.117429   518210  -0.140136     0.17044    -0.0353733     -0.0411851                0     0.187265      0.251569  0.187063             -0.464802     -0.00229552  -0.573427   -0.587225             0                -0.625118     -0.851971       -0.884854         -0.00229816   -0.0765584             0.791522                0.238641    0.130373             0.976502        17.5096\\n0.117995   0.140225  98116  0.175694      0.159167   812113  -0.072814     0.17044    -0.0310942     -0.01557                  0     0.243491      0.251569  0.187063             -0.610744     -0.00229552  -0.591031   -0.598123             0                -0.943518     -0.894116       -0.911609         -0.00229816   -0.0466641             1.0211                  0.121737    0.0780177            0.988143        17.5096\\n0.115      0.12935    2745  0.0896552     0.140419   448130  -0.140136     0.17044    -0.0353733     -0.0411851                1     0.243491      0.251569  0.187063             -0.489532     -0.00229552  -0.467802   -0.521841             0                -0.672428     -0.63074        -0.737812         -0.00229816   -0.0765584             0.938087                0.268541    0.146708             0.896445        17.5096\\n0.276347   0.224201  60657  0.175694      0.159167   812111   0.0214369    0.17044    -0.0353733     -0.0454543                1     0.243491      0.15307   0.187063             -0.181308     -0.00229552  -0.154448   -0.100549             0                -0.200048     -0.167766       -0.105971         -0.00229816   -0.0808276             1.80318                -0.213198    0.803859             1.53604         17.5096\\n[84951 rows x 29 columns]\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_with_h2o_model(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import category_encoders as ce\n",
    "from copy import deepcopy\n",
    "\n",
    "def train_model(df):\n",
    "    \"\"\"\n",
    "    Train sample model and save artifacts\n",
    "    \"\"\"\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    import pickle\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.preprocessing import FunctionTransformer\n",
    "    from sklearn.metrics import average_precision_score\n",
    "    import numpy as np\n",
    "    import warnings\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    \n",
    "    \n",
    "    target_col = \"MIS_Status\"\n",
    "    cols_to_drop = ['City', 'State', 'Zip','Bank', 'BankState', 'LowDoc','RevLineCr','MIS_Status']\n",
    "    # Removing the index column\n",
    "    if \"index\" in df.columns:\n",
    "        df.drop(columns=\"index\", inplace=True)\n",
    "    y = df[target_col] if target_col in df.columns else None\n",
    "    X = df.drop(columns=[target_col]) if target_col in df.columns else df.copy()\n",
    "\n",
    "\n",
    "    # Relacing Missing values\n",
    "    \n",
    "    for i in df['RevLineCr']:\n",
    "      if i not in ['Y','N']:\n",
    "        df['RevLineCr'].replace(i,'N',inplace=True)\n",
    "        print(\"RevLineCr\",df['RevLineCr'].unique())\n",
    "\n",
    "    for i in df['LowDoc']:\n",
    "      if i not in ['Y','N']:\n",
    "        df['LowDoc'].replace(i,'N',inplace=True)\n",
    "        print(\"LowDoc\",df['LowDoc'].unique())\n",
    "\n",
    "    for i in df['NewExist']:\n",
    "      if i not in [1,2]:\n",
    "        df['NewExist'].replace(i,None,inplace=True)\n",
    "        print(\"NewExist\",df['NewExist'].unique())\n",
    "\n",
    "\n",
    "    category_cols=['City', 'State', 'Bank', 'BankState', 'RevLineCr', 'LowDoc','NewExist']\n",
    "    for column in category_cols:\n",
    "        df[column]=df[column].fillna(df[column].mode()[0])\n",
    "\n",
    "    # Target encoding the categorical columns\n",
    "    categorical_columns = ['City', 'State', 'Bank', 'BankState', 'RevLineCr', 'LowDoc','NewExist']\n",
    "    encoder = ce.TargetEncoder(cols=categorical_columns)\n",
    "    encoder.fit(df[categorical_columns], df['MIS_Status'])\n",
    "    train_encoded = encoder.transform(df[categorical_columns])\n",
    "    train_encoded = train_encoded.add_suffix('_trg')\n",
    "    #train_encoded = pd.concat([train_encoded, data], axis=1)\n",
    "    train_encoded = pd.concat([train_encoded, df], axis=1)\n",
    "    for column in categorical_columns:\n",
    "        train_encoded[column + \"_trg\"].fillna(train_encoded[column + \"_trg\"].mean(), inplace=True)\n",
    "    \n",
    "    # Renaming the columns\n",
    "    #train_encoded.rename(columns={col: col + \"_trg\" if col in categorical_columns else col for col in train_encoded.columns}, inplace=False)\n",
    "    print(train_encoded.columns)\n",
    "    \n",
    "\n",
    "\n",
    "    # Adding Features\n",
    "    import numpy as np\n",
    "    # Apply the log transformation to the specific feature in your training data\n",
    "    small_constant = 1e-10  # You can adjust this constant as needed\n",
    "    # df['LogColumn'] = np.log(df['OriginalColumn'] + small_constant)\n",
    "    train_encoded['Log_DisbursementGross'] = np.log1p(train_encoded['DisbursementGross'])\n",
    "    train_encoded['Log_GrAppv'] = np.log1p(train_encoded['GrAppv'])\n",
    "    train_encoded['Log_SBA_Appv'] = np.log1p(train_encoded['SBA_Appv'])\n",
    "    train_encoded['Log_BalanceGross'] = np.log1p(train_encoded['BalanceGross'])\n",
    "    train_encoded['TotalJobs'] = train_encoded['CreateJob'] + train_encoded['RetainedJob']\n",
    "    #train_encoded['Loan_Efficiency'] = train_encoded['DisbursementGross'] / (train_encoded['CreateJob'] + train_encoded['RetainedJob'] + 1)\n",
    "    # Calculate 'LoanToIncomeRatio' as a ratio of 'SBA_Appv' to 'DisbursementGross'\n",
    "    train_encoded['IncomeToLoanRatio'] = train_encoded['DisbursementGross'] / train_encoded['SBA_Appv']\n",
    "    # Calculate 'LoanToEmployeesRatio' as a ratio of 'SBA_Appv' to 'NoEmp'\n",
    "    train_encoded['EmployeesToLoanRatio'] = train_encoded['NoEmp'] / train_encoded['SBA_Appv']\n",
    "    # Create a binary feature to indicate loans with a balance ('BalanceGross' > 0)\n",
    "    #train_encoded['HasBalance'] = (train_encoded['BalanceGross'] > 0).astype(int)\n",
    "    # Calculate 'LoanPerJob' as a ratio of 'SBA_Appv' to 'TotalJobs'\n",
    "    train_encoded['JobPerLoan'] = train_encoded['TotalJobs'] / train_encoded['SBA_Appv'] \n",
    "    # Calculate SBA's Gaurenteed Portion of Approved Loan\n",
    "    train_encoded['Gauren_SBA_Appv'] = train_encoded['GrAppv'] / train_encoded['SBA_Appv']\n",
    "\n",
    "    \n",
    "    # Scaling the numerical columns\n",
    "    numerical_columns = [ 'NoEmp', 'CreateJob', 'RetainedJob', 'GrAppv', 'SBA_Appv', 'DisbursementGross', 'BalanceGross',\n",
    "                        'Log_DisbursementGross', 'Log_GrAppv', 'Log_SBA_Appv', 'Log_BalanceGross','TotalJobs','IncomeToLoanRatio', \n",
    "                        'EmployeesToLoanRatio', 'JobPerLoan', 'Gauren_SBA_Appv']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    #fit and transform separately\n",
    "    scaler.fit(train_encoded[numerical_columns])\n",
    "    train_encoded[numerical_columns] = scaler.transform(train_encoded[numerical_columns])\n",
    "      \n",
    "\n",
    "    #X = df.copy()\n",
    "    #X = X.reset_index(drop=True)\n",
    "\n",
    "    clf = LogisticRegression(random_state=42,max_iter=100,n_jobs=-1, verbose=1)\n",
    "    columns_to_train = [col for col in train_encoded.columns if col not in cols_to_drop]\n",
    "    clf_ = clf.fit(train_encoded[columns_to_train],y)\n",
    "\n",
    "    # Evaluate the model using AUCPR\n",
    "    aucpr_score = average_precision_score(y_true=y, y_score=clf_.predict_proba(train_encoded[columns_to_train])[:, 1])\n",
    "    print(\"AUCPR score:\", aucpr_score)\n",
    "       \n",
    "    \n",
    "    param_grid = {'C':[10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "                 'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "                 'solver': ['lbfgs'], # 'newton-cholesky', 'sag'\n",
    "                 'l1_ratio': [0.1, 0.3, 0.7]\n",
    "                 }\n",
    "    \n",
    "\n",
    "    \n",
    "    grid1 = GridSearchCV(clf_, \n",
    "                    param_grid, cv =7 , return_train_score= True)\n",
    "    grid1.fit(train_encoded[columns_to_train], y)\n",
    "    print(\"Best parameters found: \", grid1.best_params_)\n",
    "    print(\"Best cross-validation score: {:.2f}\".format(grid1.best_score_))\n",
    "    top_clf = grid1.best_estimator_\n",
    "        \n",
    "    #clf = LogisticRegression(max_iter=100, random_state=0)\n",
    "    \n",
    "    #columns_to_train = [x for x in X.columns if x not in cols_to_drop]\n",
    "    #print(\"Training on following columns:\", columns_to_train)\n",
    "    # Create logistic regression classifier\n",
    "    # Fit classifier to training data\n",
    "    # grid1 = GridSearchCV(clf.fit(X[columns_to_train], y), \n",
    "    #                  param_grid, cv =7 , return_train_score= True)\n",
    "    # grid1.fit(X[columns_to_train], y)\n",
    "    # print(grid1.best_params_)\n",
    "    #clf.fit(X_train[columns_to_train], y_train)\n",
    "\n",
    "          \n",
    "   \n",
    "    # End Todo\n",
    "    \n",
    "    # Saving the artifacts\n",
    "    artifacts_dict = {\n",
    "        \"model\": top_clf,\n",
    "        \"target_encoder\": encoder,\n",
    "        \"te_columns\": categorical_columns,\n",
    "        \"columns_to_train\":columns_to_train,\n",
    "        \"numerical_columns\":numerical_columns,\n",
    "        \"category_cols\":category_cols,\n",
    "        \"scaler\":scaler,\n",
    "        \"h2o_model_path\":model_path\n",
    "    }\n",
    "\n",
    "    #calculating threshold\n",
    "    if y is not None:\n",
    "        optimal_threshold = calculate_optimal_threshold(clf_, train_encoded[columns_to_train], y)\n",
    "        print(f\"Optimal Threshold: {optimal_threshold}\")\n",
    "        # Saving the threshold in artifacts\n",
    "        artifacts_dict[\"threshold\"] = optimal_threshold\n",
    "\n",
    "    artifacts_dict_file = open(\"D:/Work/Gre/UTD/Courses/Fall/MIS6341/Softwares/Python/ml-fall-2023/Project1/artifacts/artifacts_dict_file.pkl\", \"wb\")\n",
    "    pickle.dump(obj=artifacts_dict, file=artifacts_dict_file)\n",
    "    \n",
    "    artifacts_dict_file.close()    \n",
    "    return clf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RevLineCr ['Y' 'N' 'T' nan '`' 'R' '1' '.' '5' '2' '3' 'A' '-' '7' 'C']\n",
      "RevLineCr ['Y' 'N' nan '`' 'R' '1' '.' '5' '2' '3' 'A' '-' '7' 'C']\n",
      "RevLineCr ['Y' 'N' '`' 'R' '1' '.' '5' '2' '3' 'A' '-' '7' 'C']\n",
      "RevLineCr ['Y' 'N' 'R' '1' '.' '5' '2' '3' 'A' '-' '7' 'C']\n",
      "RevLineCr ['Y' 'N' '1' '.' '5' '2' '3' 'A' '-' '7' 'C']\n",
      "RevLineCr ['Y' 'N' '.' '5' '2' '3' 'A' '-' '7' 'C']\n",
      "RevLineCr ['Y' 'N' '5' '2' '3' 'A' '-' '7' 'C']\n",
      "RevLineCr ['Y' 'N' '2' '3' 'A' '-' '7' 'C']\n",
      "RevLineCr ['Y' 'N' '3' 'A' '-' '7' 'C']\n",
      "RevLineCr ['Y' 'N' 'A' '-' '7' 'C']\n",
      "RevLineCr ['Y' 'N' '-' '7' 'C']\n",
      "RevLineCr ['Y' 'N' '7' 'C']\n",
      "RevLineCr ['Y' 'N' 'C']\n",
      "RevLineCr ['Y' 'N']\n",
      "LowDoc ['N' 'Y' 'R' 'S' 'C' '0' 'A' '1']\n",
      "LowDoc ['N' 'Y' 'S' 'C' '0' 'A' '1']\n",
      "LowDoc ['N' 'Y' 'C' '0' 'A' '1']\n",
      "LowDoc ['N' 'Y' '0' 'A' '1']\n",
      "LowDoc ['N' 'Y' 'A' '1']\n",
      "LowDoc ['N' 'Y' '1']\n",
      "LowDoc ['N' 'Y']\n",
      "NewExist [1.0 2.0 None nan]\n",
      "NewExist [1.0 2.0 None nan]\n",
      "NewExist [1.0 2.0 None nan]\n",
      "NewExist [1.0 2.0 None nan]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "NewExist [1.0 2.0 None]\n",
      "Index(['City_trg', 'State_trg', 'Bank_trg', 'BankState_trg', 'RevLineCr_trg',\n",
      "       'LowDoc_trg', 'NewExist_trg', 'City', 'State', 'Zip', 'Bank',\n",
      "       'BankState', 'NAICS', 'NoEmp', 'NewExist', 'CreateJob', 'RetainedJob',\n",
      "       'FranchiseCode', 'UrbanRural', 'RevLineCr', 'LowDoc',\n",
      "       'DisbursementGross', 'BalanceGross', 'GrAppv', 'SBA_Appv',\n",
      "       'MIS_Status'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUCPR score: 0.15862387003852957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'C': 10, 'l1_ratio': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best cross-validation score: 0.82\n",
      "Optimal Threshold: 0.5050505050505051\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(n_jobs=-1, random_state=42, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(n_jobs=-1, random_state=42, verbose=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(n_jobs=-1, random_state=42, verbose=1)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "        \n",
    "df = pd.read_csv(\"D:/Work/Gre/UTD/Courses/Fall/MIS6341/Softwares/Python/ml-fall-2023/Project1/SBA_loans_project_1.csv\")\n",
    "target = \"MIS_Status\"\n",
    "y = df[target]\n",
    "x = df.drop(columns=[target])\n",
    "\n",
    "# Splitting the dataset into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "X_train.reset_index(inplace=True, drop=True)\n",
    "y_train.reset_index(inplace=True, drop=True)\n",
    "X_test.reset_index(inplace=True, drop=True)\n",
    "y_test.reset_index(inplace=True, drop=True)\n",
    "df_train = X_train.copy()\n",
    "df_train[target] = y_train\n",
    "train_model(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target encoder mapping:\n",
      "['City', 'State', 'Bank', 'BankState', 'RevLineCr', 'LowDoc', 'NewExist']\n",
      "Columns to train:\n",
      "['City_trg', 'State_trg', 'Bank_trg', 'BankState_trg', 'RevLineCr_trg', 'LowDoc_trg', 'NewExist_trg', 'NAICS', 'NoEmp', 'NewExist', 'CreateJob', 'RetainedJob', 'FranchiseCode', 'UrbanRural', 'DisbursementGross', 'BalanceGross', 'GrAppv', 'SBA_Appv', 'Log_DisbursementGross', 'Log_GrAppv', 'Log_SBA_Appv', 'Log_BalanceGross', 'TotalJobs', 'IncomeToLoanRatio', 'EmployeesToLoanRatio', 'JobPerLoan', 'Gauren_SBA_Appv']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def load_and_print_artifacts_dict(path):\n",
    "    artifacts_dict = pickle.load(open(path, \"rb\"))\n",
    "\n",
    "    print(\"Target encoder mapping:\")\n",
    "    print([ac for ac in artifacts_dict[\"target_encoder\"].mapping])\n",
    "\n",
    "    print(\"Columns to train:\")\n",
    "    print([ac for ac in artifacts_dict[\"columns_to_train\"]])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_and_print_artifacts_dict(\"./Artifacts/artifacts_dict_file.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-fall-2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
